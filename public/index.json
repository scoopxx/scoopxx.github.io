[{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"Intuition I\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about RLHF, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\nHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\nIntroduction of Reinforcement Learning So first of all, why is RL needed since we already have supervised/unsupervised learning? And how is RL different from the other methods?\nWikipedia\u0026rsquo;s definition of the term Machine Learning is: statistical algorithms that can learn from data and generaize to unseen data. Say we have data $X$, and its training data sample $x_i$, we are trying to learning the distribution about $X$.\nIn supervised learning, the data is pair of $\u0026lt;x_i, y_i\u0026gt;$, so we are trying to learn distribution $p(y_ | x_)$. For example, in image classification, given a image $x$, trying to predict its label $y$. If we viewed GPT as a special case of supervised learning, it can be seen as given a text sequence\u0026rsquo;s previous tokens, predict the next token.\nIn reinforcement learning, the $x$ is no longer static, or known before training. If in supervised learning the goal is to learn a mapping from $x_i$ to $y_i$, in RL it became to learn the mapping from situations to actions, both can only get through direct interaction with the environment.\nComponents of RL We defined reinforcement learning as a method to model the interaction in a game/strategy etc. There are two entities in such RL system:\n$Agent$ is the learner and decision-maker. $Environment$ is everything outside the agent that it interacts with. Then there are three types of interactions between agent and environment:\n$Action$ is an action the agent takes to interact with the environment. $State$ is a representation of the environment, which is changed by action. $Reward$ can be seen as the environment\u0026rsquo;s corresponding response to the agent based on its state. A sequence of interactions is:\nIn $t$, the environment\u0026rsquo;s state is $S_t$, it exhibits reward $R_t$ Based on $S_t$ and $R_t$, the agent takes action $A_t$ As a result of $A_t$, the state changed to $S_{t+1}$, and exhibits reward $R_{t+1}$ The goal of an RL system is to learn a strategy so the agent makes the best action in each step to maximize its rewards. We can define three components of such agent:\n$Policy$ is the strategy the agent used to determine its next action. $Value\\ Function$ is a function the agent used to evaluate its current state $Model$ is the way the environment interacts with the agent, it defined how the environment works. 2.1 Policy Policy is a mapping from current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nIn Stochastic Policy, the result is a ditribution, in which the agent sample its action from.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t) $\nIn Deterministic Policy, the result is a action, for a given state, the agent will surely execute a specific action.\n$a_{t} = argmax\\ \\pi(a|s_t)$\n2.2 Value Function In an RL system, the end goal is to find a strategy to win the game, like in Chess or Go. At any timestamp t, we wants to have a metric to evaluate the effectiveness of current strategy.\nThe reward $R_t$ defined above only refers to immediate reward at timestamp $t$, but not the overall winning chance. So we defined $V_t$ as the overall reward at timestamp $t$, which includes the current immediate reward $R_t$, and its expected future reward $V_{t+1}$.\n$V_t = R_t + \\gamma V_{t+1}$\nSo the reward at timestamp $t$ is decided by its current state $s_t$, its taken action $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssume we take $T$ steps from timestamp $0$ to $T-1$, all these steps form a trajectory $\\tau$, the sum reward can be represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.3 Model A $model$ defined how the environment interacts with the agent. A model is comprised of $\u0026lt;S, A, P, R\u0026gt;$\n$S$, the space of all possible states $A$, the space of all possible actions $P$, transformation function of how state changed, $P(s_{t+1}|s_t, a_t)$ $R$, how reward is calculated for each state, $R(s_t, a_t)$ If all these 4 elements are known, we can easily model the interaction without actual interaction, this is called model based learning.\nIn reallity, while $S$ and $A$ is often known, the transformation and reward part is either fully unknown or hard to estimate, so agent need to interact with real environment to observe the state and reward, this is called model-free learning.\nComparing the two, model-free learning relied on real interaction to get the next state and reward, while model based learning modeled these without specific interaction.\nOptimization of RL The optimization of a RL task can be divided into two parsts:\nValue estimation: given a strategy $\\pi$, evaluate the effectiveness of how does the strategy work, this is computing $V_\\pi$. Policy optimization: given the value function $V_\\pi$, optimize to get a better policy $\\pi$. In essence this is very similar to k-Means clustering that we have two optimization targets, and we opmitize them iteratively to get the optimial answer. In k-means,\nGiven the current cluster assignment of each point, we compute the optimal centroid. Similar to value estimation. Given the current optimial centroid, we find better cluster assignment for each point. Similar to policy optimization. But are the two steps both necessary in RL optimization? Answer is No.\n3.1 Value-based agent An agent can only learn the value function $V_\\pi$, it can maintain a table of mapping from $\u0026lt;S, A\u0026gt;$ to $V$, then in each step, it picked the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitely have a strategy or policy.\n$ a_t = argmax\\ V(a | s_t)$\n3.2 Policy-based agent A policy agent directly learns the policy, and each step it directly outputs the distribution of next action without knowing value function.\n$ a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as we described above.\nActor refers to learning of policy $\\pi$ Critic refers to learning of value function $V_\\pi$ 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitily by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Initialization: Intialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 if self.agent_pos == self.end: done, reward = True, 0 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\n7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"intuition\"\u003eIntuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about \u003ccode\u003eRLHF\u003c/code\u003e, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\u003c/p\u003e\n\u003cp\u003eHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"Intuition I\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about RLHF, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\nHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\nIntroduction of Reinforcement Learning So first of all, why is RL needed since we already have supervised/unsupervised learning? And how is RL different from the other methods?\nWikipedia\u0026rsquo;s definition of the term Machine Learning is: statistical algorithms that can learn from data and generaize to unseen data. Say we have data $X$, and its training data sample $x_i$, we are trying to learning the distribution about $X$.\nIn supervised learning, the data is pair of $\u0026lt;x_i, y_i\u0026gt;$, so we are trying to learn distribution $p(y_ | x_)$. For example, in image classification, given a image $x$, trying to predict its label $y$. If we viewed GPT as a special case of supervised learning, it can be seen as given a text sequence\u0026rsquo;s previous tokens, predict the next token.\nIn reinforcement learning, the $x$ is no longer static, or known before training. If in supervised learning the goal is to learn a mapping from $x_i$ to $y_i$, in RL it became to learn the mapping from situations to actions, both can only get through direct interaction with the environment.\nComponents of RL We defined reinforcement learning as a method to model the interaction in a game/strategy etc. There are two entities in such RL system:\n$Agent$ is the learner and decision-maker. $Environment$ is everything outside the agent that it interacts with. Then there are three types of interactions between agent and environment:\n$Action$ is an action the agent takes to interact with the environment. $State$ is a representation of the environment, which is changed by action. $Reward$ can be seen as the environment\u0026rsquo;s corresponding response to the agent based on its state. A sequence of interactions is:\nIn $t$, the environment\u0026rsquo;s state is $S_t$, it exhibits reward $R_t$ Based on $S_t$ and $R_t$, the agent takes action $A_t$ As a result of $A_t$, the state changed to $S_{t+1}$, and exhibits reward $R_{t+1}$ The goal of an RL system is to learn a strategy so the agent makes the best action in each step to maximize its rewards. We can define three components of such agent:\n$Policy$ is the strategy the agent used to determine its next action. $Value\\ Function$ is a function the agent used to evaluate its current state $Model$ is the way the environment interacts with the agent, it defined how the environment works. 2.1 Policy Policy is a mapping from current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nIn Stochastic Policy, the result is a ditribution, in which the agent sample its action from.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t) $\nIn Deterministic Policy, the result is a action, for a given state, the agent will surely execute a specific action.\n$a_{t} = argmax\\ \\pi(a|s_t)$\n2.2 Value Function In an RL system, the end goal is to find a strategy to win the game, like in Chess or Go. At any timestamp t, we wants to have a metric to evaluate the effectiveness of current strategy.\nThe reward $R_t$ defined above only refers to immediate reward at timestamp $t$, but not the overall winning chance. So we defined $V_t$ as the overall reward at timestamp $t$, which includes the current immediate reward $R_t$, and its expected future reward $V_{t+1}$.\n$V_t = R_t + \\gamma V_{t+1}$\nSo the reward at timestamp $t$ is decided by its current state $s_t$, its taken action $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssume we take $T$ steps from timestamp $0$ to $T-1$, all these steps form a trajectory $\\tau$, the sum reward can be represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.3 Model A $model$ defined how the environment interacts with the agent. A model is comprised of $\u0026lt;S, A, P, R\u0026gt;$\n$S$, the space of all possible states $A$, the space of all possible actions $P$, transformation function of how state changed, $P(s_{t+1}|s_t, a_t)$ $R$, how reward is calculated for each state, $R(s_t, a_t)$ If all these 4 elements are known, we can easily model the interaction without actual interaction, this is called model based learning.\nIn reallity, while $S$ and $A$ is often known, the transformation and reward part is either fully unknown or hard to estimate, so agent need to interact with real environment to observe the state and reward, this is called model-free learning.\nComparing the two, model-free learning relied on real interaction to get the next state and reward, while model based learning modeled these without specific interaction.\nOptimization of RL The optimization of a RL task can be divided into two parsts:\nValue estimation: given a strategy $\\pi$, evaluate the effectiveness of how does the strategy work, this is computing $V_\\pi$. Policy optimization: given the value function $V_\\pi$, optimize to get a better policy $\\pi$. In essence this is very similar to k-Means clustering that we have two optimization targets, and we opmitize them iteratively to get the optimial answer. In k-means,\nGiven the current cluster assignment of each point, we compute the optimal centroid. Similar to value estimation. Given the current optimial centroid, we find better cluster assignment for each point. Similar to policy optimization. But are the two steps both necessary in RL optimization? Answer is No.\n3.1 Value-based agent An agent can only learn the value function $V_\\pi$, it can maintain a table of mapping from $\u0026lt;S, A\u0026gt;$ to $V$, then in each step, it picked the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitely have a strategy or policy.\n$ a_t = argmax\\ V(a | s_t)$\n3.2 Policy-based agent A policy agent directly learns the policy, and each step it directly outputs the distribution of next action without knowing value function.\n$ a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as we described above.\nActor refers to learning of policy $\\pi$ Critic refers to learning of value function $V_\\pi$ 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitily by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Initialization: Intialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 if self.agent_pos == self.end: done, reward = True, 0 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\n7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"intuition\"\u003eIntuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about \u003ccode\u003eRLHF\u003c/code\u003e, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\u003c/p\u003e\n\u003cp\u003eHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"Intuition I\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about RLHF, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\nHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\nIntroduction of Reinforcement Learning So first of all, why is RL needed since we already have supervised/unsupervised learning? And how is RL different from the other methods?\nWikipedia\u0026rsquo;s definition of the term Machine Learning is: statistical algorithms that can learn from data and generaize to unseen data. Say we have data $X$, and its training data sample $x_i$, we are trying to learning the distribution about $X$.\nIn supervised learning, the data is pair of $\u0026lt;x_i, y_i\u0026gt;$, so we are trying to learn distribution $p(y_ | x_)$. For example, in image classification, given a image $x$, trying to predict its label $y$. If we viewed GPT as a special case of supervised learning, it can be seen as given a text sequence\u0026rsquo;s previous tokens, predict the next token.\nIn reinforcement learning, the $x$ is no longer static, or known before training. If in supervised learning the goal is to learn a mapping from $x_i$ to $y_i$, in RL it became to learn the mapping from situations to actions, both can only get through direct interaction with the environment.\nComponents of RL We defined reinforcement learning as a method to model the interaction in a game/strategy etc. There are two entities in such RL system:\n$Agent$ is the learner and decision-maker. $Environment$ is everything outside the agent that it interacts with. Then there are three types of interactions between agent and environment:\n$Action$ is an action the agent takes to interact with the environment. $State$ is a representation of the environment, which is changed by action. $Reward$ can be seen as the environment\u0026rsquo;s corresponding response to the agent based on its state. A sequence of interactions is:\nIn $t$, the environment\u0026rsquo;s state is $S_t$, it exhibits reward $R_t$ Based on $S_t$ and $R_t$, the agent takes action $A_t$ As a result of $A_t$, the state changed to $S_{t+1}$, and exhibits reward $R_{t+1}$ The goal of an RL system is to learn a strategy so the agent makes the best action in each step to maximize its rewards. We can define three components of such agent:\n$Policy$ is the strategy the agent used to determine its next action. $Value\\ Function$ is a function the agent used to evaluate its current state $Model$ is the way the environment interacts with the agent, it defined how the environment works. 2.1 Policy Policy is a mapping from current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nIn Stochastic Policy, the result is a ditribution, in which the agent sample its action from.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t) $\nIn Deterministic Policy, the result is a action, for a given state, the agent will surely execute a specific action.\n$a_{t} = argmax\\ \\pi(a|s_t)$\n2.2 Value Function In an RL system, the end goal is to find a strategy to win the game, like in Chess or Go. At any timestamp t, we wants to have a metric to evaluate the effectiveness of current strategy.\nThe reward $R_t$ defined above only refers to immediate reward at timestamp $t$, but not the overall winning chance. So we defined $V_t$ as the overall reward at timestamp $t$, which includes the current immediate reward $R_t$, and its expected future reward $V_{t+1}$.\n$V_t = R_t + \\gamma V_{t+1}$\nSo the reward at timestamp $t$ is decided by its current state $s_t$, its taken action $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssume we take $T$ steps from timestamp $0$ to $T-1$, all these steps form a trajectory $\\tau$, the sum reward can be represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.3 Model A $model$ defined how the environment interacts with the agent. A model is comprised of $\u0026lt;S, A, P, R\u0026gt;$\n$S$, the space of all possible states $A$, the space of all possible actions $P$, transformation function of how state changed, $P(s_{t+1}|s_t, a_t)$ $R$, how reward is calculated for each state, $R(s_t, a_t)$ If all these 4 elements are known, we can easily model the interaction without actual interaction, this is called model based learning.\nIn reallity, while $S$ and $A$ is often known, the transformation and reward part is either fully unknown or hard to estimate, so agent need to interact with real environment to observe the state and reward, this is called model-free learning.\nComparing the two, model-free learning relied on real interaction to get the next state and reward, while model based learning modeled these without specific interaction.\nOptimization of RL The optimization of a RL task can be divided into two parsts:\nValue estimation: given a strategy $\\pi$, evaluate the effectiveness of how does the strategy work, this is computing $V_\\pi$. Policy optimization: given the value function $V_\\pi$, optimize to get a better policy $\\pi$. In essence this is very similar to k-Means clustering that we have two optimization targets, and we opmitize them iteratively to get the optimial answer. In k-means,\nGiven the current cluster assignment of each point, we compute the optimal centroid. Similar to value estimation. Given the current optimial centroid, we find better cluster assignment for each point. Similar to policy optimization. But are the two steps both necessary in RL optimization? Answer is No.\n3.1 Value-based agent An agent can only learn the value function $V_\\pi$, it can maintain a table of mapping from $\u0026lt;S, A\u0026gt;$ to $V$, then in each step, it picked the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitely have a strategy or policy.\n$ a_t = argmax\\ V(a | s_t)$\n3.2 Policy-based agent A policy agent directly learns the policy, and each step it directly outputs the distribution of next action without knowing value function.\n$ a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as we described above.\nActor refers to learning of policy $\\pi$ Critic refers to learning of value function $V_\\pi$ 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitily by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Initialization: Intialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 if self.agent_pos == self.end: done, reward = True, 0 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\n7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"intuition\"\u003eIntuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about \u003ccode\u003eRLHF\u003c/code\u003e, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\u003c/p\u003e\n\u003cp\u003eHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"Intuition I\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about RLHF, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\nHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\nIntroduction of Reinforcement Learning So first of all, why is RL needed since we already have supervised/unsupervised learning? And how is RL different from the other methods?\nWikipedia\u0026rsquo;s definition of the term Machine Learning is: statistical algorithms that can learn from data and generaize to unseen data. Say we have data $X$, and its training data sample $x_i$, we are trying to learning the distribution about $X$.\nIn supervised learning, the data is pair of $\u0026lt;x_i, y_i\u0026gt;$, so we are trying to learn distribution $p(y_ | x_)$. For example, in image classification, given a image $x$, trying to predict its label $y$. If we viewed GPT as a special case of supervised learning, it can be seen as given a text sequence\u0026rsquo;s previous tokens, predict the next token.\nIn reinforcement learning, the $x$ is no longer static, or known before training. If in supervised learning the goal is to learn a mapping from $x_i$ to $y_i$, in RL it became to learn the mapping from situations to actions, both can only get through direct interaction with the environment.\nComponents of RL We defined reinforcement learning as a method to model the interaction in a game/strategy etc. There are two entities in such RL system:\n$Agent$ is the learner and decision-maker. $Environment$ is everything outside the agent that it interacts with. Then there are three types of interactions between agent and environment:\n$Action$ is an action the agent takes to interact with the environment. $State$ is a representation of the environment, which is changed by action. $Reward$ can be seen as the environment\u0026rsquo;s corresponding response to the agent based on its state. A sequence of interactions is:\nIn $t$, the environment\u0026rsquo;s state is $S_t$, it exhibits reward $R_t$ Based on $S_t$ and $R_t$, the agent takes action $A_t$ As a result of $A_t$, the state changed to $S_{t+1}$, and exhibits reward $R_{t+1}$ The goal of an RL system is to learn a strategy so the agent makes the best action in each step to maximize its rewards. We can define three components of such agent:\n$Policy$ is the strategy the agent used to determine its next action. $Value\\ Function$ is a function the agent used to evaluate its current state $Model$ is the way the environment interacts with the agent, it defined how the environment works. 2.1 Policy Policy is a mapping from current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nIn Stochastic Policy, the result is a ditribution, in which the agent sample its action from.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t) $\nIn Deterministic Policy, the result is a action, for a given state, the agent will surely execute a specific action.\n$a_{t} = argmax\\ \\pi(a|s_t)$\n2.2 Value Function In an RL system, the end goal is to find a strategy to win the game, like in Chess or Go. At any timestamp t, we wants to have a metric to evaluate the effectiveness of current strategy.\nThe reward $R_t$ defined above only refers to immediate reward at timestamp $t$, but not the overall winning chance. So we defined $V_t$ as the overall reward at timestamp $t$, which includes the current immediate reward $R_t$, and its expected future reward $V_{t+1}$.\n$V_t = R_t + \\gamma V_{t+1}$\nSo the reward at timestamp $t$ is decided by its current state $s_t$, its taken action $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssume we take $T$ steps from timestamp $0$ to $T-1$, all these steps form a trajectory $\\tau$, the sum reward can be represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.3 Model A $model$ defined how the environment interacts with the agent. A model is comprised of $\u0026lt;S, A, P, R\u0026gt;$\n$S$, the space of all possible states $A$, the space of all possible actions $P$, transformation function of how state changed, $P(s_{t+1}|s_t, a_t)$ $R$, how reward is calculated for each state, $R(s_t, a_t)$ If all these 4 elements are known, we can easily model the interaction without actual interaction, this is called model based learning.\nIn reallity, while $S$ and $A$ is often known, the transformation and reward part is either fully unknown or hard to estimate, so agent need to interact with real environment to observe the state and reward, this is called model-free learning.\nComparing the two, model-free learning relied on real interaction to get the next state and reward, while model based learning modeled these without specific interaction.\nOptimization of RL The optimization of a RL task can be divided into two parsts:\nValue estimation: given a strategy $\\pi$, evaluate the effectiveness of how does the strategy work, this is computing $V_\\pi$. Policy optimization: given the value function $V_\\pi$, optimize to get a better policy $\\pi$. In essence this is very similar to k-Means clustering that we have two optimization targets, and we opmitize them iteratively to get the optimial answer. In k-means,\nGiven the current cluster assignment of each point, we compute the optimal centroid. Similar to value estimation. Given the current optimial centroid, we find better cluster assignment for each point. Similar to policy optimization. But are the two steps both necessary in RL optimization? Answer is No.\n3.1 Value-based agent An agent can only learn the value function $V_\\pi$, it can maintain a table of mapping from $\u0026lt;S, A\u0026gt;$ to $V$, then in each step, it picked the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitely have a strategy or policy.\n$ a_t = argmax\\ V(a | s_t)$\n3.2 Policy-based agent A policy agent directly learns the policy, and each step it directly outputs the distribution of next action without knowing value function.\n$ a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as we described above.\nActor refers to learning of policy $\\pi$ Critic refers to learning of value function $V_\\pi$ 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitily by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Initialization: Intialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 if self.agent_pos == self.end: done, reward = True, 0 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\n7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"intuition\"\u003eIntuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about \u003ccode\u003eRLHF\u003c/code\u003e, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\u003c/p\u003e\n\u003cp\u003eHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"Intuition I\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about RLHF, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\nHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\nIntroduction of Reinforcement Learning So first of all, why is RL needed since we already have supervised/unsupervised learning? And how is RL different from the other methods?\nWikipedia\u0026rsquo;s definition of the term Machine Learning is: statistical algorithms that can learn from data and generaize to unseen data. Say we have data $X$, and its training data sample $x_i$, we are trying to learning the distribution about $X$.\nIn supervised learning, the data is pair of $\u0026lt;x_i, y_i\u0026gt;$, so we are trying to learn distribution $p(y_ | x_)$. For example, in image classification, given a image $x$, trying to predict its label $y$. If we viewed GPT as a special case of supervised learning, it can be seen as given a text sequence\u0026rsquo;s previous tokens, predict the next token.\nIn reinforcement learning, the $x$ is no longer static, or known before training. If in supervised learning the goal is to learn a mapping from $x_i$ to $y_i$, in RL it became to learn the mapping from situations to actions, both can only get through direct interaction with the environment.\nComponents of RL We defined reinforcement learning as a method to model the interaction in a game/strategy etc. There are two entities in such RL system:\n$Agent$ is the learner and decision-maker. $Environment$ is everything outside the agent that it interacts with. Then there are three types of interactions between agent and environment:\n$Action$ is an action the agent takes to interact with the environment. $State$ is a representation of the environment, which is changed by action. $Reward$ can be seen as the environment\u0026rsquo;s corresponding response to the agent based on its state. A sequence of interactions is:\nIn $t$, the environment\u0026rsquo;s state is $S_t$, it exhibits reward $R_t$ Based on $S_t$ and $R_t$, the agent takes action $A_t$ As a result of $A_t$, the state changed to $S_{t+1}$, and exhibits reward $R_{t+1}$ The goal of an RL system is to learn a strategy so the agent makes the best action in each step to maximize its rewards. We can define three components of such agent:\n$Policy$ is the strategy the agent used to determine its next action. $Value\\ Function$ is a function the agent used to evaluate its current state $Model$ is the way the environment interacts with the agent, it defined how the environment works. 2.1 Policy Policy is a mapping from current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nIn Stochastic Policy, the result is a ditribution, in which the agent sample its action from.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t) $\nIn Deterministic Policy, the result is a action, for a given state, the agent will surely execute a specific action.\n$a_{t} = argmax\\ \\pi(a|s_t)$\n2.2 Value Function In an RL system, the end goal is to find a strategy to win the game, like in Chess or Go. At any timestamp t, we wants to have a metric to evaluate the effectiveness of current strategy.\nThe reward $R_t$ defined above only refers to immediate reward at timestamp $t$, but not the overall winning chance. So we defined $V_t$ as the overall reward at timestamp $t$, which includes the current immediate reward $R_t$, and its expected future reward $V_{t+1}$.\n$V_t = R_t + \\gamma V_{t+1}$\nSo the reward at timestamp $t$ is decided by its current state $s_t$, its taken action $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssume we take $T$ steps from timestamp $0$ to $T-1$, all these steps form a trajectory $\\tau$, the sum reward can be represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.3 Model A $model$ defined how the environment interacts with the agent. A model is comprised of $\u0026lt;S, A, P, R\u0026gt;$\n$S$, the space of all possible states $A$, the space of all possible actions $P$, transformation function of how state changed, $P(s_{t+1}|s_t, a_t)$ $R$, how reward is calculated for each state, $R(s_t, a_t)$ If all these 4 elements are known, we can easily model the interaction without actual interaction, this is called model based learning.\nIn reallity, while $S$ and $A$ is often known, the transformation and reward part is either fully unknown or hard to estimate, so agent need to interact with real environment to observe the state and reward, this is called model-free learning.\nComparing the two, model-free learning relied on real interaction to get the next state and reward, while model based learning modeled these without specific interaction.\nOptimization of RL The optimization of a RL task can be divided into two parsts:\nValue estimation: given a strategy $\\pi$, evaluate the effectiveness of how does the strategy work, this is computing $V_\\pi$. Policy optimization: given the value function $V_\\pi$, optimize to get a better policy $\\pi$. In essence this is very similar to k-Means clustering that we have two optimization targets, and we opmitize them iteratively to get the optimial answer. In k-means,\nGiven the current cluster assignment of each point, we compute the optimal centroid. Similar to value estimation. Given the current optimial centroid, we find better cluster assignment for each point. Similar to policy optimization. But are the two steps both necessary in RL optimization? Answer is No.\n3.1 Value-based agent An agent can only learn the value function $V_\\pi$, it can maintain a table of mapping from $\u0026lt;S, A\u0026gt;$ to $V$, then in each step, it picked the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitely have a strategy or policy.\n$ a_t = argmax\\ V(a | s_t)$\n3.2 Policy-based agent A policy agent directly learns the policy, and each step it directly outputs the distribution of next action without knowing value function.\n$ a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as we described above.\nActor refers to learning of policy $\\pi$ Critic refers to learning of value function $V_\\pi$ 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitily by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Initialization: Intialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 if self.agent_pos == self.end: done, reward = True, 0 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\n7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"intuition\"\u003eIntuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about \u003ccode\u003eRLHF\u003c/code\u003e, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\u003c/p\u003e\n\u003cp\u003eHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"Intuition I\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about RLHF, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\nHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\nIntroduction of Reinforcement Learning So first of all, why is RL needed since we already have supervised/unsupervised learning? And how is RL different from the other methods?\nWikipedia\u0026rsquo;s definition of the term Machine Learning is: statistical algorithms that can learn from data and generaize to unseen data. Say we have data $X$, and its training data sample $x_i$, we are trying to learning the distribution about $X$.\nIn supervised learning, the data is pair of $\u0026lt;x_i, y_i\u0026gt;$, so we are trying to learn distribution $p(y_ | x_)$. For example, in image classification, given a image $x$, trying to predict its label $y$. If we viewed GPT as a special case of supervised learning, it can be seen as given a text sequence\u0026rsquo;s previous tokens, predict the next token.\nIn reinforcement learning, the $x$ is no longer static, or known before training. If in supervised learning the goal is to learn a mapping from $x_i$ to $y_i$, in RL it became to learn the mapping from situations to actions, both can only get through direct interaction with the environment.\nComponents of RL We defined reinforcement learning as a method to model the interaction in a game/strategy etc. There are two entities in such RL system:\n$Agent$ is the learner and decision-maker. $Environment$ is everything outside the agent that it interacts with. Then there are three types of interactions between agent and environment:\n$Action$ is an action the agent takes to interact with the environment. $State$ is a representation of the environment, which is changed by action. $Reward$ can be seen as the environment\u0026rsquo;s corresponding response to the agent based on its state. A sequence of interactions is:\nIn $t$, the environment\u0026rsquo;s state is $S_t$, it exhibits reward $R_t$ Based on $S_t$ and $R_t$, the agent takes action $A_t$ As a result of $A_t$, the state changed to $S_{t+1}$, and exhibits reward $R_{t+1}$ The goal of an RL system is to learn a strategy so the agent makes the best action in each step to maximize its rewards. We can define three components of such agent:\n$Policy$ is the strategy the agent used to determine its next action. $Value\\ Function$ is a function the agent used to evaluate its current state $Model$ is the way the environment interacts with the agent, it defined how the environment works. 2.1 Policy Policy is a mapping from current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nIn Stochastic Policy, the result is a ditribution, in which the agent sample its action from.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t) $\nIn Deterministic Policy, the result is a action, for a given state, the agent will surely execute a specific action.\n$a_{t} = argmax\\ \\pi(a|s_t)$\n2.2 Value Function In an RL system, the end goal is to find a strategy to win the game, like in Chess or Go. At any timestamp t, we wants to have a metric to evaluate the effectiveness of current strategy.\nThe reward $R_t$ defined above only refers to immediate reward at timestamp $t$, but not the overall winning chance. So we defined $V_t$ as the overall reward at timestamp $t$, which includes the current immediate reward $R_t$, and its expected future reward $V_{t+1}$.\n$V_t = R_t + \\gamma V_{t+1}$\nSo the reward at timestamp $t$ is decided by its current state $s_t$, its taken action $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssume we take $T$ steps from timestamp $0$ to $T-1$, all these steps form a trajectory $\\tau$, the sum reward can be represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.3 Model A $model$ defined how the environment interacts with the agent. A model is comprised of $\u0026lt;S, A, P, R\u0026gt;$\n$S$, the space of all possible states $A$, the space of all possible actions $P$, transformation function of how state changed, $P(s_{t+1}|s_t, a_t)$ $R$, how reward is calculated for each state, $R(s_t, a_t)$ If all these 4 elements are known, we can easily model the interaction without actual interaction, this is called model based learning.\nIn reallity, while $S$ and $A$ is often known, the transformation and reward part is either fully unknown or hard to estimate, so agent need to interact with real environment to observe the state and reward, this is called model-free learning.\nComparing the two, model-free learning relied on real interaction to get the next state and reward, while model based learning modeled these without specific interaction.\nOptimization of RL The optimization of a RL task can be divided into two parsts:\nValue estimation: given a strategy $\\pi$, evaluate the effectiveness of how does the strategy work, this is computing $V_\\pi$. Policy optimization: given the value function $V_\\pi$, optimize to get a better policy $\\pi$. In essence this is very similar to k-Means clustering that we have two optimization targets, and we opmitize them iteratively to get the optimial answer. In k-means,\nGiven the current cluster assignment of each point, we compute the optimal centroid. Similar to value estimation. Given the current optimial centroid, we find better cluster assignment for each point. Similar to policy optimization. But are the two steps both necessary in RL optimization? Answer is No.\n3.1 Value-based agent An agent can only learn the value function $V_\\pi$, it can maintain a table of mapping from $\u0026lt;S, A\u0026gt;$ to $V$, then in each step, it picked the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitely have a strategy or policy.\n$ a_t = argmax\\ V(a | s_t)$\n3.2 Policy-based agent A policy agent directly learns the policy, and each step it directly outputs the distribution of next action without knowing value function.\n$ a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as we described above.\nActor refers to learning of policy $\\pi$ Critic refers to learning of value function $V_\\pi$ 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitily by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Initialization: Intialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 if self.agent_pos == self.end: done, reward = True, 0 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\n7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"intuition\"\u003eIntuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about \u003ccode\u003eRLHF\u003c/code\u003e, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\u003c/p\u003e\n\u003cp\u003eHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"Intuition I\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about RLHF, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\nHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\nIntroduction of Reinforcement Learning So first of all, why is RL needed since we already have supervised/unsupervised learning? And how is RL different from the other methods?\nWikipedia\u0026rsquo;s definition of the term Machine Learning is: statistical algorithms that can learn from data and generaize to unseen data. Say we have data $X$, and its training data sample $x_i$, we are trying to learning the distribution about $X$.\nIn supervised learning, the data is pair of $\u0026lt;x_i, y_i\u0026gt;$, so we are trying to learn distribution $p(y_ | x_)$. For example, in image classification, given a image $x$, trying to predict its label $y$. If we viewed GPT as a special case of supervised learning, it can be seen as given a text sequence\u0026rsquo;s previous tokens, predict the next token.\nIn reinforcement learning, the $x$ is no longer static, or known before training. If in supervised learning the goal is to learn a mapping from $x_i$ to $y_i$, in RL it became to learn the mapping from situations to actions, both can only get through direct interaction with the environment.\nComponents of RL We defined reinforcement learning as a method to model the interaction in a game/strategy etc. There are two entities in such RL system:\n$Agent$ is the learner and decision-maker. $Environment$ is everything outside the agent that it interacts with. Then there are three types of interactions between agent and environment:\n$Action$ is an action the agent takes to interact with the environment. $State$ is a representation of the environment, which is changed by action. $Reward$ can be seen as the environment\u0026rsquo;s corresponding response to the agent based on its state. A sequence of interactions is:\nIn $t$, the environment\u0026rsquo;s state is $S_t$, it exhibits reward $R_t$ Based on $S_t$ and $R_t$, the agent takes action $A_t$ As a result of $A_t$, the state changed to $S_{t+1}$, and exhibits reward $R_{t+1}$ The goal of an RL system is to learn a strategy so the agent makes the best action in each step to maximize its rewards. We can define three components of such agent:\n$Policy$ is the strategy the agent used to determine its next action. $Value\\ Function$ is a function the agent used to evaluate its current state $Model$ is the way the environment interacts with the agent, it defined how the environment works. 2.1 Policy Policy is a mapping from current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nIn Stochastic Policy, the result is a ditribution, in which the agent sample its action from.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t) $\nIn Deterministic Policy, the result is a action, for a given state, the agent will surely execute a specific action.\n$a_{t} = argmax\\ \\pi(a|s_t)$\n2.2 Value Function In an RL system, the end goal is to find a strategy to win the game, like in Chess or Go. At any timestamp t, we wants to have a metric to evaluate the effectiveness of current strategy.\nThe reward $R_t$ defined above only refers to immediate reward at timestamp $t$, but not the overall winning chance. So we defined $V_t$ as the overall reward at timestamp $t$, which includes the current immediate reward $R_t$, and its expected future reward $V_{t+1}$.\n$V_t = R_t + \\gamma V_{t+1}$\nSo the reward at timestamp $t$ is decided by its current state $s_t$, its taken action $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssume we take $T$ steps from timestamp $0$ to $T-1$, all these steps form a trajectory $\\tau$, the sum reward can be represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.3 Model A $model$ defined how the environment interacts with the agent. A model is comprised of $\u0026lt;S, A, P, R\u0026gt;$\n$S$, the space of all possible states $A$, the space of all possible actions $P$, transformation function of how state changed, $P(s_{t+1}|s_t, a_t)$ $R$, how reward is calculated for each state, $R(s_t, a_t)$ If all these 4 elements are known, we can easily model the interaction without actual interaction, this is called model based learning.\nIn reallity, while $S$ and $A$ is often known, the transformation and reward part is either fully unknown or hard to estimate, so agent need to interact with real environment to observe the state and reward, this is called model-free learning.\nComparing the two, model-free learning relied on real interaction to get the next state and reward, while model based learning modeled these without specific interaction.\nOptimization of RL The optimization of a RL task can be divided into two parsts:\nValue estimation: given a strategy $\\pi$, evaluate the effectiveness of how does the strategy work, this is computing $V_\\pi$. Policy optimization: given the value function $V_\\pi$, optimize to get a better policy $\\pi$. In essence this is very similar to k-Means clustering that we have two optimization targets, and we opmitize them iteratively to get the optimial answer. In k-means,\nGiven the current cluster assignment of each point, we compute the optimal centroid. Similar to value estimation. Given the current optimial centroid, we find better cluster assignment for each point. Similar to policy optimization. But are the two steps both necessary in RL optimization? Answer is No.\n3.1 Value-based agent An agent can only learn the value function $V_\\pi$, it can maintain a table of mapping from $\u0026lt;S, A\u0026gt;$ to $V$, then in each step, it picked the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitely have a strategy or policy.\n$ a_t = argmax\\ V(a | s_t)$\n3.2 Policy-based agent A policy agent directly learns the policy, and each step it directly outputs the distribution of next action without knowing value function.\n$ a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as we described above.\nActor refers to learning of policy $\\pi$ Critic refers to learning of value function $V_\\pi$ 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitily by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Initialization: Intialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 if self.agent_pos == self.end: done, reward = True, 0 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\n7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"intuition\"\u003eIntuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about \u003ccode\u003eRLHF\u003c/code\u003e, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\u003c/p\u003e\n\u003cp\u003eHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"Intuition I\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about RLHF, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\nHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\nIntroduction of Reinforcement Learning So first of all, why is RL needed since we already have supervised/unsupervised learning? And how is RL different from the other methods?\nWikipedia\u0026rsquo;s definition of the term Machine Learning is: statistical algorithms that can learn from data and generaize to unseen data. Say we have data $X$, and its training data sample $x_i$, we are trying to learning the distribution about $X$.\nIn supervised learning, the data is pair of $\u0026lt;x_i, y_i\u0026gt;$, so we are trying to learn distribution $p(y_ | x_)$. For example, in image classification, given a image $x$, trying to predict its label $y$. If we viewed GPT as a special case of supervised learning, it can be seen as given a text sequence\u0026rsquo;s previous tokens, predict the next token.\nIn reinforcement learning, the $x$ is no longer static, or known before training. If in supervised learning the goal is to learn a mapping from $x_i$ to $y_i$, in RL it became to learn the mapping from situations to actions, both can only get through direct interaction with the environment.\nComponents of RL We defined reinforcement learning as a method to model the interaction in a game/strategy etc. There are two entities in such RL system:\n$Agent$ is the learner and decision-maker. $Environment$ is everything outside the agent that it interacts with. Then there are three types of interactions between agent and environment:\n$Action$ is an action the agent takes to interact with the environment. $State$ is a representation of the environment, which is changed by action. $Reward$ can be seen as the environment\u0026rsquo;s corresponding response to the agent based on its state. A sequence of interactions is:\nIn $t$, the environment\u0026rsquo;s state is $S_t$, it exhibits reward $R_t$ Based on $S_t$ and $R_t$, the agent takes action $A_t$ As a result of $A_t$, the state changed to $S_{t+1}$, and exhibits reward $R_{t+1}$ The goal of an RL system is to learn a strategy so the agent makes the best action in each step to maximize its rewards. We can define three components of such agent:\n$Policy$ is the strategy the agent used to determine its next action. $Value\\ Function$ is a function the agent used to evaluate its current state $Model$ is the way the environment interacts with the agent, it defined how the environment works. 2.1 Policy Policy is a mapping from current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nIn Stochastic Policy, the result is a ditribution, in which the agent sample its action from.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t) $\nIn Deterministic Policy, the result is a action, for a given state, the agent will surely execute a specific action.\n$a_{t} = argmax\\ \\pi(a|s_t)$\n2.2 Value Function In an RL system, the end goal is to find a strategy to win the game, like in Chess or Go. At any timestamp t, we wants to have a metric to evaluate the effectiveness of current strategy.\nThe reward $R_t$ defined above only refers to immediate reward at timestamp $t$, but not the overall winning chance. So we defined $V_t$ as the overall reward at timestamp $t$, which includes the current immediate reward $R_t$, and its expected future reward $V_{t+1}$.\n$V_t = R_t + \\gamma V_{t+1}$\nSo the reward at timestamp $t$ is decided by its current state $s_t$, its taken action $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssume we take $T$ steps from timestamp $0$ to $T-1$, all these steps form a trajectory $\\tau$, the sum reward can be represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.3 Model A $model$ defined how the environment interacts with the agent. A model is comprised of $\u0026lt;S, A, P, R\u0026gt;$\n$S$, the space of all possible states $A$, the space of all possible actions $P$, transformation function of how state changed, $P(s_{t+1}|s_t, a_t)$ $R$, how reward is calculated for each state, $R(s_t, a_t)$ If all these 4 elements are known, we can easily model the interaction without actual interaction, this is called model based learning.\nIn reallity, while $S$ and $A$ is often known, the transformation and reward part is either fully unknown or hard to estimate, so agent need to interact with real environment to observe the state and reward, this is called model-free learning.\nComparing the two, model-free learning relied on real interaction to get the next state and reward, while model based learning modeled these without specific interaction.\nOptimization of RL The optimization of a RL task can be divided into two parsts:\nValue estimation: given a strategy $\\pi$, evaluate the effectiveness of how does the strategy work, this is computing $V_\\pi$. Policy optimization: given the value function $V_\\pi$, optimize to get a better policy $\\pi$. In essence this is very similar to k-Means clustering that we have two optimization targets, and we opmitize them iteratively to get the optimial answer. In k-means,\nGiven the current cluster assignment of each point, we compute the optimal centroid. Similar to value estimation. Given the current optimial centroid, we find better cluster assignment for each point. Similar to policy optimization. But are the two steps both necessary in RL optimization? Answer is No.\n3.1 Value-based agent An agent can only learn the value function $V_\\pi$, it can maintain a table of mapping from $\u0026lt;S, A\u0026gt;$ to $V$, then in each step, it picked the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitely have a strategy or policy.\n$ a_t = argmax\\ V(a | s_t)$\n3.2 Policy-based agent A policy agent directly learns the policy, and each step it directly outputs the distribution of next action without knowing value function.\n$ a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as we described above.\nActor refers to learning of policy $\\pi$ Critic refers to learning of value function $V_\\pi$ 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitily by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Initialization: Intialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 if self.agent_pos == self.end: done, reward = True, 0 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\n7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"intuition\"\u003eIntuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about \u003ccode\u003eRLHF\u003c/code\u003e, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\u003c/p\u003e\n\u003cp\u003eHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"Intuition I\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about RLHF, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\nHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\nIntroduction of Reinforcement Learning So first of all, why is RL needed since we already have supervised/unsupervised learning? And how is RL different from the other methods?\nWikipedia\u0026rsquo;s definition of the term Machine Learning is: statistical algorithms that can learn from data and generaize to unseen data. Say we have data $X$, and its training data sample $x_i$, we are trying to learning the distribution about $X$.\nIn supervised learning, the data is pair of $\u0026lt;x_i, y_i\u0026gt;$, so we are trying to learn distribution $p(y_ | x_)$. For example, in image classification, given a image $x$, trying to predict its label $y$. If we viewed GPT as a special case of supervised learning, it can be seen as given a text sequence\u0026rsquo;s previous tokens, predict the next token.\nIn reinforcement learning, the $x$ is no longer static, or known before training. If in supervised learning the goal is to learn a mapping from $x_i$ to $y_i$, in RL it became to learn the mapping from situations to actions, both can only get through direct interaction with the environment.\nComponents of RL We defined reinforcement learning as a method to model the interaction in a game/strategy etc. There are two entities in such RL system:\n$Agent$ is the learner and decision-maker. $Environment$ is everything outside the agent that it interacts with. Then there are three types of interactions between agent and environment:\n$Action$ is an action the agent takes to interact with the environment. $State$ is a representation of the environment, which is changed by action. $Reward$ can be seen as the environment\u0026rsquo;s corresponding response to the agent based on its state. A sequence of interactions is:\nIn $t$, the environment\u0026rsquo;s state is $S_t$, it exhibits reward $R_t$ Based on $S_t$ and $R_t$, the agent takes action $A_t$ As a result of $A_t$, the state changed to $S_{t+1}$, and exhibits reward $R_{t+1}$ The goal of an RL system is to learn a strategy so the agent makes the best action in each step to maximize its rewards. We can define three components of such agent:\n$Policy$ is the strategy the agent used to determine its next action. $Value\\ Function$ is a function the agent used to evaluate its current state $Model$ is the way the environment interacts with the agent, it defined how the environment works. 2.1 Policy Policy is a mapping from current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nIn Stochastic Policy, the result is a ditribution, in which the agent sample its action from.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t) $\nIn Deterministic Policy, the result is a action, for a given state, the agent will surely execute a specific action.\n$a_{t} = argmax\\ \\pi(a|s_t)$\n2.2 Value Function In an RL system, the end goal is to find a strategy to win the game, like in Chess or Go. At any timestamp t, we wants to have a metric to evaluate the effectiveness of current strategy.\nThe reward $R_t$ defined above only refers to immediate reward at timestamp $t$, but not the overall winning chance. So we defined $V_t$ as the overall reward at timestamp $t$, which includes the current immediate reward $R_t$, and its expected future reward $V_{t+1}$.\n$V_t = R_t + \\gamma V_{t+1}$\nSo the reward at timestamp $t$ is decided by its current state $s_t$, its taken action $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssume we take $T$ steps from timestamp $0$ to $T-1$, all these steps form a trajectory $\\tau$, the sum reward can be represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.3 Model A $model$ defined how the environment interacts with the agent. A model is comprised of $\u0026lt;S, A, P, R\u0026gt;$\n$S$, the space of all possible states $A$, the space of all possible actions $P$, transformation function of how state changed, $P(s_{t+1}|s_t, a_t)$ $R$, how reward is calculated for each state, $R(s_t, a_t)$ If all these 4 elements are known, we can easily model the interaction without actual interaction, this is called model based learning.\nIn reallity, while $S$ and $A$ is often known, the transformation and reward part is either fully unknown or hard to estimate, so agent need to interact with real environment to observe the state and reward, this is called model-free learning.\nComparing the two, model-free learning relied on real interaction to get the next state and reward, while model based learning modeled these without specific interaction.\nOptimization of RL The optimization of a RL task can be divided into two parsts:\nValue estimation: given a strategy $\\pi$, evaluate the effectiveness of how does the strategy work, this is computing $V_\\pi$. Policy optimization: given the value function $V_\\pi$, optimize to get a better policy $\\pi$. In essence this is very similar to k-Means clustering that we have two optimization targets, and we opmitize them iteratively to get the optimial answer. In k-means,\nGiven the current cluster assignment of each point, we compute the optimal centroid. Similar to value estimation. Given the current optimial centroid, we find better cluster assignment for each point. Similar to policy optimization. But are the two steps both necessary in RL optimization? Answer is No.\n3.1 Value-based agent An agent can only learn the value function $V_\\pi$, it can maintain a table of mapping from $\u0026lt;S, A\u0026gt;$ to $V$, then in each step, it picked the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitely have a strategy or policy.\n$ a_t = argmax\\ V(a | s_t)$\n3.2 Policy-based agent A policy agent directly learns the policy, and each step it directly outputs the distribution of next action without knowing value function.\n$ a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as we described above.\nActor refers to learning of policy $\\pi$ Critic refers to learning of value function $V_\\pi$ 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitily by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Initialization: Intialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 if self.agent_pos == self.end: done, reward = True, 0 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\n7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"intuition\"\u003eIntuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about \u003ccode\u003eRLHF\u003c/code\u003e, but often felt it\u0026rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners.\u003c/p\u003e\n\u003cp\u003eHowever, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after there are many recent progress in LLM posttraining using RL. However, the mathematical notions and various concepts used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. For example, in traditional ML we only talked about model, data, loss function. But in RL you heard a lot of words like on-policy, reward, model-free, agent etc.\nRecently I finally found some spare time to learn through it, and fortunately the famous Richard Sutton\u0026rsquo;s \u0026lt;Reinforcement Learning: an Introduction\u0026gt; book is surprisingly intuitive even for RL beginners. I want to summarize and share my learning here.\nIn this article I want to focus on explaining some of the basic concepts and elements in RL. I will solve the RL problem of cliff walking using 3 classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, 3. Temporal-Difference Learning. This mainly covers content in chapter 1-6 in Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning So first of all, why is RL needed since we already have supervised/unsupervised learning? And how is RL different from the other methods?\nWikipedia\u0026rsquo;s definition of the term Machine Learning is: statistical algorithms that can learn from data and generaize to unseen data. Say we have data $X$, and its training data sample $x_i$, we are trying to learning the distribution about $X$.\nIn supervised learning, the data is pair of $\u0026lt;x_i, y_i\u0026gt;$, so we are trying to learn distribution $p(y_ | x_)$. For example, in image classification, given a image $x$, trying to predict its label $y$. If we viewed GPT as a special case of supervised learning, it can be seen as given a text sequence\u0026rsquo;s previous tokens, predict the next token.\nIn reinforcement learning, the $x$ is no longer static, or known before training. If in supervised learning the goal is to learn a mapping from $x_i$ to $y_i$, in RL it became to learn the mapping from situations to actions, both can only get through direct interaction with the environment.\nComponents of RL We defined reinforcement learning as a method to model the interaction in a game/strategy etc. There are two entities in such RL system:\n$Agent$ is the learner and decision-maker. $Environment$ is everything outside the agent that it interacts with. Then there are three types of interactions between agent and environment:\n$Action$ is an action the agent takes to interact with the environment. $State$ is a representation of the environment, which is changed by action. $Reward$ can be seen as the environment\u0026rsquo;s corresponding response to the agent based on its state. A sequence of interactions is:\nIn $t$, the environment\u0026rsquo;s state is $S_t$, it exhibits reward $R_t$ Based on $S_t$ and $R_t$, the agent takes action $A_t$ As a result of $A_t$, the state changed to $S_{t+1}$, and exhibits reward $R_{t+1}$ The goal of an RL system is to learn a strategy so the agent makes the best action in each step to maximize its rewards. We can define three components of such agent:\n$Policy$ is the strategy the agent used to determine its next action. $Value\\ Function$ is a function the agent used to evaluate its current state $Model$ is the way the environment interacts with the agent, it defined how the environment works. 2.1 Policy Policy is a mapping from current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nIn Stochastic Policy, the result is a ditribution, in which the agent sample its action from.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t) $\nIn Deterministic Policy, the result is a action, for a given state, the agent will surely execute a specific action.\n$a_{t} = argmax\\ \\pi(a|s_t)$\n2.2 Value Function In an RL system, the end goal is to find a strategy to win the game, like in Chess or Go. At any timestamp t, we wants to have a metric to evaluate the effectiveness of current strategy.\nThe reward $R_t$ defined above only refers to immediate reward at timestamp $t$, but not the overall winning chance. So we defined $V_t$ as the overall reward at timestamp $t$, which includes the current immediate reward $R_t$, and its expected future reward $V_{t+1}$.\n$V_t = R_t + \\gamma V_{t+1}$\nSo the reward at timestamp $t$ is decided by its current state $s_t$, its taken action $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssume we take $T$ steps from timestamp $0$ to $T-1$, all these steps form a trajectory $\\tau$, the sum reward can be represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.3 Model A $model$ defined how the environment interacts with the agent. A model is comprised of $\u0026lt;S, A, P, R\u0026gt;$\n$S$, the space of all possible states $A$, the space of all possible actions $P$, transformation function of how state changed, $P(s_{t+1}|s_t, a_t)$ $R$, how reward is calculated for each state, $R(s_t, a_t)$ If all these 4 elements are known, we can easily model the interaction without actual interaction, this is called model based learning.\nIn reallity, while $S$ and $A$ is often known, the transformation and reward part is either fully unknown or hard to estimate, so agent need to interact with real environment to observe the state and reward, this is called model-free learning.\nComparing the two, model-free learning relied on real interaction to get the next state and reward, while model based learning modeled these without specific interaction.\nOptimization of RL The optimization of a RL task can be divided into two parsts:\nValue estimation: given a strategy $\\pi$, evaluate the effectiveness of how does the strategy work, this is computing $V_\\pi$. Policy optimization: given the value function $V_\\pi$, optimize to get a better policy $\\pi$. In essence this is very similar to k-Means clustering that we have two optimization targets, and we opmitize them iteratively to get the optimial answer. In k-means,\nGiven the current cluster assignment of each point, we compute the optimal centroid. Similar to value estimation. Given the current optimial centroid, we find better cluster assignment for each point. Similar to policy optimization. But are the two steps both necessary in RL optimization? Answer is No.\n3.1 Value-based agent An agent can only learn the value function $V_\\pi$, it can maintain a table of mapping from $\u0026lt;S, A\u0026gt;$ to $V$, then in each step, it picked the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitely have a strategy or policy.\n$ a_t = argmax\\ V(a | s_t)$\n3.2 Policy-based agent A policy agent directly learns the policy, and each step it directly outputs the distribution of next action without knowing value function.\n$ a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as we described above.\nActor refers to learning of policy $\\pi$ Critic refers to learning of value function $V_\\pi$ 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitily by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Initialization: Intialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 if self.agent_pos == self.end: done, reward = True, 0 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\n7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after there are many recent progress in LLM posttraining using RL. However, the mathematical notions and various concepts used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. For example, in traditional ML we only talked about \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, \u003ccode\u003eloss function\u003c/code\u003e. But in RL you heard a lot of words like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, \u003ccode\u003eagent\u003c/code\u003e etc.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning So first of all, why is RL needed since we already have supervised/unsupervised learning? And how is RL different from the other methods?\nWikipedia\u0026rsquo;s definition of the term Machine Learning is: statistical algorithms that can learn from data and generaize to unseen data. Say we have data $X$, and its training data sample $x_i$, we are trying to learning the distribution about $X$.\nIn supervised learning, the data is pair of $\u0026lt;x_i, y_i\u0026gt;$, so we are trying to learn distribution $p(y_ | x_)$. For example, in image classification, given a image $x$, trying to predict its label $y$. If we viewed GPT as a special case of supervised learning, it can be seen as given a text sequence\u0026rsquo;s previous tokens, predict the next token.\nIn reinforcement learning, the $x$ is no longer static, or known before training. If in supervised learning the goal is to learn a mapping from $x_i$ to $y_i$, in RL it became to learn the mapping from situations to actions, both can only get through direct interaction with the environment.\nComponents of RL We defined reinforcement learning as a method to model the interaction in a game/strategy etc. There are two entities in such RL system:\n$Agent$ is the learner and decision-maker. $Environment$ is everything outside the agent that it interacts with. Then there are three types of interactions between agent and environment:\n$Action$ is an action the agent takes to interact with the environment. $State$ is a representation of the environment, which is changed by action. $Reward$ can be seen as the environment\u0026rsquo;s corresponding response to the agent based on its state. A sequence of interactions is:\nIn $t$, the environment\u0026rsquo;s state is $S_t$, it exhibits reward $R_t$ Based on $S_t$ and $R_t$, the agent takes action $A_t$ As a result of $A_t$, the state changed to $S_{t+1}$, and exhibits reward $R_{t+1}$ The goal of an RL system is to learn a strategy so the agent makes the best action in each step to maximize its rewards. We can define three components of such agent:\n$Policy$ is the strategy the agent used to determine its next action. $Value\\ Function$ is a function the agent used to evaluate its current state $Model$ is the way the environment interacts with the agent, it defined how the environment works. 2.1 Policy Policy is a mapping from current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nIn Stochastic Policy, the result is a ditribution, in which the agent sample its action from.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t) $\nIn Deterministic Policy, the result is a action, for a given state, the agent will surely execute a specific action.\n$a_{t} = argmax\\ \\pi(a|s_t)$\n2.2 Value Function In an RL system, the end goal is to find a strategy to win the game, like in Chess or Go. At any timestamp t, we wants to have a metric to evaluate the effectiveness of current strategy.\nThe reward $R_t$ defined above only refers to immediate reward at timestamp $t$, but not the overall winning chance. So we defined $V_t$ as the overall reward at timestamp $t$, which includes the current immediate reward $R_t$, and its expected future reward $V_{t+1}$.\n$V_t = R_t + \\gamma V_{t+1}$\nSo the reward at timestamp $t$ is decided by its current state $s_t$, its taken action $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssume we take $T$ steps from timestamp $0$ to $T-1$, all these steps form a trajectory $\\tau$, the sum reward can be represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.3 Model A $model$ defined how the environment interacts with the agent. A model is comprised of $\u0026lt;S, A, P, R\u0026gt;$\n$S$, the space of all possible states $A$, the space of all possible actions $P$, transformation function of how state changed, $P(s_{t+1}|s_t, a_t)$ $R$, how reward is calculated for each state, $R(s_t, a_t)$ If all these 4 elements are known, we can easily model the interaction without actual interaction, this is called model based learning.\nIn reallity, while $S$ and $A$ is often known, the transformation and reward part is either fully unknown or hard to estimate, so agent need to interact with real environment to observe the state and reward, this is called model-free learning.\nComparing the two, model-free learning relied on real interaction to get the next state and reward, while model based learning modeled these without specific interaction.\nOptimization of RL The optimization of a RL task can be divided into two parsts:\nValue estimation: given a strategy $\\pi$, evaluate the effectiveness of how does the strategy work, this is computing $V_\\pi$. Policy optimization: given the value function $V_\\pi$, optimize to get a better policy $\\pi$. In essence this is very similar to k-Means clustering that we have two optimization targets, and we opmitize them iteratively to get the optimial answer. In k-means,\nGiven the current cluster assignment of each point, we compute the optimal centroid. Similar to value estimation. Given the current optimial centroid, we find better cluster assignment for each point. Similar to policy optimization. But are the two steps both necessary in RL optimization? Answer is No.\n3.1 Value-based agent An agent can only learn the value function $V_\\pi$, it can maintain a table of mapping from $\u0026lt;S, A\u0026gt;$ to $V$, then in each step, it picked the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitely have a strategy or policy.\n$ a_t = argmax\\ V(a | s_t)$\n3.2 Policy-based agent A policy agent directly learns the policy, and each step it directly outputs the distribution of next action without knowing value function.\n$ a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as we described above.\nActor refers to learning of policy $\\pi$ Critic refers to learning of value function $V_\\pi$ 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 if self.agent_pos == self.end: done, reward = True, 0 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\n7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn reinforcement learning, $x$ is no longer static or known before training. While supervised learning aims to learn a mapping from $x_i$ to $y_i$, RL focuses on learning the mapping from situations to actions through direct interaction with the environment.\nComponents of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $S_t$, providing reward $R_t$. Based on $S_t$ and $R_t$, the agent takes action $A_t$. As a result of $A_t$, the state changes to $S_{t+1}$, providing reward $R_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current strategy.\nThe reward $R_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = R_t + \\gamma V_{t+1}$\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\nOptimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n3.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n3.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nTODO: In unsupervisde learning,\nIn reinforcement learning, $x$ is no longer static or known before training. While supervised learning aims to learn a mapping from $x_i$ to $y_i$, RL focuses on learning the mapping from situations to actions through direct interaction with the environment.\nTODO: explain more clearly, with examples, and why supervised learning won\u0026rsquo;t work\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nTODO: expalin gamma, also conside why we need it, can use examples of financial like expected return of bank deposit.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. This can involve clustering data points into groups based on similarity, as in k-means clustering, or reducing dimensionality to capture the most important features, as in principal component analysis (PCA). Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, $x$ is no longer static or known before training. While supervised learning aims to learn a mapping from $x_i$ to $y_i$, RL focuses on learning the mapping from situations to actions through direct interaction with the environment.\nTODO: explain more clearly, with examples, and why supervised learning won\u0026rsquo;t work\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nTODO: expalin gamma, also conside why we need it, can use examples of financial like expected return of bank deposit.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, $x$ is no longer static or known before training. While supervised learning aims to learn a mapping from $x_i$ to $y_i$, RL focuses on learning the mapping from situations to actions through direct interaction with the environment.\nTODO: explain more clearly, with examples, and why supervised learning won\u0026rsquo;t work\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nTODO: expalin gamma, also conside why we need it, can use examples of financial like expected return of bank deposit.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nTODO: expalin gamma, also conside why we need it, can use examples of financial like expected return of bank deposit.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nFor example, consider the expected return of a bank deposit. If you have a deposit that yields interest over time, the immediate reward is the interest earned in the first period, while future rewards are the interest earned in subsequent periods. A higher discount factor would mean valuing the future interest more, akin to valuing long-term gains in reinforcement learning. This approach helps in making decisions that are beneficial in the long run rather than focusing solely on immediate gains.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n3.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 4. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Now let\u0026rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton \u0026amp; Barto’s book, “Reinforcement Learning: An Introduction.”\n4.1 Problem Statement: The world is represented as a 4×12 grid world. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Now let\u0026rsquo;s map this problem statement to different components of RL system.\n$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards. $Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic. $State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids. $Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, UP, DOWN, LEFT, RIGHT. $Reward$, a scalar signal from the environment for the agent\u0026rsquo;s each move. We can define it as: -1 for each normal move. -100 if the agent fells into the cliff. 0 upon reaching the goal location. $Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, (3, 0) -\u0026gt; MOVE UP. The end goal for this problem is to find such a policy. 4.1.1 Define the Environemnt We can define the environment as following.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Then we can define the cells for start, end and cliff cells, and initialize a environment.\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check github for visualization code. env.render_plot() 4.1.2 Define the Step function Next, we define rules and rewards for the agent\u0026rsquo;s action.\nIn self.actions we defined 4 type of actions, each representing walking 1 step in the direction. The step function takes in a current location (i, j) and action index, execute it and returns a tuple representing: The agents new position after the action Reward of current action def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The moving rules and rewards are:\nAgent received -1 for each normal move. Agent received -100 if fell off the cliff. Agent received 0 if reaching the goal. If the agent moved out of the grid, it stayed still in the same grid and still received -1. Now we introduced the environment setup of the cliff walking problem, now let\u0026rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.\n5. Dynamic Programming Methods 5.1 Introduction 5.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n5.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Let\u0026rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton \u0026amp; Barto’s book, Reinforcement Learning: An Introduction.\n3.1 Problem Statement: The world is represented as a 4×12 grid. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Mapping this problem to RL components:\nAgent: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards. Environment: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic. State: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states. Action: The action space consists of four possible moves: UP, DOWN, LEFT, RIGHT. Reward: A scalar signal from the environment for each move: -1 for each normal move. -100 if the agent falls into the cliff. 0 upon reaching the goal. Policy: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., (3, 0) -\u0026gt; MOVE UP. The objective is to discover such a policy. 3.1.1 Define the Environment The environment can be defined as follows:\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Initialize the environment with specified start, end, and cliff cells:\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check GitHub for visualization code. env.render_plot() 3.1.2 Define the Step Function Next, define the rules and rewards for the agent\u0026rsquo;s actions:\nself.actions defines four types of actions, each representing a step in a direction. The step function takes the current location (i, j) and action index, executes it, and returns a tuple representing: The agent\u0026rsquo;s new position after the action. The reward for the current action. def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The movement rules and rewards are:\nThe agent receives -1 for each normal move. The agent receives -100 if it falls off the cliff. The agent receives 0 upon reaching the goal. If the agent moves out of the grid, it remains in the same cell and still receives -1. With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.\n4. Dynamic Programming Methods 4.1 Introduction 4.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n4.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n4.2 Value Iteration 4.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 4.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 4.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 4.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n4.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 4.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 4.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 5. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n5.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 5.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n5.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 5.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n5.3 Python Implementation of MC in Cliff Walking 5.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 5.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 5.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 5.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 5.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 6. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 6.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n6.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 6.1.2 Python Implementation of SARSA 6.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 6.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 6.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n6.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 6.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 6.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n7. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Let\u0026rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton \u0026amp; Barto’s book, Reinforcement Learning: An Introduction.\n3.1 Problem Statement: The world is represented as a 4×12 grid. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Mapping this problem to RL components:\nAgent: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards. Environment: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic. State: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states. Action: The action space consists of four possible moves: UP, DOWN, LEFT, RIGHT. Reward: A scalar signal from the environment for each move: -1 for each normal move. -100 if the agent falls into the cliff. 0 upon reaching the goal. Policy: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., (3, 0) -\u0026gt; MOVE UP. The objective is to discover such a policy. 3.1.1 Define the Environment The environment can be defined as follows:\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Initialize the environment with specified start, end, and cliff cells:\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check GitHub for visualization code. env.render_plot() 3.1.2 Define the Step Function Next, define the rules and rewards for the agent\u0026rsquo;s actions:\nself.actions defines four types of actions, each representing a step in a direction. The step function takes the current location (i, j) and action index, executes it, and returns a tuple representing: The agent\u0026rsquo;s new position after the action. The reward for the current action. def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The movement rules and rewards are:\nThe agent receives -1 for each normal move. The agent receives -100 if it falls off the cliff. The agent receives 0 upon reaching the goal. If the agent moves out of the grid, it remains in the same cell and still receives -1. With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.\n4. Dynamic Programming Methods 4.1 Introduction 4.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n4.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n4.2 Value Iteration 4.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 4.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 4.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 4.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n4.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 4.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 4.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 5. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n5.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 5.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n5.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 5.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n5.3 Python Implementation of MC in Cliff Walking 5.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 5.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 5.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 5.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 5.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 6. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 6.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n6.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 6.1.2 Python Implementation of SARSA 6.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 6.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 6.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n6.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 6.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 6.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n7. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Let\u0026rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton \u0026amp; Barto’s book, Reinforcement Learning: An Introduction.\n3.1 Problem Statement: The world is represented as a 4×12 grid. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Mapping this problem to RL components:\nAgent: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards. Environment: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic. State: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states. Action: The action space consists of four possible moves: UP, DOWN, LEFT, RIGHT. Reward: A scalar signal from the environment for each move: -1 for each normal move. -100 if the agent falls into the cliff. 0 upon reaching the goal. Policy: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., (3, 0) -\u0026gt; MOVE UP. The objective is to discover such a policy. 3.1.1 Define the Environment The environment can be defined as follows:\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.reset() def reset(self): self.agent_pos = self.start return self.agent_pos Initialize the environment with specified start, end, and cliff cells:\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check GitHub for visualization code. env.render_plot() 3.1.2 Define the Step Function Next, define the rules and rewards for the agent\u0026rsquo;s actions:\nself.actions defines four types of actions, each representing a step in a direction. The step function takes the current location (i, j) and action index, executes it, and returns a tuple representing: The agent\u0026rsquo;s new position after the action. The reward for the current action. def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The movement rules and rewards are:\nThe agent receives -1 for each normal move. The agent receives -100 if it falls off the cliff. The agent receives 0 upon reaching the goal. If the agent moves out of the grid, it remains in the same cell and still receives -1. With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.\n4. Dynamic Programming Methods 4.1 Introduction 4.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n4.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n4.2 Value Iteration 4.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 4.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 4.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 4.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n4.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 4.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 4.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 5. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n5.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 5.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n5.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 5.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n5.3 Python Implementation of MC in Cliff Walking 5.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 5.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 5.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 5.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 5.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 6. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 6.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n6.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 6.1.2 Python Implementation of SARSA 6.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 6.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 6.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n6.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 6.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 6.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n7. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Let\u0026rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton \u0026amp; Barto’s book, Reinforcement Learning: An Introduction.\n3.1 Problem Statement: The world is represented as a 4×12 grid. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Mapping this problem to RL components:\nAgent: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards. Environment: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic. State: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states. Action: The action space consists of four possible moves: UP, DOWN, LEFT, RIGHT. Reward: A scalar signal from the environment for each move: -1 for each normal move. -100 if the agent falls into the cliff. 0 upon reaching the goal. Policy: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., (3, 0) -\u0026gt; MOVE UP. The objective is to discover such a policy. 3.2 Python Implementation 3.2.1 Cliff Walking Environment The environment can be defined as follows:\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right Initialize the environment with specified start, end, and cliff cells:\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check GitHub for visualization code. env.render_plot() 3.2.2 Define the Step Function Next, define the rules and rewards for the agent\u0026rsquo;s actions:\nself.actions defines four types of actions, each representing a step in a direction. The step function takes the current location (i, j) and action index, executes it, and returns a tuple representing: The agent\u0026rsquo;s new position after the action. The reward for the current action. def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The movement rules and rewards are:\nThe agent receives -1 for each normal move. The agent receives -100 if it falls off the cliff. The agent receives 0 upon reaching the goal. If the agent moves out of the grid, it remains in the same cell and still receives -1. With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.\n4. Dynamic Programming Methods 4.1 Introduction 4.1.1 Markov Property A Markov decision process(MDP) is defined by 5 elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|st, at)$ $R$: reward function immediate reward after a transition, $r(s_{t+1},st, at)$ $\\gamma$: discount factor that weights the importance of future reward. We define a deicision process has Markov Property if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the markovian propterty.\n4.1.2 Bellman Optimal Function We define the optimal value function is: $$ V^{}(s) = maxV_\\pi(s) $$ Here we searched a policy $\\pi$ to maximize the state $V$, the result policy is our optimal policy. $$ \\pi^{}(s) = argmaxV_\\pi(s) $$ For each state $s$, we searched over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, a = argmaxQ^(s, a) $$\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Let\u0026rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton \u0026amp; Barto’s book, Reinforcement Learning: An Introduction.\n3.1 Problem Statement: The world is represented as a 4×12 grid. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Mapping this problem to RL components:\nAgent: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards. Environment: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic. State: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states. Action: The action space consists of four possible moves: UP, DOWN, LEFT, RIGHT. Reward: A scalar signal from the environment for each move: -1 for each normal move. -100 if the agent falls into the cliff. 0 upon reaching the goal. Policy: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., (3, 0) -\u0026gt; MOVE UP. The objective is to discover such a policy. 3.2 Python Implementation 3.2.1 Cliff Walking Environment The environment can be defined as follows:\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right Initialize the environment with specified start, end, and cliff cells:\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check GitHub for visualization code. env.render_plot() 3.2.2 Define the Step Function Next, define the rules and rewards for the agent\u0026rsquo;s actions:\nself.actions defines four types of actions, each representing a step in a direction. The step function takes the current location (i, j) and action index, executes it, and returns a tuple representing: The agent\u0026rsquo;s new position after the action. The reward for the current action. def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The movement rules and rewards are:\nThe agent receives -1 for each normal move. The agent receives -100 if it falls off the cliff. The agent receives 0 upon reaching the goal. If the agent moves out of the grid, it remains in the same cell and still receives -1. With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.\n4. Dynamic Programming Methods 4.1 Introduction Dynamic programming (DP) is a method used in reinforcement learning to solve Markov decision processes (MDPs) by breaking them down into simpler subproblems. It works by iteratively improving the value function, which estimates the expected return of states, and deriving an optimal policy from these values. DP requires a complete model of the environment, including the transition probabilities and reward functions.\n4.1.1 Markov Property A Markov decision process (MDP) is defined by five elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|s_t, a_t)$ $R$: reward function immediate reward after a transition, $r(s_{t+1}, s_t, a_t)$ $\\gamma$: discount factor that weights the importance of future rewards A decision process has the Markov Property if its next state and reward depend only on its current state and action, not the full history. The cliff walking problem satisfies the Markovian property.\n4.1.2 Bellman Optimal Function The Bellman optimality equation defines the optimal value function as: $$ V^{}(s) = \\max V_\\pi(s) $$ Here, we search for a policy $\\pi$ that maximizes the value of state $V$. The resulting policy is our optimal policy. $$ \\pi^{}(s) = \\arg\\max V_\\pi(s) $$ For each state $s$, we search over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, \\text{ if } a = \\arg\\max Q^(s, a) $$\nDynamic programming utilizes these equations to iteratively update the value function and improve the policy until convergence, ensuring that the policy becomes optimal.\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Let\u0026rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton \u0026amp; Barto’s book, Reinforcement Learning: An Introduction.\n3.1 Problem Statement: The world is represented as a 4×12 grid. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Mapping this problem to RL components:\nAgent: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards. Environment: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic. State: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states. Action: The action space consists of four possible moves: UP, DOWN, LEFT, RIGHT. Reward: A scalar signal from the environment for each move: -1 for each normal move. -100 if the agent falls into the cliff. 0 upon reaching the goal. Policy: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., (3, 0) -\u0026gt; MOVE UP. The objective is to discover such a policy. 3.2 Python Implementation 3.2.1 Cliff Walking Environment The environment can be defined as follows:\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right Initialize the environment with specified start, end, and cliff cells:\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check GitHub for visualization code. env.render_plot() 3.2.2 Define the Step Function Next, define the rules and rewards for the agent\u0026rsquo;s actions:\nself.actions defines four types of actions, each representing a step in a direction. The step function takes the current location (i, j) and action index, executes it, and returns a tuple representing: The agent\u0026rsquo;s new position after the action. The reward for the current action. def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The movement rules and rewards are:\nThe agent receives -1 for each normal move. The agent receives -100 if it falls off the cliff. The agent receives 0 upon reaching the goal. If the agent moves out of the grid, it remains in the same cell and still receives -1. With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.\n4. Dynamic Programming Methods 4.1 Introduction Dynamic programming (DP) is a method used in reinforcement learning to solve Markov decision processes (MDPs) by breaking them down into simpler subproblems. It works by iteratively improving the value function, which estimates the expected return of states, and deriving an optimal policy from these values. DP requires a complete model of the environment, including the transition probabilities and reward functions.\n4.1.1 Markov Property A Markov decision process (MDP) is defined by five elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|s_t, a_t)$ $R$: reward function immediate reward after a transition, $r(s_{t+1}, s_t, a_t)$ $\\gamma$: discount factor that weights the importance of future rewards A decision process has the Markov Property if its next state and reward depend only on its current state and action, not the full history. The cliff walking problem satisfies the Markovian property.\n4.1.2 Bellman Optimal Function The Bellman optimality equation defines the optimal value function as: $$ V^{}(s) = \\max V_\\pi(s) $$ Here, we search for a policy $\\pi$ that maximizes the value of state $V$. The resulting policy is our optimal policy. $$ \\pi^{}(s) = \\arg\\max V_\\pi(s) $$ For each state $s$, we search over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, \\text{ if } a = \\arg\\max Q^(s, a) $$\nDynamic programming utilizes these equations to iteratively update the value function and improve the policy until convergence, ensuring that the policy becomes optimal.\n5.2 Value Iteration 5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$ seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ update current state\u0026rsquo;s value function with the action that beares largest reward. $V(s_t) = \\underset{a}{max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below we defined one iteration for value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a gird location of (i, j), value is initlized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iteartions. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitely by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stoped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$ each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stoped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Let\u0026rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton \u0026amp; Barto’s book, Reinforcement Learning: An Introduction.\n3.1 Problem Statement: The world is represented as a 4×12 grid. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Mapping this problem to RL components:\nAgent: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards. Environment: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic. State: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states. Action: The action space consists of four possible moves: UP, DOWN, LEFT, RIGHT. Reward: A scalar signal from the environment for each move: -1 for each normal move. -100 if the agent falls into the cliff. 0 upon reaching the goal. Policy: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., (3, 0) -\u0026gt; MOVE UP. The objective is to discover such a policy. 3.2 Python Implementation 3.2.1 Cliff Walking Environment The environment can be defined as follows:\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right Initialize the environment with specified start, end, and cliff cells:\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check GitHub for visualization code. env.render_plot() 3.2.2 Define the Step Function Next, define the rules and rewards for the agent\u0026rsquo;s actions:\nself.actions defines four types of actions, each representing a step in a direction. The step function takes the current location (i, j) and action index, executes it, and returns a tuple representing: The agent\u0026rsquo;s new position after the action. The reward for the current action. def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The movement rules and rewards are:\nThe agent receives -1 for each normal move. The agent receives -100 if it falls off the cliff. The agent receives 0 upon reaching the goal. If the agent moves out of the grid, it remains in the same cell and still receives -1. With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.\n4. Dynamic Programming Methods 4.1 Introduction Dynamic programming (DP) is a method used in reinforcement learning to solve Markov decision processes (MDPs) by breaking them down into simpler subproblems. It works by iteratively improving the value function, which estimates the expected return of states, and deriving an optimal policy from these values. DP requires a complete model of the environment, including the transition probabilities and reward functions.\n4.1.1 Markov Property A Markov decision process (MDP) is defined by five elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|s_t, a_t)$ $R$: reward function immediate reward after a transition, $r(s_{t+1}, s_t, a_t)$ $\\gamma$: discount factor that weights the importance of future rewards A decision process has the Markov Property if its next state and reward depend only on its current state and action, not the full history. The cliff walking problem satisfies the Markovian property.\n4.1.2 Bellman Optimal Function The Bellman optimality equation defines the optimal value function as: $$ V^{}(s) = \\max V_\\pi(s) $$ Here, we search for a policy $\\pi$ that maximizes the value of state $V$. The resulting policy is our optimal policy. $$ \\pi^{}(s) = \\arg\\max V_\\pi(s) $$ For each state $s$, we search over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, \\text{ if } a = \\arg\\max Q^(s, a) $$\nDynamic programming utilizes these equations to iteratively update the value function and improve the policy until convergence, ensuring that the policy becomes optimal.\n5.2 Value Iteration Value iteration is a dynamic programming method used to compute the optimal policy and value function for a Markov decision process (MDP). It iteratively updates the value function by considering the expected returns of all possible actions at each state and selecting the action that maximizes this return.\n5.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$: Search over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ Update the current state\u0026rsquo;s value function with the action that bears the largest reward: $V(s_t) = \\underset{a}{\\max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 5.2.2 Python Implementation of Value Iteration for Cliff Walking Below is the implementation for one iteration of value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a grid location of (i, j), value is initialized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iterations. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitly by selecting over an action that maximized its next value state. To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stopped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 5.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 5.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitly from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n5.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 5.3.2 Python Implementation of Policy Iteration for Cliff Walking Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stopped until the the policy no longer changed. 5.3.3 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Let\u0026rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton \u0026amp; Barto’s book, Reinforcement Learning: An Introduction.\n3.1 Problem Statement: The world is represented as a 4×12 grid. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Mapping this problem to RL components:\nAgent: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards. Environment: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic. State: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states. Action: The action space consists of four possible moves: UP, DOWN, LEFT, RIGHT. Reward: A scalar signal from the environment for each move: -1 for each normal move. -100 if the agent falls into the cliff. 0 upon reaching the goal. Policy: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., (3, 0) -\u0026gt; MOVE UP. The objective is to discover such a policy. 3.2 Python Implementation 3.2.1 Cliff Walking Environment The environment can be defined as follows:\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right Initialize the environment with specified start, end, and cliff cells:\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check GitHub for visualization code. env.render_plot() 3.2.2 Define the Step Function Next, define the rules and rewards for the agent\u0026rsquo;s actions:\nself.actions defines four types of actions, each representing a step in a direction. The step function takes the current location (i, j) and action index, executes it, and returns a tuple representing: The agent\u0026rsquo;s new position after the action. The reward for the current action. def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The movement rules and rewards are:\nThe agent receives -1 for each normal move. The agent receives -100 if it falls off the cliff. The agent receives 0 upon reaching the goal. If the agent moves out of the grid, it remains in the same cell and still receives -1. With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.\n4. Dynamic Programming Methods 4.1 Introduction Dynamic programming (DP) is a method used in reinforcement learning to solve Markov decision processes (MDPs) by breaking them down into simpler subproblems. It works by iteratively improving the value function, which estimates the expected return of states, and deriving an optimal policy from these values. DP requires a complete model of the environment, including the transition probabilities and reward functions.\n4.1.1 Markov Property A Markov decision process (MDP) is defined by five elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|s_t, a_t)$ $R$: reward function immediate reward after a transition, $r(s_{t+1}, s_t, a_t)$ $\\gamma$: discount factor that weights the importance of future rewards A decision process has the Markov Property if its next state and reward depend only on its current state and action, not the full history. The cliff walking problem satisfies the Markovian property.\n4.1.2 Bellman Optimal Function The Bellman optimality equation defines the optimal value function as: $$ V^{}(s) = \\max V_\\pi(s) $$ Here, we search for a policy $\\pi$ that maximizes the value of state $V$. The resulting policy is our optimal policy. $$ \\pi^{}(s) = \\arg\\max V_\\pi(s) $$ For each state $s$, we search over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, \\text{ if } a = \\arg\\max Q^(s, a) $$\nDynamic programming utilizes these equations to iteratively update the value function and improve the policy until convergence, ensuring that the policy becomes optimal.\n4.2 Value Iteration Value iteration is a dynamic programming method used to compute the optimal policy and value function for a Markov decision process (MDP). It iteratively updates the value function by considering the expected returns of all possible actions at each state and selecting the action that maximizes this return.\n4.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$: Search over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ Update the current state\u0026rsquo;s value function with the action that bears the largest reward: $V(s_t) = \\underset{a}{\\max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 4.2.2 Python Implementation of Value Iteration for Cliff Walking 4.2.2.1 One Epoch Update Below is the implementation for one iteration of value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a grid location of (i, j), value is initialized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iterations. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitly by selecting over an action that maximized its next value state.\n4.2.2.2 Training To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stopped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 4.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 4.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitly from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n4.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 4.3.2 Python Implementation of Policy Iteration for Cliff Walking 4.3.2.1 Policy Evaluation Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. 4.3.2.2 Policy Improvement Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. 4.3.2.3 Training Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stopped until the the policy no longer changed. 4.3.2.4 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Let\u0026rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton \u0026amp; Barto’s book, Reinforcement Learning: An Introduction.\n3.1 Problem Statement: The world is represented as a 4×12 grid. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Mapping this problem to RL components:\nAgent: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards. Environment: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic. State: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states. Action: The action space consists of four possible moves: UP, DOWN, LEFT, RIGHT. Reward: A scalar signal from the environment for each move: -1 for each normal move. -100 if the agent falls into the cliff. 0 upon reaching the goal. Policy: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., (3, 0) -\u0026gt; MOVE UP. The objective is to discover such a policy. 3.2 Python Implementation 3.2.1 Cliff Walking Environment The environment can be defined as follows:\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right Initialize the environment with specified start, end, and cliff cells:\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check GitHub for visualization code. env.render_plot() 3.2.2 Define the Step Function Next, define the rules and rewards for the agent\u0026rsquo;s actions:\nself.actions defines four types of actions, each representing a step in a direction. The step function takes the current location (i, j) and action index, executes it, and returns a tuple representing: The agent\u0026rsquo;s new position after the action. The reward for the current action. def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The movement rules and rewards are:\nThe agent receives -1 for each normal move. The agent receives -100 if it falls off the cliff. The agent receives 0 upon reaching the goal. If the agent moves out of the grid, it remains in the same cell and still receives -1. With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.\n4. Dynamic Programming Methods 4.1 Introduction Dynamic programming (DP) is a method used in reinforcement learning to solve Markov decision processes (MDPs) by breaking them down into simpler subproblems. It works by iteratively improving the value function, which estimates the expected return of states, and deriving an optimal policy from these values. DP requires a complete model of the environment, including the transition probabilities and reward functions.\n4.1.1 Markov Property A Markov decision process (MDP) is defined by five elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|s_t, a_t)$ $R$: reward function immediate reward after a transition, $r(s_{t+1}, s_t, a_t)$ $\\gamma$: discount factor that weights the importance of future rewards A decision process has the Markov Property if its next state and reward depend only on its current state and action, not the full history. The cliff walking problem satisfies the Markovian property.\n4.1.2 Bellman Optimal Function The Bellman optimality equation defines the optimal value function as: $$ V^{}(s) = \\max V_\\pi(s) $$ Here, we search for a policy $\\pi$ that maximizes the value of state $V$. The resulting policy is our optimal policy. $$ \\pi^{}(s) = \\arg\\max V_\\pi(s) $$ For each state $s$, we search over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, \\text{ if } a = \\arg\\max Q^(s, a) $$\nDynamic programming utilizes these equations to iteratively update the value function and improve the policy until convergence, ensuring that the policy becomes optimal.\n4.2 Value Iteration Value iteration is a dynamic programming method used to compute the optimal policy and value function for a Markov decision process (MDP). It iteratively updates the value function by considering the expected returns of all possible actions at each state and selecting the action that maximizes this return.\n4.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$: Search over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ Update the current state\u0026rsquo;s value function with the action that bears the largest reward: $V(s_t) = \\underset{a}{\\max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 4.2.2 Python Implementation of Value Iteration for Cliff Walking 4.2.2.1 One Epoch Update Below is the implementation for one iteration of value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a grid location of (i, j), value is initialized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iterations. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitly by selecting over an action that maximized its next value state.\n4.2.2.2 Training To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stopped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 4.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 4.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitly from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n4.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 4.3.2 Python Implementation of Policy Iteration for Cliff Walking 4.3.2.1 Policy Evaluation Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. 4.3.2.2 Policy Improvement Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. 4.3.2.3 Training Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stopped until the the policy no longer changed. 4.3.2.4 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Let\u0026rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton \u0026amp; Barto’s book, Reinforcement Learning: An Introduction.\n3.1 Problem Statement: The world is represented as a 4×12 grid. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Mapping this problem to RL components:\nAgent: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards. Environment: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic. State: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states. Action: The action space consists of four possible moves: UP, DOWN, LEFT, RIGHT. Reward: A scalar signal from the environment for each move: -1 for each normal move. -100 if the agent falls into the cliff. 0 upon reaching the goal. Policy: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., (3, 0) -\u0026gt; MOVE UP. The objective is to discover such a policy. 3.2 Python Implementation 3.2.1 Cliff Walking Environment The environment can be defined as follows:\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right Initialize the environment with specified start, end, and cliff cells:\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check GitHub for visualization code. env.render_plot() 3.2.2 Define the Step Function Next, define the rules and rewards for the agent\u0026rsquo;s actions:\nself.actions defines four types of actions, each representing a step in a direction. The step function takes the current location (i, j) and action index, executes it, and returns a tuple representing: The agent\u0026rsquo;s new position after the action. The reward for the current action. def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The movement rules and rewards are:\nThe agent receives -1 for each normal move. The agent receives -100 if it falls off the cliff. The agent receives 0 upon reaching the goal. If the agent moves out of the grid, it remains in the same cell and still receives -1. With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.\n4. Dynamic Programming Methods 4.1 Introduction Dynamic programming (DP) is a method used in reinforcement learning to solve Markov decision processes (MDPs) by breaking them down into simpler subproblems. It works by iteratively improving the value function, which estimates the expected return of states, and deriving an optimal policy from these values. DP requires a complete model of the environment, including the transition probabilities and reward functions.\n4.1.1 Markov Property A Markov decision process (MDP) is defined by five elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|s_t, a_t)$ $R$: reward function immediate reward after a transition, $r(s_{t+1}, s_t, a_t)$ $\\gamma$: discount factor that weights the importance of future rewards A decision process has the Markov Property if its next state and reward depend only on its current state and action, not the full history. The cliff walking problem satisfies the Markovian property.\n4.1.2 Bellman Optimal Function The Bellman optimality equation defines the optimal value function as: $$ V^{}(s) = \\max V_\\pi(s) $$ Here, we search for a policy $\\pi$ that maximizes the value of state $V$. The resulting policy is our optimal policy. $$ \\pi^{}(s) = \\arg\\max V_\\pi(s) $$ For each state $s$, we search over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, \\text{ if } a = \\arg\\max Q^(s, a) $$\nDynamic programming utilizes these equations to iteratively update the value function and improve the policy until convergence, ensuring that the policy becomes optimal.\n4.2 Value Iteration Value iteration is a dynamic programming method used to compute the optimal policy and value function for a Markov decision process (MDP). It iteratively updates the value function by considering the expected returns of all possible actions at each state and selecting the action that maximizes this return.\n4.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$: Search over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ Update the current state\u0026rsquo;s value function with the action that bears the largest reward: $V(s_t) = \\underset{a}{\\max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 4.2.2 Python Implementation of Value Iteration for Cliff Walking 4.2.2.1 One Epoch Update Below is the implementation for one iteration of value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a grid location of (i, j), value is initialized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iterations. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitly by selecting over an action that maximized its next value state.\n4.2.2.2 Training To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stopped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 4.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 4.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitly from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n4.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 4.3.2 Python Implementation of Policy Iteration for Cliff Walking 4.3.2.1 Policy Evaluation Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. 4.3.2.2 Policy Improvement Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. 4.3.2.3 Training Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stopped until the the policy no longer changed. 4.3.2.4 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Let\u0026rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton \u0026amp; Barto’s book, Reinforcement Learning: An Introduction.\n3.1 Problem Statement: The world is represented as a 4×12 grid. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Mapping this problem to RL components:\nAgent: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards. Environment: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic. State: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states. Action: The action space consists of four possible moves: UP, DOWN, LEFT, RIGHT. Reward: A scalar signal from the environment for each move: -1 for each normal move. -100 if the agent falls into the cliff. 0 upon reaching the goal. Policy: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., (3, 0) -\u0026gt; MOVE UP. The objective is to discover such a policy. 3.2 Python Implementation 3.2.1 Cliff Walking Environment The environment can be defined as follows:\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right Initialize the environment with specified start, end, and cliff cells:\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check GitHub for visualization code. env.render_plot() 3.2.2 Define the Step Function Next, define the rules and rewards for the agent\u0026rsquo;s actions:\nself.actions defines four types of actions, each representing a step in a direction. The step function takes the current location (i, j) and action index, executes it, and returns a tuple representing: The agent\u0026rsquo;s new position after the action. The reward for the current action. def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The movement rules and rewards are:\nThe agent receives -1 for each normal move. The agent receives -100 if it falls off the cliff. The agent receives 0 upon reaching the goal. If the agent moves out of the grid, it remains in the same cell and still receives -1. With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.\n4. Dynamic Programming Methods 4.1 Introduction Dynamic programming (DP) is a method used in reinforcement learning to solve Markov decision processes (MDPs) by breaking them down into simpler subproblems. It works by iteratively improving the value function, which estimates the expected return of states, and deriving an optimal policy from these values. DP requires a complete model of the environment, including the transition probabilities and reward functions.\n4.1.1 Markov Property A Markov decision process (MDP) is defined by five elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|s_t, a_t)$ $R$: reward function immediate reward after a transition, $r(s_{t+1}, s_t, a_t)$ $\\gamma$: discount factor that weights the importance of future rewards A decision process has the Markov Property if its next state and reward depend only on its current state and action, not the full history. The cliff walking problem satisfies the Markovian property.\n4.1.2 Bellman Optimal Function The Bellman optimality equation defines the optimal value function as: $$ V^{}(s) = \\max V_\\pi(s) $$ Here, we search for a policy $\\pi$ that maximizes the value of state $V$. The resulting policy is our optimal policy. $$ \\pi^{}(s) = \\arg\\max V_\\pi(s) $$ For each state $s$, we search over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, \\text{ if } a = \\arg\\max Q^(s, a) $$\nDynamic programming utilizes these equations to iteratively update the value function and improve the policy until convergence, ensuring that the policy becomes optimal.\n4.2 Value Iteration Value iteration is a dynamic programming method used to compute the optimal policy and value function for a Markov decision process (MDP). It iteratively updates the value function by considering the expected returns of all possible actions at each state and selecting the action that maximizes this return.\n4.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$: Search over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ Update the current state\u0026rsquo;s value function with the action that bears the largest reward: $V(s_t) = \\underset{a}{\\max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 4.2.2 Python Implementation of Value Iteration for Cliff Walking 4.2.2.1 One Epoch Update Below is the implementation for one iteration of value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a grid location of (i, j), value is initialized to 0 We iterate over all grid locations that\u0026rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value. We used max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iterations. Value iteration does not explicitly optimize the policy, instead it\u0026rsquo;s learnt implicitly by selecting over an action that maximized its next value state.\n4.2.2.2 Training To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discounted factor that defined the future reward\u0026rsquo;s current value The training iteration stopped until the difference between two value functions are \u0026lt;tolerance. We returned the V and policy data during training for evaluation purpose 4.2.3 Result and Visualization We run training using gamma=0.9, it converges in 15 epoches\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 15 epoches We visulize both the value function and policy in epoch 1, 7, 14\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visuliaztion:\nIn epoch 1, the agent learnt to avoid the cliff In epoch 7, the agent learnt the best actions on right side of the grid, which is either take DOWN or RIGHT action to reach the goal grid. In eppch 14, the value function converges, which the optimal path now is to take UP from start then always take RIGHT until close to the goal. 4.3 Policy Iteration In previous Value Iteration method, during iterations we only updated the value function until convergence, the policy is derived implicitly from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation, given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvment, update the agent\u0026rsquo;s policy respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n4.3.1 Policy Iteration for Cliff Walking Step 0, Initilization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$ Search over its policy\u0026rsquo;s actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ Update current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function convergent. Step 2, policy improvement, for each state $s_t$ Seach over current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat step 1 and 2 until the policy doens\u0026rsquo;t change. 4.3.2 Python Implementation of Policy Iteration for Cliff Walking 4.3.2.1 Policy Evaluation Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value interation, except one major difference: In value_iterate, we compute value functions among all actions and used np.max(values) to pick the best action, which means we are implicitely changing the policy using argmax. In policy_eval, we only iterate actions in existing policy policy[(i, j)], and used np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returned a new value function V_new after convergence. 4.3.2.2 Policy Improvement Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in current policy policy to compare whether there is any changes between the two policy. It returns both the updated policy policy_new and a boolean indicated whether the policy changed during optimization. 4.3.2.3 Training Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, value function V is default to 0, policy is default to all 4 actions. The training iteration stopped until the the policy no longer changed. 4.3.2.4 Results and Evaluation We run training using gamma=0.9, it converges in 6 epoches\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epoches\u0026#34;) Trained 6 epoches We also visualize the value function and policy in epoch 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row has a low value function as it has 25% of falling into the cliff and incur -100 reward, so the learnt policy for most grids is to move upward and avoid the cliff. In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent need to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and otpimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and compute the best path. Monte-Carlo is like an explorer that tries different routes and keep optimizing the policy.\n6.1 Interaction Environment We first need to chang the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean varaible done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL system, it\u0026rsquo;s very common to face the exploration vs exploitation dillema:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\\epsilon$-search try to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With pobability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose action with highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow current policy to generate full episode until the agent reached goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimzed action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() set the agent to start state. Q is the Value table, with key is the current location (i, j), value is a list of size 4, the value at index k represents value for action k. The simulation stop after it reached the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with key being a \u0026lt;state, action\u0026gt; combination, value being a list that stored its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initilized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:\nThe $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also we used $\\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in current batch, update the parameters, then move to the next batch. Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learnt policy in epoch=9999 is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of next step Policy Update: use the actual action $a_{t+1}$ to update value function. So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focused on exploration, it decides the interaction with the environment. One Target Policy that focused on exploitation, it doesn\u0026rsquo;t do interaction, but focused on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initilization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converge.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Let\u0026rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton \u0026amp; Barto’s book, Reinforcement Learning: An Introduction.\n3.1 Problem Statement: The world is represented as a 4×12 grid. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Mapping this problem to RL components:\nAgent: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards. Environment: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic. State: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states. Action: The action space consists of four possible moves: UP, DOWN, LEFT, RIGHT. Reward: A scalar signal from the environment for each move: -1 for each normal move. -100 if the agent falls into the cliff. 0 upon reaching the goal. Policy: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., (3, 0) -\u0026gt; MOVE UP. The objective is to discover such a policy. 3.2 Python Implementation 3.2.1 Cliff Walking Environment The environment can be defined as follows:\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right Initialize the environment with specified start, end, and cliff cells:\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check GitHub for visualization code. env.render_plot() 3.2.2 Define the Step Function Next, define the rules and rewards for the agent\u0026rsquo;s actions:\nself.actions defines four types of actions, each representing a step in a direction. The step function takes the current location (i, j) and action index, executes it, and returns a tuple representing: The agent\u0026rsquo;s new position after the action. The reward for the current action. def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The movement rules and rewards are:\nThe agent receives -1 for each normal move. The agent receives -100 if it falls off the cliff. The agent receives 0 upon reaching the goal. If the agent moves out of the grid, it remains in the same cell and still receives -1. With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.\n4. Dynamic Programming Methods 4.1 Introduction Dynamic programming (DP) is a method used in reinforcement learning to solve Markov decision processes (MDPs) by breaking them down into simpler subproblems. It works by iteratively improving the value function, which estimates the expected return of states, and deriving an optimal policy from these values. DP requires a complete model of the environment, including the transition probabilities and reward functions.\n4.1.1 Markov Property A Markov decision process (MDP) is defined by five elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|s_t, a_t)$ $R$: reward function immediate reward after a transition, $r(s_{t+1}, s_t, a_t)$ $\\gamma$: discount factor that weights the importance of future rewards A decision process has the Markov Property if its next state and reward depend only on its current state and action, not the full history. The cliff walking problem satisfies the Markovian property.\n4.1.2 Bellman Optimal Function The Bellman optimality equation defines the optimal value function as: $$ V^{}(s) = \\max V_\\pi(s) $$ Here, we search for a policy $\\pi$ that maximizes the value of state $V$. The resulting policy is our optimal policy. $$ \\pi^{}(s) = \\arg\\max V_\\pi(s) $$ For each state $s$, we search over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, \\text{ if } a = \\arg\\max Q^(s, a) $$\nDynamic programming utilizes these equations to iteratively update the value function and improve the policy until convergence, ensuring that the policy becomes optimal.\n4.2 Value Iteration Value iteration is a dynamic programming method used to compute the optimal policy and value function for a Markov decision process (MDP). It iteratively updates the value function by considering the expected returns of all possible actions at each state and selecting the action that maximizes this return.\n4.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$: Search over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ Update the current state\u0026rsquo;s value function with the action that bears the largest reward: $V(s_t) = \\underset{a}{\\max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 4.2.2 Python Implementation of Value Iteration for Cliff Walking 4.2.2.1 One Epoch Update Below is the implementation for one iteration of value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a grid location of (i, j), and its value is initialized to 0. We iterate over all grid locations that are not cliffs or goal locations. For each grid, we iterate over its 4 actions and pick the action with the largest value to update the current value. We use max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iterations. Value iteration does not explicitly optimize the policy; instead, it is learned implicitly by selecting the action that maximizes its next value state.\n4.2.2.2 Training To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discount factor that defines the current value of future rewards. The training iteration stops when the difference between two value functions is \u0026lt;tolerance. We return the V and policy data during training for evaluation purposes. 4.2.3 Result and Visualization We run training using gamma=0.9, and it converges in 15 epochs.\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epochs\u0026#34;) Trained 15 epochs We visualize both the value function and policy in epochs 1, 7, 14:\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visualization:\nIn epoch 1, the agent learns to avoid the cliff. In epoch 7, the agent learns the best actions on the right side of the grid, which are either to take DOWN or RIGHT actions to reach the goal grid. In epoch 14, the value function converges, and the optimal path is to take UP from the start and then always take RIGHT until close to the goal. 4.3 Policy Iteration In the previous Value Iteration method, during iterations, we only updated the value function until convergence. The policy is derived implicitly from the value function. So, can we optimize the policy directly? This leads us to another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation: Given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvement: Update the agent\u0026rsquo;s policy with respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n4.3.1 Policy Iteration for Cliff Walking Step 0, Initialization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize the policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$: Search over its policy\u0026rsquo;s actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$. Update the current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function converges. Step 2, policy improvement, for each state $s_t$: Search over the current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat steps 1 and 2 until the policy doesn\u0026rsquo;t change. 4.3.2 Python Implementation of Policy Iteration for Cliff Walking 4.3.2.1 Policy Evaluation Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value iteration, except for one major difference: In value_iterate, we compute value functions among all actions and use np.max(values) to pick the best action, which means we are implicitly changing the policy using argmax. In policy_eval, we only iterate actions in the existing policy policy[(i, j)], and use np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returns a new value function V_new after convergence. 4.3.2.2 Policy Improvement Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one-step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in the current policy policy to compare whether there is any change between the two policies. It returns both the updated policy policy_new and a boolean indicating whether the policy changed during optimization. 4.3.2.3 Training Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, the value function V is default to 0, and the policy is default to all 4 actions. The training iteration stops until the policy no longer changes. 4.3.2.4 Results and Evaluation We run training using gamma=0.9, and it converges in 6 epochs.\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epochs\u0026#34;) Trained 6 epochs We also visualize the value function and policy in epochs 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row having a low value function as it has 25% of falling into the cliff and incurring -100 reward, so the learned policy for most grids is to move upward and avoid the cliff. In later epochs, since the policy no longer includes actions that lead to falling off the cliff, the value function improves for all grids, and it learns the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nwhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent needs to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and optimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and computes the best path. Monte-Carlo is like an explorer that tries different routes and keeps optimizing the policy.\n6.1 Interaction Environment We first need to change the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with the previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean variable done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return observed after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL systems, it\u0026rsquo;s very common to face the exploration vs exploitation dilemma:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal paths, without getting opportunities to discover potentially better paths. The $\\epsilon$-search tries to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With probability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose the action with the highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initialization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow the current policy to generate a full episode until the agent reaches the goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in the episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with the updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimized action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() sets the agent to the start state. Q is the Value table, with the key being the current location (i, j), and the value being a list of size 4, the value at index k represents the value for action k. The simulation stops after it reaches the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with the key being a \u0026lt;state, action\u0026gt; combination, and the value being a list that stores its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using the function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initialized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier episodes, the agent has no prior knowledge, so we encourage more exploration, then in later episodes focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learned policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is explainable:\nThe $Q$ value function is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also, we used $\\epsilon$-greedy policy, so even in later episodes when the agent learned a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustrations of Monte Carlo methods, it estimates the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural network optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in the current batch, update the parameters, then move to the next batch. Monte-Carlo methods generate a full episode, backpropagate along the episode to update the value function. While TD methods run one step, use its TD difference to update the value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and use it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward ways in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method is called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initialization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converges.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learned policy in epoch=9999 is similar to that learned from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learned a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learned to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of the next step Policy Update: use the actual action $a_{t+1}$ to update the value function. So this policy has to incorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focuses on exploration, it decides the interaction with the environment. One Target Policy that focuses on exploitation, it doesn\u0026rsquo;t do interaction, but focuses on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focuses on greedily learning the optimal policy, without being penalized by random exploratory behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of the actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initialization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converges.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"},{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"1. Intuition I\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss model, data, and loss function. In contrast, RL introduces terms like on-policy, reward, model-free, and agent.\nRecently, I finally found some spare time to delve into RL, and fortunately, Richard Sutton\u0026rsquo;s book, Reinforcement Learning: An Introduction, is surprisingly intuitive, even for RL beginners. I want to summarize and share my learnings here.\nIn this article, I aim to explain some of the basic concepts and elements of RL. I will address the RL problem of cliff walking using three classes of methods: 1. Dynamic Programming, 2. Monte Carlo Methods, and 3. Temporal-Difference Learning. This discussion primarily covers content from chapters 1-6 of Sutton \u0026amp; Barto\u0026rsquo;s book.\n2. Introduction of Reinforcement Learning Why is RL necessary when we already have supervised and unsupervised learning? How does RL differ from these methods?\nAccording to Wikipedia, Machine Learning involves statistical algorithms that can learn from data and generalize to unseen data. Suppose we have data $X$ and its training sample $x_i$; we aim to learn the distribution of $X$.\nIn supervised learning, the data consists of pairs $\u0026lt;x_i, y_i\u0026gt;$, and we aim to learn the distribution $p(y | x)$. For example, in image classification, given an image $x$, we predict its label $y$. Viewing GPT as a special case of supervised learning, it predicts the next token given a text sequence\u0026rsquo;s previous tokens.\nIn unsupervised learning, the data consists of input features $x_i$ without associated labels. The goal is to learn the underlying structure or distribution of the data. Unsupervised learning is used when we want to explore the data\u0026rsquo;s inherent patterns without predefined categories.\nIn reinforcement learning, the input $x$ is dynamic and evolves over time, unlike in supervised learning where $x_i$ and $y_i$ are static pairs, or in unsupervised learning where we only deal with $x_i$. Reinforcement learning focuses on learning a policy $\\pi(a | s)$ that maps situations (states) to actions through direct interaction with the environment over time. This temporal aspect is crucial because the agent\u0026rsquo;s actions influence future states and rewards.\nFor example, consider a self-driving car navigating through traffic. In this scenario, the car must continuously decide on actions (accelerate, brake, turn) based on the current state (traffic conditions, road layout). Supervised learning would struggle here because it requires predefined labels for each possible scenario, which is impractical. Reinforcement learning, however, allows the car to learn optimal driving strategies by interacting with the environment and receiving feedback (rewards) based on its actions.\n2.1 Components of RL Reinforcement learning models interactions in games or strategies. There are two entities in an RL system:\nAgent: The learner and decision-maker. Environment: Everything outside the agent that it interacts with. There are three types of interactions between the agent and environment:\nAction: An action the agent takes to interact with the environment. State: A representation of the environment, altered by actions. Reward: The environment\u0026rsquo;s response to the agent based on its state. A sequence of interactions is:\nAt time $t$, the environment\u0026rsquo;s state is $s_t$, providing reward $r_t$. Based on $s_t$ and $r_t$, the agent takes action $a_t$. As a result of $a_t$, the state changes to $s_{t+1}$, providing reward $r_{t+1}$. The goal of an RL system is to learn a strategy that enables the agent to make the best action at each step to maximize rewards. An agent is characterized by three components:\nPolicy: The strategy the agent uses to determine its next action. Value Function: A function the agent uses to evaluate its current state. Model: The way the environment interacts with the agent, defining how the environment operates. 2.1.1 Policy A policy maps the current state $s_t$ to action $a_t$. It can be deterministic or stochastic:\nStochastic Policy: The result is a distribution from which the agent samples its action.\n$\\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t)$\nDeterministic Policy: The result is a specific action; for a given state, the agent executes a specific action.\n$a_{t} = \\text{argmax}\\ \\pi(a|s_t)$\n2.1.2 Value Function In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.\nThe reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.\n$V_t = r_t + \\gamma V_{t+1}$\nThe gamma ($\\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.\nThe reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.\n$r_t = R(s_t, a_t, s_{t+1})$\nAssuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\\tau$, and the sum reward is represented as the reward over the trajectory.\n$$ \\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \u0026hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \\newline\nR(\\tau) = \\sum_{t=0}^{T-1}r_t $$\n2.1.3 Model A model defines how the environment interacts with the agent, comprising $\u0026lt;S, A, P, R\u0026gt;$:\n$S$: The space of all possible states. $A$: The space of all possible actions. $P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$. $R$: How rewards are calculated for each state, $R(s_t, a_t)$. If these four elements are known, we can model the interaction without actual interaction, known as model-based learning.\nIn reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as model-free learning.\nComparing the two, model-free learning relies on real interaction to get the next state and reward, while model-based learning models these without specific interaction.\n2.2 Optimization of RL The optimization of an RL task can be divided into two parts:\nValue Estimation: Given a strategy $\\pi$, evaluate its effectiveness, computing $V_\\pi$. Policy Optimization: Given the value function $V_\\pi$, optimize to get a better policy $\\pi$. This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:\nGiven the current cluster assignment of each point, compute the optimal centroid. Similar to value estimation. Given the current optimal centroid, find a better cluster assignment for each point. Similar to policy optimization. Are both steps necessary in RL optimization? The answer is no.\n2.2.1 Value-based Agent An agent can learn only the value function $V_\\pi$, maintaining a table mapping $\u0026lt;S, A\u0026gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn\u0026rsquo;t explicitly have a strategy or policy.\n$a_t = \\text{argmax}\\ V(a | s_t)$\n2.2.2 Policy-based Agent A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.\n$a_t \\sim P(a|s_t)$\n2.2.3 Actor-Critic An agent can learn both $\\pi$ and $V_{\\pi}$ as described above.\nActor: Learning of policy $\\pi$. Critic: Learning of value function $V_\\pi$. 3. Example Problem, Cliff Walking Problem Let\u0026rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton \u0026amp; Barto’s book, Reinforcement Learning: An Introduction.\n3.1 Problem Statement: The world is represented as a 4×12 grid. The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$. The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start. Each non-terminal move incurs a reward of -1. Mapping this problem to RL components:\nAgent: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards. Environment: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic. State: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states. Action: The action space consists of four possible moves: UP, DOWN, LEFT, RIGHT. Reward: A scalar signal from the environment for each move: -1 for each normal move. -100 if the agent falls into the cliff. 0 upon reaching the goal. Policy: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., (3, 0) -\u0026gt; MOVE UP. The objective is to discover such a policy. 3.2 Python Implementation 3.2.1 Cliff Walking Environment The environment can be defined as follows:\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): self.height = height self.width = width self.start = start self.end = end self.cliff = set(cliff) self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right Initialize the environment with specified start, end, and cliff cells:\nstart = (3, 0) end = (3, 11) height = 4 width = 12 cliff_cells = [(3, i) for i in range(1, 11)] env = CliffWalk(height, width, start, end, cliff_cells) # Check GitHub for visualization code. env.render_plot() 3.2.2 Define the Step Function Next, define the rules and rewards for the agent\u0026rsquo;s actions:\nself.actions defines four types of actions, each representing a step in a direction. The step function takes the current location (i, j) and action index, executes it, and returns a tuple representing: The agent\u0026rsquo;s new position after the action. The reward for the current action. def step(self, i: int, j: int, a: int) -\u0026gt; tuple[tuple[int, int], int]: ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: return self.start, -100 if (ni, nj) == self.end: return (ni, nj), 0 # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: return (ni, nj), -1 # Move out of grid, get -1 reward return (i, j), -1 The movement rules and rewards are:\nThe agent receives -1 for each normal move. The agent receives -100 if it falls off the cliff. The agent receives 0 upon reaching the goal. If the agent moves out of the grid, it remains in the same cell and still receives -1. With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.\n4. Dynamic Programming Methods 4.1 Introduction Dynamic programming (DP) is a method used in reinforcement learning to solve Markov decision processes (MDPs) by breaking them down into simpler subproblems. It works by iteratively improving the value function, which estimates the expected return of states, and deriving an optimal policy from these values. DP requires a complete model of the environment, including the transition probabilities and reward functions.\n4.1.1 Markov Property A Markov decision process (MDP) is defined by five elements: $$ MDP = \u0026lt;S, A, P, R, \\gamma\u0026gt; $$\n$S$: state space $A$: action space $P$: transition probability from a state and action to its next state, $p(s_{t+1}|s_t, a_t)$ $R$: reward function immediate reward after a transition, $r(s_{t+1}, s_t, a_t)$ $\\gamma$: discount factor that weights the importance of future rewards A decision process has the Markov Property if its next state and reward depend only on its current state and action, not the full history. The cliff walking problem satisfies the Markovian property.\n4.1.2 Bellman Optimal Function The Bellman optimality equation defines the optimal value function as: $$ V^{}(s) = \\max V_\\pi(s) $$ Here, we search for a policy $\\pi$ that maximizes the value of state $V$. The resulting policy is our optimal policy. $$ \\pi^{}(s) = \\arg\\max V_\\pi(s) $$ For each state $s$, we search over its possible actions to maximize the value: $$ \\pi^{}(a | s) = 1, \\text{ if } a = \\arg\\max Q^(s, a) $$\nDynamic programming utilizes these equations to iteratively update the value function and improve the policy until convergence, ensuring that the policy becomes optimal.\n4.2 Value Iteration Value iteration is a dynamic programming method used to compute the optimal policy and value function for a Markov decision process (MDP). It iteratively updates the value function by considering the expected returns of all possible actions at each state and selecting the action that maximizes this return.\n4.2.1 Value Iteration for Cliff Walking Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ For each state $s_t$: Search over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$ Update the current state\u0026rsquo;s value function with the action that bears the largest reward: $V(s_t) = \\underset{a}{\\max}(r_a + V(s_{t+1}))$ Repeat the previous steps until convergence 4.2.2 Python Implementation of Value Iteration for Cliff Walking 4.2.2.1 One Epoch Update Below is the implementation for one iteration of value update:\ndef value_iterate(env, V, gamma=0.9): \u0026#34;\u0026#34;\u0026#34;Run one epoch of value iteration\u0026#34;\u0026#34;\u0026#34; V_new = V.copy() policy = defaultdict(list) delta = 0. for i in range(env.height): for j in range(env.width): if (i, j) not in cliff_cells and (i, j) != (end): values = [] for a in range(4): (i_new, j_new), reward= env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_value = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_value] V_new[(i, j)] = max_value policy[(i, j)] = best_actions delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) return V_new, policy, delta V is the value function, its key is a grid location of (i, j), and its value is initialized to 0. We iterate over all grid locations that are not cliffs or goal locations. For each grid, we iterate over its 4 actions and pick the action with the largest value to update the current value. We use max(abs(V_new[(i, j)] - V[i, j])) as the difference between value iterations. Value iteration does not explicitly optimize the policy; instead, it is learned implicitly by selecting the action that maximizes its next value state.\n4.2.2.2 Training To train the value iteration until convergence:\ndef value_iteration_train(env, gamma=0.9, tolerance=1e-6): progress_data = [] V = defaultdict(float) policy = defaultdict(list) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: float(\u0026#34;inf\u0026#34;)}) while True: V, policy, delta = value_iterate(env, V, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;delta\u0026#34;: delta}) if delta \u0026lt; tolerance: break return progress_data gamma is a discount factor that defines the current value of future rewards. The training iteration stops when the difference between two value functions is \u0026lt;tolerance. We return the V and policy data during training for evaluation purposes. 4.2.3 Result and Visualization We run training using gamma=0.9, and it converges in 15 epochs.\ndp_progress = value_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epochs\u0026#34;) Trained 15 epochs We visualize both the value function and policy in epochs 1, 7, 14:\nepoches = [1, 7, 14] for i, ax in enumerate(axs): iter = epoches[i // 2] if i % 2: env.render_plot(policy=dp_progress[iter][\u0026#39;policy\u0026#39;], title = f\u0026#39;Cliff Walking Policy in Epoch {iter}\u0026#39;, ax=ax) else: env.render_plot(value=dp_progress[iter][\u0026#39;V\u0026#39;], title = f\u0026#39;Cliff Walking Value in Epoch {iter}\u0026#39;, ax=ax) plt.tight_layout() plt.show() From the visualization:\nIn epoch 1, the agent learns to avoid the cliff. In epoch 7, the agent learns the best actions on the right side of the grid, which are either to take DOWN or RIGHT actions to reach the goal grid. In epoch 14, the value function converges, and the optimal path is to take UP from the start and then always take RIGHT until close to the goal. 4.3 Policy Iteration In the previous Value Iteration method, during iterations, we only updated the value function until convergence. The policy is derived implicitly from the value function. So, can we optimize the policy directly? This leads us to another dynamic programming method in MDPs, Policy Iteration.\nA policy iteration consists of two parts:\nPolicy Evaluation: Given a policy $\\pi$, compute its state-value function $V^{\\pi}(s)$, which is the expected return of following the policy $\\pi$. $$ V(s_t) = \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\nPolicy Improvement: Update the agent\u0026rsquo;s policy with respect to the current value function. $$ \\pi_{new}(s) = \\underset{a}{argmax}\\ \\sum P(s_{t+1} | s_t, \\pi) * [r(s_t, \\pi, s_{t+1}) + \\gamma * V(s_{t+1})] $$\n4.3.1 Policy Iteration for Cliff Walking Step 0, Initialization: Initialize each state\u0026rsquo;s value function to 0: $V(s) = 0$ Initialize the policy to take all 4 actions in all states. Step 1, policy evaluation, for each state $s_t$: Search over its policy\u0026rsquo;s actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$. Update the current state\u0026rsquo;s value function with the mean reward of policy actions. $V(s_t) = \\underset{a}{mean}(r_a + \\gamma * V(s_{t+1}))$ Repeat until the value function converges. Step 2, policy improvement, for each state $s_t$: Search over the current policy\u0026rsquo;s actions $a_t$ at each $s_t$, compute its value function. Update the policy $\\pi(s_t)$ by only keeping actions with the largest value function. Repeat steps 1 and 2 until the policy doesn\u0026rsquo;t change. 4.3.2 Python Implementation of Policy Iteration for Cliff Walking 4.3.2.1 Policy Evaluation Let\u0026rsquo;s first implement the policy evaluation function:\ndef policy_eval(env, V, policy, gamma=0.9, tolerance=1e-4): V_new = V.copy() while True: delta = 0 for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in policy[(i, j)]: (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) V_new[(i, j)] = np.mean(values) delta = max(delta, abs(V_new[(i, j)] - V[(i, j)])) if delta \u0026lt; tolerance: break V = V_new.copy() return V_new This function is very similar to the value_iterate function in value iteration, except for one major difference: In value_iterate, we compute value functions among all actions and use np.max(values) to pick the best action, which means we are implicitly changing the policy using argmax. In policy_eval, we only iterate actions in the existing policy policy[(i, j)], and use np.mean to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here. The function returns a new value function V_new after convergence. 4.3.2.2 Policy Improvement Then let\u0026rsquo;s implement the policy improvement step:\ndef policy_improve(env, V, policy, gamma=0.9): policy_new = defaultdict(list) policy_stable = True for i in range(env.height): for j in range(env.width): if (i, j) not in env.cliff and (i, j) != env.end: values = [] for a in range(4): (i_new, j_new), reward = env.step(i, j, a) values.append(V[(i_new, j_new)] * gamma + reward) max_val = np.max(values) best_actions = [a for a, v in enumerate(values) if v == max_val] if set(best_actions) != set(policy[(i, j)]): policy_stable = False policy_new[(i, j)] = best_actions return policy_new, policy_stable The policy_improve is a one-step optimization, it takes in the current value function V, picked the argmax action to update the policy, it also takes in the current policy policy to compare whether there is any change between the two policies. It returns both the updated policy policy_new and a boolean indicating whether the policy changed during optimization. 4.3.2.3 Training Combining these two sub-steps, we can train using policy iteration:\ndef policy_iteration_train(env, gamma=0.9, tolerance=1e-6): V = defaultdict(float) policy = defaultdict(lambda : range(4)) progress_data = [{\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}] while True: # Value evaluation V = policy_eval(env, V, policy, gamma) # Policy improvement policy, policy_stable = policy_improve(env, V, policy, gamma) progress_data.append({\u0026#34;V\u0026#34;: V.copy(), \u0026#34;policy\u0026#34;: policy.copy()}) if policy_stable: break idx += 1 return progress_data For all states, the value function V is default to 0, and the policy is default to all 4 actions. The training iteration stops until the policy no longer changes. 4.3.2.4 Results and Evaluation We run training using gamma=0.9, and it converges in 6 epochs.\ndp_progress = policy_iteration_train(env, gamma=0.9, tolerance=1e-6) print(f\u0026#34;Trained {len(dp_progress)} epochs\u0026#34;) Trained 6 epochs We also visualize the value function and policy in epochs 1, 3, 5:\nIn epoch 1, because the initialized policy includes all actions, this leads to grid in the i=2 row having a low value function as it has 25% of falling into the cliff and incurring -100 reward, so the learned policy for most grids is to move upward and avoid the cliff. In later epochs, since the policy no longer includes actions that lead to falling off the cliff, the value function improves for all grids, and it learns the optimal path towards the goal grid. 6. Monte-Carlo Methods It\u0026rsquo;s nice that we solved the cliff walking problem with DP methods, and what\u0026rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:\nThe transition probability: $P(s_{t+1} | s_t, a_t)$ The reward function : $r(s_{t+1}, a_t, s_t)$ What if the agent is in another environment that itself doesn\u0026rsquo;t know any of such information ahead? Assume the agent was placed in the start location, with no knowledge about:\nWhere is the goal grid, and how to reach it. Which grid it will go to if taking an action and what reward it will get. Then the agent needs to interact with the environment to generate episodes (sequence of states, actions, rewards) until it reached the goal grid, and learn these information and optimize the policy during the interaction.\nCompare the two methods, DP is like a planner who knows the full map and computes the best path. Monte-Carlo is like an explorer that tries different routes and keeps optimizing the policy.\n6.1 Interaction Environment We first need to change the CliffWalk environment to mimic an interaction environment.\nclass CliffWalk: def __init__(self, height, width, start, end, cliff): ... # Ignore previous codes self.actions = [(-1, 0), (1, 0), (0, -1), (0, 1)] # up, down, left, right self.agent_pos = self.start def reset(self): self.agent_pos = self.start return self.agent_pos def step(self, a: int) -\u0026gt; tuple[tuple[int, int], int, bool]: i, j = self.agent_pos ni, nj = i + self.actions[a][0], j + self.actions[a][1] # Fell into Cliff, get -100 reward, back to start point if (ni, nj) in self.cliff: self.reset() return self.start, -100, False if (ni, nj) == self.end: return (ni, nj), 0, True # Move, get -1 reward if 0 \u0026lt;= ni \u0026lt; self.height and 0 \u0026lt;= nj \u0026lt; self.width: self.agent_pos = ni, nj else: # Move out of grid, get -1 reward self.agent_pos = i, j done, reward = False, -1 return self.agent_pos, reward, done Compare the new implementation of step function with the previous one:\nThe new implementation only takes an action index, it tracks the agent\u0026rsquo;s state using self.agent_pos We are forbidden to compute the state and reward for any $\u0026lt;state, action\u0026gt;$ now. The agent has to reach a specific $s_t$ and take an $a_t$, call step to finally get the $s_{t+1}, r_t$ from interaction. The step function returns a boolean variable done indicating whether the agent reached the goal grid 6.2 Monte-Carlo Simulation Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $\u0026lt;state, action\u0026gt;$ pair by averaging the total return observed after visiting a state across multiple episodes.\n6.2.1 $\\epsilon$-search algorithm In RL systems, it\u0026rsquo;s very common to face the exploration vs exploitation dilemma:\nExploitation: Pick the best known action so far (greedy) Exploration: Try other actions to discover potentially better ones If the agent always acts greedily, it may get stuck in suboptimal paths, without getting opportunities to discover potentially better paths. The $\\epsilon$-search tries to balance this by introducing a random $\\epsilon$, in each step:\nExploration: With probability $\\epsilon$, choose a random action. Exploitation: With probability $1 - \\epsilon$, choose the action with the highest value: $a = argmax\\ Q(s, a)$ 6.2.2 Monte-Carlo method in Cliff Walking Step 0, Initialization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Generate episodes:\nFrom the start state, follow the current policy to generate a full episode until the agent reaches the goal grid, we will get a sequence of $\u0026lt;s_t, a_t, r_t\u0026gt;$ Step 2, Update value function: $Q(s, a)$\nFor each $\u0026lt;s_t, a_t\u0026gt;$ pair in the episode trace, compute its return by $G_t = r\\ + \\gamma*G_{t+1}$ Update $Q(s, a)$ by averaging returns across multiple episodes. Step 3, Improve policy:\nThe new policy is the $\\epsilon$-greedy policy with the updated value function $Q(s, a)$. Repeat step 1-3 until the policy converges.\n6.3 Python Implementation of MC in Cliff Walking 6.3.1 $\\epsilon$-greedy search This function implements the $\\epsilon$-search to pick the action,\ndef epsilon_greedy(action_values, epsilon): if np.random.rand() \u0026lt; epsilon: # Random action action = np.random.randint(0, 4) else: # Optimized action max_val = np.max(action_values) best_actions = [i for i in range(4) if action_values[i] == max_val] # Random pick among best actions action = np.random.choice(best_actions) return action 6.3.2 MC-Simulation This function simulates 1 episode of MC simulation.\nAt the beginning, env.reset() sets the agent to the start state. Q is the Value table, with the key being the current location (i, j), and the value being a list of size 4, the value at index k represents the value for action k. The simulation stops after it reaches the goal state. def mc_simulation(env, Q, epsilon=0.1): state = env.reset() done = False curr_eps = epsilon episode_data = [] while not done: action = epsilon_greedy(Q[state], epsilon) next_state, reward, done = env.step_interactive(action) episode_data.append((state, action, reward)) state = next_state return episode_data, done 6.3.3 Value Function Update def improve_policy(Q, returns, episode_data, gamma=0.9): # Compute reward visited = set() G = 0 for t in reversed(range(len(episode_data))): state_t, action_t, reward_t = episode_data[t] G = gamma*G + reward_t # First-time update if (state_t, action_t) not in visited: visited.add((state_t, action_t)) returns[(state_t, action_t)].append(G) Q[state_t][action_t] = np.mean(returns[(state_t, action_t)]) policy = defaultdict(int) for k, v in Q.items(): policy[k] = np.argmax(v) return Q, policy returns is a dictionary, with the key being a \u0026lt;state, action\u0026gt; combination, and the value being a list that stores its expected reward in each episode. For each episode, we traversed backwards, iteratively computing each state\u0026rsquo;s value using the function: $G_t = r_t + \\gamma * G_{t+1}$. 6.3.4 Monte-Carlo training Combining the previous steps, we can train the agent:\ndef monte_carlo_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1): Q = defaultdict(lambda: [0.1] * 4) returns = defaultdict(list) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.99) # Run MC simulation episode_data, finished = mc_simulation(env, Q, epsilon) if not finished: continue # Policy improvement Q, policy = improve_policy(Q, returns, episode_data, gamma) progress_data.append({\u0026#34;Q\u0026#34;: copy.deepcopy(Q), \u0026#34;policy\u0026#34;: copy.deepcopy(policy), \u0026#34;episode\u0026#34;: episode}) return progress_data Q is initialized by giving equal weights to each action. We set $\\epsilon$ to decay over episodes, epsilon = max(0.01, epsilon*0.99). In earlier episodes, the agent has no prior knowledge, so we encourage more exploration, then in later episodes focus more on exploitation. 6.3.5 Results and Visualizations We run MC sampling for 5000 episodes:\nprogress_data = monte_carlo_training(env, num_episodes=5000, gamma=0.9, epsilon=0.3) We can see the learned policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is explainable:\nThe $Q$ value function is averaged over episodes, an early cliff fall trace will drag the average return for those cliff-adjacent grids. Also, we used $\\epsilon$-greedy policy, so even in later episodes when the agent learned a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids. 7. Temporal-Difference Methods In previous illustrations of Monte Carlo methods, it estimates the value function using complete episodes. While this is intuitively simple and unbiased, it\u0026rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.\nTemporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.\nI found an intuitive way to understand the difference between TD and MC methods are compare this to Gradient Descent and SGD in neural network optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.\nGradient descent computes the gradient using the full dataset, while SGD compute using only data points in the current batch, update the parameters, then move to the next batch. Monte-Carlo methods generate a full episode, backpropagate along the episode to update the value function. While TD methods run one step, use its TD difference to update the value function, then move to the next step. Then how is TD-difference computed, remember we want to estimate value function using: $$ V(s_t)\\ = r_{t+1} + \\gamma\\ V(s_{t+1}) $$\nSo we can bootstrap at $s_t$, execute one more step and compute the value estimates and use it to update the value function: $$ G(s_t) = r_{t+1} + \\gamma\\ V(s_{t+1}) \\newline \\text{TD Error} = G(s_t) - V(s_t) \\newline V(s_t) \\leftarrow V(s_t) + \\alpha \\cdot (G(s_t) - V(s_t)) $$\n$\\gamma$ is the discount factor $\\alpha$ is the single step learning rate 7.1. SARSA SARSA is one of the most straightforward ways in TD-methods. The idea is intuitive, using next step\u0026rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step\u0026rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function. $$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$ In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $\u0026lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}\u0026gt;$, and this is why this method is called SARSA.\n7.1.1 SARSA method in Cliff Walking Step 0, Initialization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Use the same policy to get the new action $a_{t+1}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converges.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.1.2 Python Implementation of SARSA 7.1.2.1 SARSA def sarsa_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() action = epsilon_greedy(Q[state], epsilon) done = False while not done: next_state, reward, done = env.step_interactive(action) # Sample next action next_action = epsilon_greedy(Q[next_state], epsilon) # TD-Update current function Q[state][action] += alpha*(reward + gamma*Q[next_state][next_action] - Q[state][action]) state, action = next_state, next_action policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We use the same $\\epsilon$-greedy search to get the action The $Q$ value function is updated within each step of the epoch, this is called 1-step SARSA, alternatively, we can also update $Q$ value function every fixed number of steps, which is called $n$-step SARSA To train multiple episodes:\ndef td_sarsa_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = sarsa_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data Similar to that of Monte-Carlo methods, we used a decaying $\\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes. 7.1.2.2 Visualization and Result We train SARSA for 10000 episodes, and visualize the result\nsarsa_progress = td_sarsa_training(env, num_episodes=10000, gamma=0.9, epsilon=0.1, alpha=0.2) The learned policy in epoch=9999 is similar to that learned from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:\nThe agent used $\\epsilon$-greedy search, so even the agent learned a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learned to walk far away from the cliff, taking the constant cost of extra -1 reward, to avoid a potential -100 reward. 7.2 Q-Learning Let\u0026rsquo;s recap the SARSA algorithm again, it used $\\epsilon$-greedy search on $Q$ value functions for two purposes:\nPlanning: Decide the action $a_{t+1}$ of the next step Policy Update: use the actual action $a_{t+1}$ to update the value function. So this policy has to incorporate a trade-off between exploration and exploitation. What if we have two policies:\nOne Behavior Policy that focuses on exploration, it decides the interaction with the environment. One Target Policy that focuses on exploitation, it doesn\u0026rsquo;t do interaction, but focuses on learning from previous interactions. Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focuses on greedily learning the optimal policy, without being penalized by random exploratory behaviors.\nThis new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as On-Policy vs Off-Policy:\nOn-Policy learns the value of the policy it is actually using to make decisions. Off-Policy Learns the value of a different policy than the one it is currently using to make decisions. In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:\n$$ G(s_t) = r_{t+1} + \\gamma\\ Q(s_{t+1}, a_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\nIn Q-Learning, we used the theoretical optimal next action instead of the actual next action for updates: $$ G(s_t) = r_{t+1} + \\gamma\\ \\underset{a}{max}\\ Q(s_{t+1}) \\newline Q(s_{t}, a_{t}) \\leftarrow Q(s_{t}, a_{t}) + \\alpha \\cdot (G(s_t) - Q(s_t, a_t)) $$\n7.2.1 Q-Learning in Cliff Walking Step 0, Initialization:\nInitialize a random value function: $Q(s, a)$ Initialize an $\\epsilon$-greedy policy Step 1, Bootstrap a step:\nAgent in state $s_t$ and action $a_t$ Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$ Step 2, Update value function for the step: $Q(s_t, a_t)$\n$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \\alpha \\cdot (r_{t+1} + \\gamma\\ max\\ Q(s_{t+1}, a) - Q(s_t, a_t))$ Finish 1 episode by repeated running step 1-2 until the agent reached goal state.\nRun above algorithm multiple times until the policy converges.\nThis looks very similar to SARSA, the only difference is:\nWe no longer need to sample $s_{t+1}$ in each step. We used $max\\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update. 7.2.2 Python Implementation of Q-Learning def q_learning_one_epoch(env, Q, gamma=0.9, epsilon=0.1, alpha=0.1): # Get init action state = env.reset() done = False while not done: action = theta_greedy_action(Q, state, epsilon) next_state, reward, done = env.step_interactive(action) # TD-Update current function Q[state][action] += alpha*(reward + gamma*max(Q[next_state]) - Q[state][action]) state = next_state policy = defaultdict(int) for k, v in Q.items(): best_actions = [a for a, i in enumerate(v) if i == np.max(v)] policy[k] = best_actions return Q, policy We no longer computed next_action in each step. Q is updated using max(Q[next_state]). def q_learning_training(env, num_episodes=1000, gamma=0.9, epsilon=0.1, alpha=0.1): # Key: position, Value: value for of each action Q = defaultdict(lambda: np.random.rand(4) * 0.01) progress_data = [] for episode in tqdm.tqdm(range(num_episodes)): epsilon = max(0.01, epsilon*0.95) Q, policy = q_learning_one_epoch(env, Q, gamma, epsilon, alpha) progress_data.append({\u0026#34;Q\u0026#34;: Q.copy(), \u0026#34;policy\u0026#34;: policy.copy(), \u0026#34;episode\u0026#34;: episode}) return progress_data 7.2.3 Visualization and Result Q-Learning converges faster than SARSA, we only trained 200 episodes.\nq_learning_progress = q_learning_training(env, num_episodes=200, gamma=1.0, epsilon=0.1, alpha=0.2) While SARSA found a safe path under randomness of $\\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!\n8. Summary ","permalink":"http://localhost:1313/posts/rl-intro/","summary":"\u003ch2 id=\"1-intuition\"\u003e1. Intuition\u003c/h2\u003e\n\u003cp\u003eI\u0026rsquo;ve been interested in Reinforcement Learning (RL) for a while, especially following the recent advancements in LLM post-training using RL. However, the mathematical concepts and various notions used in RL differ significantly from those in supervised learning, which can feel strange or confusing for beginners like myself. For instance, in traditional machine learning, we typically discuss \u003ccode\u003emodel\u003c/code\u003e, \u003ccode\u003edata\u003c/code\u003e, and \u003ccode\u003eloss function\u003c/code\u003e. In contrast, RL introduces terms like \u003ccode\u003eon-policy\u003c/code\u003e, \u003ccode\u003ereward\u003c/code\u003e, \u003ccode\u003emodel-free\u003c/code\u003e, and \u003ccode\u003eagent\u003c/code\u003e.\u003c/p\u003e","title":"Introduction to Reinforcement Learning"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Test KV Cache in Huggingface\u0026rsquo;s transformers We can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformers.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% A brief peek into transformer\u0026rsquo;s KV Cache implementation To better understand KV Cache, we can look at the transformer\u0026rsquo;s KV Cache implementation.\nLet\u0026rsquo;s use GPT2 as an example. The GPT2Attention.forward takes a use_cache boolean argument, it will return current KV matriices if use_cache=True.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Attention(nn.Module): def forward(..., use_cache: Optional[bool] = False): ... query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2) if use_cache is True: present = (key_states, value_states) else: present = None outputs = (attn_output, present) if output_attentions: outputs += (attn_weights,) return outputs # a, present, (attentions) The GPT2Block class does similar things, then GPT2Model.forward will output the KV matrics for all layers.\n#src/transformers/models/gpt2/modeling_gpt2.py class GPT2Model(GPT2PreTrainedModel): def __init__(self): self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)]) def forward(..., past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None, use_cache: Optional[bool] = False): ... # presents is used to store KV matrics for all layers. presents = () if use_cache else None for i in range(len(self.h)): # Get previous KV matrics from input. block, layer_past = self.h[i], past_key_values[i] outputs = block(input_ids, layer_past=layer_past, use_cache=use_cache) if use_cache is True: presents = presents + (outputs[1],) return BaseModelOutputWithPastAndCrossAttentions( last_hidden_state=hidden_states, past_key_values=presents, hidden_states=all_hidden_states, attentions=all_self_attentions, cross_attentions=all_cross_attentions, ) The KV Cache in past_key_values of GPT2Model.forward is a BaseModelOutputWithPastAndCrossAttentions. It\u0026rsquo;s of shape (num_layers, 2), where the first dimension corresponds to the layer index and the second dimension is key at index 0 and value at index 1. Then each tensor is of shape (batch_size, num_heads, seq_len, head_dim).\nDuring generation, a DynamicCache instance is created in GenerationMixin.\n#/src/transformers/src/transformers/generation/utils.py class GenerationMixin: ... def _prepare_cache_for_generation(self, model_kwargs: Dict[str, Any]): ... cache_name = \u0026#34;past_key_values\u0026#34; model_kwargs[cache_name] = DynamicCache() #src/transformers/cache_utils.py class DynamicCache(Cache): def __init__(self): self._seen_tokens = 0 # Used in `generate` to keep tally of how many tokens the cache has seen self.key_cache: List[torch.Tensor] = [] self.value_cache: List[torch.Tensor] = [] def update( self, key_states: torch.Tensor, value_states: torch.Tensor, layer_idx: int, cache_kwargs: Optional[Dict[str, Any]] = None, ) -\u0026gt; Tuple[torch.Tensor, torch.Tensor]: # Update the number of seen tokens on layer 0. if layer_idx == 0: self._seen_tokens += key_states.shape[-2] # Update the cache if key_states is not None: # Initialization phase, the layer cache not there yet. if len(self.key_cache) \u0026lt;= layer_idx: ... self.key_cache.append(key_states) self.value_cache.append(value_states) else: # Otherwise, only append current key and value to the cache. self.key_cache[layer_idx] = torch.cat([self.key_cache[layer_idx], key_states], dim=-2) self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_states], dim=-2) return self.key_cache[layer_idx], self.value_cache[layer_idx] @classmethod def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -\u0026gt; \u0026#34;DynamicCache\u0026#34;: \u0026#34;\u0026#34;\u0026#34;Converts a cache in the legacy cache format into an equivalent `DynamicCache`. \u0026#34;\u0026#34;\u0026#34; cache = cls() if past_key_values is not None: for layer_idx in range(len(past_key_values)): key_states, value_states = past_key_values[layer_idx] cache.update(key_states, value_states, layer_idx) return cache Then the KV Cache is loaded and used for generation in _sample:\nsrc/transformers/generation/utils.py class GenerationMixin: ... def _sample(self, ...): ... while self._has_unfinished_sequences(): # Prepare KV Cache is in prepare_inputs_for_generation model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs) outputs = model_forward(**model_inputs, return_dict=True) ... return GenerateDecoderOnlyOutput( sequences=input_ids, scores=scores, logits=raw_logits, attentions=decoder_attentions, hidden_states=decoder_hidden_states, past_key_values=model_kwargs.get(\u0026#34;past_key_values\u0026#34;), ) ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"}]