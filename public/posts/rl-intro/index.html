<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>Introduction to Reinforcement Learning | Hxx&#39;Log</title>
<meta name="keywords" content="Reinforcement Learning">
<meta name="description" content="Intuition
I&rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about RLHF, but often felt it&rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton&rsquo;s &lt;Reinforcement Learning: an Introduction&gt; book is surprisingly intuitive even for RL beginners.
However, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.">
<meta name="author" content="hxx">
<link rel="canonical" href="http://localhost:1313/posts/rl-intro/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/rl-intro/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" onload="
  renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false},
      {left: '\\(', right: '\\)', display: false},
      {left: '\\[', right: '\\]', display: true}
    ],
    throwOnError: false
  });
"></script>


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Hxx&#39;Log (Alt + H)">Hxx&#39;Log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      Introduction to Reinforcement Learning
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-03-03 00:00:00 +0000 UTC'>March 3, 2025</span>&nbsp;·&nbsp;27 min&nbsp;·&nbsp;hxx

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#intuition" aria-label="Intuition">Intuition</a></li>
                <li>
                    <a href="#introduction-of-reinforcement-learning" aria-label="Introduction of Reinforcement Learning">Introduction of Reinforcement Learning</a><ul>
                        
                <li>
                    <a href="#components-of-rl" aria-label="Components of RL">Components of RL</a><ul>
                        
                <li>
                    <a href="#21-policy" aria-label="2.1 Policy">2.1 Policy</a></li>
                <li>
                    <a href="#22-value-function" aria-label="2.2 Value Function">2.2 Value Function</a></li>
                <li>
                    <a href="#23-model" aria-label="2.3 Model">2.3 Model</a></li></ul>
                </li>
                <li>
                    <a href="#optimization-of-rl" aria-label="Optimization of RL">Optimization of RL</a></li>
                <li>
                    <a href="#31-value-based-agent" aria-label="3.1 Value-based agent">3.1 Value-based agent</a></li>
                <li>
                    <a href="#32-policy-based-agent" aria-label="3.2 Policy-based agent">3.2 Policy-based agent</a></li>
                <li>
                    <a href="#33-actor-critic" aria-label="3.3 Actor-Critic">3.3 Actor-Critic</a></li></ul>
                </li>
                <li>
                    <a href="#4-example-problem-cliff-walking-problem" aria-label="4. Example Problem, Cliff Walking Problem">4. Example Problem, Cliff Walking Problem</a><ul>
                        
                <li>
                    <a href="#41-problem-statement" aria-label="4.1 Problem Statement:">4.1 Problem Statement:</a><ul>
                        
                <li>
                    <a href="#411-define-the-environemnt" aria-label="4.1.1 Define the Environemnt">4.1.1 Define the Environemnt</a></li>
                <li>
                    <a href="#412-define-the-step-function" aria-label="4.1.2 Define the Step function">4.1.2 Define the Step function</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#5-dynamic-programming-methods" aria-label="5. Dynamic Programming Methods">5. Dynamic Programming Methods</a><ul>
                        
                <li>
                    <a href="#51-introduction" aria-label="5.1 Introduction">5.1 Introduction</a></li>
                <li>
                    <a href="#511-markov-property" aria-label="5.1.1 Markov Property">5.1.1 Markov Property</a></li>
                <li>
                    <a href="#512-bellman-optimal-function" aria-label="5.1.2 Bellman Optimal Function">5.1.2 Bellman Optimal Function</a></li>
                <li>
                    <a href="#52-value-iteration" aria-label="5.2 Value Iteration">5.2 Value Iteration</a><ul>
                        
                <li>
                    <a href="#521-value-iteration-for-cliff-walking" aria-label="5.2.1 Value Iteration for Cliff Walking">5.2.1 Value Iteration for Cliff Walking</a></li>
                <li>
                    <a href="#522-python-implementation-of-value-iteration-for-cliff-walking" aria-label="5.2.2 Python Implementation of Value Iteration for Cliff Walking">5.2.2 Python Implementation of Value Iteration for Cliff Walking</a></li>
                <li>
                    <a href="#523-result-and-visualization" aria-label="5.2.3 Result and Visualization">5.2.3 Result and Visualization</a></li></ul>
                </li>
                <li>
                    <a href="#53-policy-iteration" aria-label="5.3 Policy Iteration">5.3 Policy Iteration</a><ul>
                        
                <li>
                    <a href="#531-policy-iteration-for-cliff-walking" aria-label="5.3.1 Policy Iteration for Cliff Walking">5.3.1 Policy Iteration for Cliff Walking</a></li>
                <li>
                    <a href="#532-python-implementation-of-policy-iteration-for-cliff-walking" aria-label="5.3.2 Python Implementation of Policy Iteration for Cliff Walking">5.3.2 Python Implementation of Policy Iteration for Cliff Walking</a></li>
                <li>
                    <a href="#533-results-and-evaluation" aria-label="5.3.3 Results and Evaluation">5.3.3 Results and Evaluation</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#6-monte-carlo-methods" aria-label="6. Monte-Carlo Methods">6. Monte-Carlo Methods</a><ul>
                        
                <li>
                    <a href="#61-interaction-environment" aria-label="6.1 Interaction Environment">6.1 Interaction Environment</a></li>
                <li>
                    <a href="#62-monte-carlo-simulation" aria-label="6.2 Monte-Carlo Simulation">6.2 Monte-Carlo Simulation</a></li>
                <li>
                    <a href="#621-epsilon-search-algorithm" aria-label="6.2.1 $\epsilon$-search algorithm">6.2.1 $\epsilon$-search algorithm</a></li>
                <li>
                    <a href="#622-monte-carlo-method-in-cliff-walking" aria-label="6.2.2 Monte-Carlo method in Cliff Walking">6.2.2 Monte-Carlo method in Cliff Walking</a></li>
                <li>
                    <a href="#63-python-implementation-of-mc-in-cliff-walking" aria-label="6.3 Python Implementation of MC in Cliff Walking">6.3 Python Implementation of MC in Cliff Walking</a><ul>
                        
                <li>
                    <a href="#631-epsilon-greedy-search" aria-label="6.3.1 $\epsilon$-greedy search">6.3.1 $\epsilon$-greedy search</a></li>
                <li>
                    <a href="#632-mc-simulation" aria-label="6.3.2 MC-Simulation">6.3.2 MC-Simulation</a></li>
                <li>
                    <a href="#633-value-function-update" aria-label="6.3.3 Value Function Update">6.3.3 Value Function Update</a></li>
                <li>
                    <a href="#634-monte-carlo-training" aria-label="6.3.4 Monte-Carlo training">6.3.4 Monte-Carlo training</a></li>
                <li>
                    <a href="#635-results-and-visualizations" aria-label="6.3.5 Results and Visualizations">6.3.5 Results and Visualizations</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#7-temporal-difference-methods" aria-label="7. Temporal-Difference Methods">7. Temporal-Difference Methods</a><ul>
                        
                <li>
                    <a href="#71-sarsa" aria-label="7.1. SARSA">7.1. SARSA</a></li>
                <li>
                    <a href="#711-sarsa-method-in-cliff-walking" aria-label="7.1.1 SARSA method in Cliff Walking">7.1.1 SARSA method in Cliff Walking</a></li>
                <li>
                    <a href="#712-python-implementation-of-sarsa" aria-label="7.1.2 Python Implementation of SARSA">7.1.2 Python Implementation of SARSA</a><ul>
                        
                <li>
                    <a href="#7121-sarsa" aria-label="7.1.2.1 SARSA">7.1.2.1 SARSA</a></li>
                <li>
                    <a href="#7122-visualization-and-result" aria-label="7.1.2.2 Visualization and Result">7.1.2.2 Visualization and Result</a></li></ul>
                </li>
                <li>
                    <a href="#72-q-learning" aria-label="7.2 Q-Learning">7.2 Q-Learning</a></li>
                <li>
                    <a href="#721-q-learning-in-cliff-walking" aria-label="7.2.1 Q-Learning in Cliff Walking">7.2.1 Q-Learning in Cliff Walking</a></li>
                <li>
                    <a href="#722-python-implementation-of-q-learning" aria-label="7.2.2 Python Implementation of Q-Learning">7.2.2 Python Implementation of Q-Learning</a><ul>
                        
                <li>
                    <a href="#723-visualization-and-result" aria-label="7.2.3 Visualization and Result">7.2.3 Visualization and Result</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#8-summary" aria-label="8. Summary">8. Summary</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="intuition">Intuition<a hidden class="anchor" aria-hidden="true" href="#intuition">#</a></h2>
<p>I&rsquo;ve been having interests in Reinforcement Learning(RL) for a while, especially after ChatGPT when everyone was talking about <code>RLHF</code>, but often felt it&rsquo;s too daunting for me to do some reading and dig into it. Recently I finally found some spare time to go through it, and fortunately the famous Richard Sutton&rsquo;s &lt;Reinforcement Learning: an Introduction&gt; book is surprisingly intuitive even for RL beginners.</p>
<p>However, the mathematical notions used in RL is quite different from those used in supervised learning, and may felt weird or confusing for beginners just like me. As a reference, I will share and summarize my reading notes here.</p>
<h2 id="introduction-of-reinforcement-learning">Introduction of Reinforcement Learning<a hidden class="anchor" aria-hidden="true" href="#introduction-of-reinforcement-learning">#</a></h2>
<p>So first of all, why is RL needed since we already have supervised/unsupervised learning? And how is RL different from the other methods?</p>
<p>Wikipedia&rsquo;s definition of the term <a href="https://en.wikipedia.org/wiki/Machine_learning"><strong>Machine Learning</strong></a> is: <em>statistical algorithms that can learn from data and generaize to unseen data.</em> Say we have data $X$, and its training data sample $x_i$, we are trying to learning the distribution about $X$.</p>
<ul>
<li>
<p>In supervised learning, the data is pair of $&lt;x_i, y_i&gt;$, so we are trying to learn distribution $p(y_ | x_)$. For example, in image classification, given a image $x$, trying to predict its label $y$. If we viewed GPT as a special case of supervised learning, it can be seen as given a text sequence&rsquo;s previous tokens, predict the next token.</p>
</li>
<li>
<p>In reinforcement learning, the $x$ is no longer static, or known before training. If in supervised learning the goal is to learn a mapping from $x_i$ to $y_i$, in RL it became to learn the mapping from situations to actions, both can only get through direct interaction with the environment.</p>
</li>
</ul>
<h3 id="components-of-rl">Components of RL<a hidden class="anchor" aria-hidden="true" href="#components-of-rl">#</a></h3>
<p>We defined reinforcement learning as a method to model the interaction in a game/strategy etc. There are two entities in such RL system:</p>
<p><img alt="Agent Environment Interaction" loading="lazy" src="/images/ae1.png"></p>
<ul>
<li>$Agent$ is the learner and decision-maker.</li>
<li>$Environment$ is everything outside the agent that it interacts with.</li>
</ul>
<p>Then there are three types of interactions between agent and environment:</p>
<ul>
<li>$Action$ is an action the agent takes to interact with the environment.</li>
<li>$State$ is a representation of the environment, which is changed by action.</li>
<li>$Reward$ can be seen as the environment&rsquo;s corresponding response to the agent based on its state.</li>
</ul>
<p>A sequence of interactions is:</p>
<ul>
<li>In $t$, the environment&rsquo;s state is $S_t$, it exhibits reward $R_t$</li>
<li>Based on $S_t$ and $R_t$, the agent takes action $A_t$</li>
<li>As a result of $A_t$, the state changed to $S_{t+1}$, and exhibits reward $R_{t+1}$</li>
</ul>
<p>The goal of an RL system is to learn a strategy so the agent makes the best action in each step to maximize its rewards. We can define three components of such agent:</p>
<ul>
<li>$Policy$ is the strategy the agent used to determine its next action.</li>
<li>$Value\ Function$ is a function the agent used to evaluate its current state</li>
<li>$Model$ is the way the environment interacts with the agent, it defined how the environment works.</li>
</ul>
<h4 id="21-policy">2.1 Policy<a hidden class="anchor" aria-hidden="true" href="#21-policy">#</a></h4>
<p>Policy is a mapping from current state $s_t$ to action $a_t$. It can be deterministic or stochastic:</p>
<p>In <strong>Stochastic Policy</strong>, the result is a ditribution, in which the agent sample its action from.</p>
<p>$\pi(a_{t} | s_{t}) = p(a = a_t | s=s_t) $</p>
<p>In <strong>Deterministic Policy</strong>, the result is a action, for a given state, the agent will surely execute a specific action.</p>
<p>$a_{t} = argmax\ \pi(a|s_t)$</p>
<h4 id="22-value-function">2.2 Value Function<a hidden class="anchor" aria-hidden="true" href="#22-value-function">#</a></h4>
<p>In an RL system, the end goal is to find a strategy to win the game, like in Chess or Go. At any timestamp t, we wants to have a metric to evaluate the effectiveness of current strategy.</p>
<p>The reward $R_t$ defined above only refers to immediate reward at timestamp $t$, but not the overall winning chance. So we defined $V_t$ as the overall reward at timestamp $t$, which includes the current immediate reward $R_t$, and its expected future reward $V_{t+1}$.</p>
<p>$V_t = R_t + \gamma V_{t+1}$</p>
<p>So the reward at timestamp $t$ is decided by its current state $s_t$, its taken action $a_t$, and its next state $s_{t+1}$.</p>
<p>$r_t = R(s_t, a_t, s_{t+1})$</p>
<p>Assume we take $T$ steps from timestamp $0$ to $T-1$, all these steps form a trajectory $\tau$, the sum reward can be represented as the reward over the trajectory.</p>
<p>$$
\tau = (s_0, a_0, r_0, s_1, a_1, r_1, &hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \</p>
<p>R(\tau) = \sum_{t=0}^{T-1}r_t <br>
$$</p>
<h4 id="23-model">2.3 Model<a hidden class="anchor" aria-hidden="true" href="#23-model">#</a></h4>
<p>A $model$ defined how the environment interacts with the agent. A model is comprised of $&lt;S, A, P, R&gt;$</p>
<ul>
<li>$S$, the space of all possible states</li>
<li>$A$, the space of all possible actions</li>
<li>$P$, transformation function of how state changed, $P(s_{t+1}|s_t, a_t)$</li>
<li>$R$, how reward is calculated for each state, $R(s_t, a_t)$</li>
</ul>
<p>If all these 4 elements are known, we can easily model the interaction without actual interaction, this is called <em>model based learning</em>.</p>
<p>In reallity, while $S$ and $A$ is often known, the transformation and reward part is either fully unknown or hard to estimate, so agent need to interact with real environment to observe the state and reward, this is called <em>model-free learning</em>.</p>
<p>Comparing the two, <em>model-free learning</em> relied on real interaction to get the next state and reward, while <em>model based learning</em> modeled these without specific interaction.</p>
<h3 id="optimization-of-rl">Optimization of RL<a hidden class="anchor" aria-hidden="true" href="#optimization-of-rl">#</a></h3>
<p>The optimization of a RL task can be divided into two parsts:</p>
<ul>
<li><strong>Value estimation</strong>: given a strategy $\pi$, evaluate the effectiveness of how does the strategy work, this is computing $V_\pi$.</li>
<li><strong>Policy optimization</strong>: given the value function $V_\pi$, optimize to get a better policy $\pi$.</li>
</ul>
<p>In essence this is very similar to k-Means clustering that we have two optimization targets, and we opmitize them iteratively to get the optimial answer. In k-means,</p>
<ul>
<li>Given the current cluster assignment of each point, we compute the optimal centroid. Similar to <em>value estimation</em>.</li>
<li>Given the current optimial centroid, we find better cluster assignment for each point. Similar to <em>policy optimization</em>.</li>
</ul>
<p>But are the two steps both necessary in RL optimization? Answer is No.</p>
<h3 id="31-value-based-agent">3.1 Value-based agent<a hidden class="anchor" aria-hidden="true" href="#31-value-based-agent">#</a></h3>
<p>An agent can only learn the value function $V_\pi$, it can maintain a table of mapping from $&lt;S, A&gt;$ to $V$, then in each step, it picked the action that will maximize the ultimate value. In this type of work, the agent doesn&rsquo;t explicitely have a strategy or policy.</p>
<p>$ a_t = argmax\ V(a | s_t)$</p>
<h3 id="32-policy-based-agent">3.2 Policy-based agent<a hidden class="anchor" aria-hidden="true" href="#32-policy-based-agent">#</a></h3>
<p>A policy agent directly learns the policy, and each step it directly outputs the distribution of next action without knowing value function.</p>
<p>$ a_t \sim P(a|s_t)$</p>
<h3 id="33-actor-critic">3.3 Actor-Critic<a hidden class="anchor" aria-hidden="true" href="#33-actor-critic">#</a></h3>
<p>An agent can learn both $\pi$ and $V_{\pi}$ as we described above.</p>
<ul>
<li>Actor refers to learning of policy $\pi$</li>
<li>Critic refers to learning of value function $V_\pi$</li>
</ul>
<h2 id="4-example-problem-cliff-walking-problem">4. Example Problem, Cliff Walking Problem<a hidden class="anchor" aria-hidden="true" href="#4-example-problem-cliff-walking-problem">#</a></h2>
<p>Now let&rsquo;s work on a problem together to walk through all the different pieces and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning (RL) environment introduced in Sutton &amp; Barto’s book, “Reinforcement Learning: An Introduction.”</p>
<h3 id="41-problem-statement">4.1 Problem Statement:<a hidden class="anchor" aria-hidden="true" href="#41-problem-statement">#</a></h3>
<ul>
<li>The world is represented as a 4×12 grid world.</li>
<li>The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$.</li>
<li>The bottom row between the start and goal is called the cliff $(3, 1-10)$ — if the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start.</li>
<li>Each non-terminal move incurs a reward of -1.</li>
</ul>
<p>Now let&rsquo;s map this problem statement to different components of RL system.</p>
<ul>
<li>$Agent$, the robot that exists in the grid world, the agent needs to find a path from start position to end position to collect the rewards.</li>
<li>$Environment$, the 4x12 grid world, as well as the transition and reward for each move. This environment is fully observable and deterministic.</li>
<li>$State$, the state space is all the possible locations of the agent on the grid, there are 48 grids and minus the 10 cliff grids, there are 38 possible grids.</li>
<li>$Action$, the action space is all the possible moves the agent can take. There are 4 possible actions, <code>UP</code>, <code>DOWN</code>, <code>LEFT</code>, <code>RIGHT</code>.</li>
<li>$Reward$, a scalar signal from the environment for the agent&rsquo;s each move. We can define it as:
<ul>
<li>-1 for each normal move.</li>
<li>-100 if the agent fells into the cliff.</li>
<li>0 upon reaching the goal location.</li>
</ul>
</li>
<li>$Policy$, the strategy the agent should take to reach the goal state. Here it should be a mapping from $state$ to $action$, here it tells what direction should the agent take in each grid cell. For example, <code>(3, 0) -&gt; MOVE UP</code>. The end goal for this problem is to find such a policy.</li>
</ul>
<h4 id="411-define-the-environemnt">4.1.1 Define the Environemnt<a hidden class="anchor" aria-hidden="true" href="#411-define-the-environemnt">#</a></h4>
<p>We can define the  environment as following.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CliffWalk</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, height, width, start, end, cliff):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>height <span style="color:#f92672">=</span> height
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>width <span style="color:#f92672">=</span> width
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>start <span style="color:#f92672">=</span> start
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>end <span style="color:#f92672">=</span> end
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>cliff <span style="color:#f92672">=</span> set(cliff)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actions <span style="color:#f92672">=</span> [(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>), (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)]  <span style="color:#75715e"># up, down, left, right</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">reset</span>(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>agent_pos <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>start
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>agent_pos
</span></span></code></pre></div><p>Then we can define the cells for start, end and cliff cells, and initialize a environment.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>start <span style="color:#f92672">=</span> (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>end <span style="color:#f92672">=</span> (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">11</span>)
</span></span><span style="display:flex;"><span>height <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>width <span style="color:#f92672">=</span> <span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>cliff_cells <span style="color:#f92672">=</span> [(<span style="color:#ae81ff">3</span>, i) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">11</span>)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>env <span style="color:#f92672">=</span> CliffWalk(height, width, start, end, cliff_cells)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Check github for visualization code.</span>
</span></span><span style="display:flex;"><span>env<span style="color:#f92672">.</span>render_plot()
</span></span></code></pre></div><p><img alt="Cliff Walking Visualization" loading="lazy" src="/images/CliffWalking-04-08-2025_10_12_AM.png"></p>
<h4 id="412-define-the-step-function">4.1.2 Define the Step function<a hidden class="anchor" aria-hidden="true" href="#412-define-the-step-function">#</a></h4>
<p>Next, we define rules and rewards for the agent&rsquo;s action.</p>
<ul>
<li>In <code>self.actions</code> we defined 4 type of actions, each representing walking 1 step in the direction.</li>
<li>The <code>step</code> function takes in a current location <code>(i, j)</code> and <code>action</code> index, execute it and returns a tuple representing:
<ul>
<li>The agents new position after the action</li>
<li>Reward of current action</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">step</span>(self, i: int, j: int, a: int) <span style="color:#f92672">-&gt;</span> tuple[tuple[int, int], int]:
</span></span><span style="display:flex;"><span>        ni, nj <span style="color:#f92672">=</span> i <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>actions[a][<span style="color:#ae81ff">0</span>], j <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>actions[a][<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Fell into Cliff, get -100 reward, back to start point</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (ni, nj) <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>cliff:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>start, <span style="color:#f92672">-</span><span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (ni, nj) <span style="color:#f92672">==</span> self<span style="color:#f92672">.</span>end:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> (ni, nj), <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Move, get -1 reward</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">&lt;=</span> ni <span style="color:#f92672">&lt;</span> self<span style="color:#f92672">.</span>height <span style="color:#f92672">and</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">&lt;=</span> nj <span style="color:#f92672">&lt;</span> self<span style="color:#f92672">.</span>width:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> (ni, nj), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Move out of grid, get -1 reward</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> (i, j), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
</span></span></code></pre></div><p>The moving rules and rewards are:</p>
<ul>
<li>Agent received -1 for each normal move.</li>
<li>Agent received -100 if fell off the cliff.</li>
<li>Agent received 0 if reaching the goal.</li>
<li>If the agent moved out of the grid, it stayed still in the same grid and still received -1.</li>
</ul>
<p>Now we introduced the environment setup of the cliff walking problem, now let&rsquo;s try to solve it with three classes of RL methods, which is dynamic programming, monte-carlo and temporal-difference methods. Comparison between these three methods will be given at the end of the article.</p>
<h2 id="5-dynamic-programming-methods">5. Dynamic Programming Methods<a hidden class="anchor" aria-hidden="true" href="#5-dynamic-programming-methods">#</a></h2>
<h3 id="51-introduction">5.1 Introduction<a hidden class="anchor" aria-hidden="true" href="#51-introduction">#</a></h3>
<h3 id="511-markov-property">5.1.1 Markov Property<a hidden class="anchor" aria-hidden="true" href="#511-markov-property">#</a></h3>
<p>A Markov decision process(MDP) is defined by 5 elements:
$$
MDP = &lt;S, A, P, R, \gamma&gt;
$$</p>
<ul>
<li>$S$: state space</li>
<li>$A$: action space</li>
<li>$P$: <strong>transition probability</strong> from a state and action to its next state, $p(s_{t+1}|st, at)$</li>
<li>$R$: <strong>reward function</strong> immediate reward after a transition, $r(s_{t+1},st, at)$</li>
<li>$\gamma$: <strong>discount factor</strong> that weights the importance of future reward.</li>
</ul>
<p>We define a deicision process has <strong>Markov Property</strong> if its next state and reward only depend on its current state and action, not the full history. We can see the cliff walking problem suffices the <strong>markovian propterty</strong>.</p>
<h3 id="512-bellman-optimal-function">5.1.2 Bellman Optimal Function<a hidden class="anchor" aria-hidden="true" href="#512-bellman-optimal-function">#</a></h3>
<p>We define the optimal value function is:
$$
V^{<em>}(s) = maxV_\pi(s)
$$
Here we searched a policy $\pi$ to maximize the state $V$, the result policy is our optimal policy.
$$
\pi^{</em>}(s) = argmaxV_\pi(s)
$$
For each state $s$, we searched over its possible actions to maximize the value:
$$
\pi^{<em>}(a | s) = 1, a = argmaxQ^</em>(s, a)
$$</p>
<h3 id="52-value-iteration">5.2 Value Iteration<a hidden class="anchor" aria-hidden="true" href="#52-value-iteration">#</a></h3>
<h4 id="521-value-iteration-for-cliff-walking">5.2.1 Value Iteration for Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#521-value-iteration-for-cliff-walking">#</a></h4>
<ul>
<li>Initialize each state&rsquo;s value function to 0: $V(s) = 0$</li>
<li>For each state $s_t$
<ul>
<li>seach over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$</li>
<li>update current state&rsquo;s value function with the action that beares largest reward. $V(s_t) = \underset{a}{max}(r_a + V(s_{t+1}))$</li>
</ul>
</li>
<li>Repeat the previous steps until convergence</li>
</ul>
<h4 id="522-python-implementation-of-value-iteration-for-cliff-walking">5.2.2 Python Implementation of Value Iteration for Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#522-python-implementation-of-value-iteration-for-cliff-walking">#</a></h4>
<p>Below we defined one iteration for value update:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">value_iterate</span>(env, V, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Run one epoch of value iteration&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    V_new <span style="color:#f92672">=</span> V<span style="color:#f92672">.</span>copy()
</span></span><span style="display:flex;"><span>    policy <span style="color:#f92672">=</span> defaultdict(list)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    delta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(env<span style="color:#f92672">.</span>height):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(env<span style="color:#f92672">.</span>width):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> (i, j) <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> cliff_cells <span style="color:#f92672">and</span> (i, j) <span style="color:#f92672">!=</span> (end):
</span></span><span style="display:flex;"><span>                values <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">for</span> a <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">4</span>):
</span></span><span style="display:flex;"><span>                    (i_new, j_new), reward<span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(i, j, a)
</span></span><span style="display:flex;"><span>                    values<span style="color:#f92672">.</span>append(V[(i_new, j_new)] <span style="color:#f92672">*</span> gamma <span style="color:#f92672">+</span> reward)
</span></span><span style="display:flex;"><span>                max_value <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(values)
</span></span><span style="display:flex;"><span>                best_actions <span style="color:#f92672">=</span> [a <span style="color:#66d9ef">for</span> a, v <span style="color:#f92672">in</span> enumerate(values) <span style="color:#66d9ef">if</span> v <span style="color:#f92672">==</span> max_value]
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>                V_new[(i, j)] <span style="color:#f92672">=</span> max_value
</span></span><span style="display:flex;"><span>                policy[(i, j)] <span style="color:#f92672">=</span> best_actions
</span></span><span style="display:flex;"><span>                delta <span style="color:#f92672">=</span> max(delta, abs(V_new[(i, j)] <span style="color:#f92672">-</span> V[(i, j)]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> V_new, policy, delta
</span></span></code></pre></div><ul>
<li><code>V</code> is the value function, its key is a gird location of <code>(i, j)</code>, value is initlized to 0</li>
<li>We iterate over all grid locations that&rsquo;s not a cliff or goal location, for each grid, we iterated over its 4 actions, and pick the action with the largest value to update current value.</li>
<li>We used <code>max(abs(V_new[(i, j)] - V[i, j]))</code> as the difference between value iteartions.</li>
<li>Value iteration does not explicitly optimize the policy, instead it&rsquo;s learnt implicitily by selecting over an action that maximized its next value state.</li>
</ul>
<p>To train the value iteration until convergence:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">value_iteration_train</span>(env, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, tolerance<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>):
</span></span><span style="display:flex;"><span>    progress_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    V <span style="color:#f92672">=</span> defaultdict(float)
</span></span><span style="display:flex;"><span>    policy <span style="color:#f92672">=</span> defaultdict(list)
</span></span><span style="display:flex;"><span>    progress_data<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#34;V&#34;</span>: V<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;policy&#34;</span>: policy<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;delta&#34;</span>: float(<span style="color:#e6db74">&#34;inf&#34;</span>)})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        V, policy, delta <span style="color:#f92672">=</span> value_iterate(env, V, gamma)
</span></span><span style="display:flex;"><span>        progress_data<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#34;V&#34;</span>: V<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;policy&#34;</span>: policy<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;delta&#34;</span>: delta})
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> delta <span style="color:#f92672">&lt;</span> tolerance:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> progress_data
</span></span></code></pre></div><ul>
<li><code>gamma</code> is a discounted factor that defined the future reward&rsquo;s current value</li>
<li>The training iteration stoped until the difference between two value functions are <code>&lt;tolerance</code>.</li>
<li>We returned the <code>V</code> and <code>policy</code> data during training for evaluation purpose</li>
</ul>
<h4 id="523-result-and-visualization">5.2.3 Result and Visualization<a hidden class="anchor" aria-hidden="true" href="#523-result-and-visualization">#</a></h4>
<p>We run training using <code>gamma=0.9</code>, it converges in 15 epoches</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dp_progress <span style="color:#f92672">=</span> value_iteration_train(env, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, tolerance<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Trained </span><span style="color:#e6db74">{</span>len(dp_progress)<span style="color:#e6db74">}</span><span style="color:#e6db74"> epoches&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Trained 15 epoches
</span></span></code></pre></div><p>We visulize both the value function and policy in epoch <code>1, 7, 14</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>epoches <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">14</span>]
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, ax <span style="color:#f92672">in</span> enumerate(axs):
</span></span><span style="display:flex;"><span>    iter <span style="color:#f92672">=</span> epoches[i <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> i <span style="color:#f92672">%</span> <span style="color:#ae81ff">2</span>:
</span></span><span style="display:flex;"><span>        env<span style="color:#f92672">.</span>render_plot(policy<span style="color:#f92672">=</span>dp_progress[iter][<span style="color:#e6db74">&#39;policy&#39;</span>], title <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Cliff Walking Policy in Epoch </span><span style="color:#e6db74">{</span>iter<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>, ax<span style="color:#f92672">=</span>ax)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        env<span style="color:#f92672">.</span>render_plot(value<span style="color:#f92672">=</span>dp_progress[iter][<span style="color:#e6db74">&#39;V&#39;</span>], title <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Cliff Walking Value in Epoch </span><span style="color:#e6db74">{</span>iter<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>, ax<span style="color:#f92672">=</span>ax)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p>From the visuliaztion:</p>
<ul>
<li>In epoch 1, the agent learnt to avoid the cliff</li>
<li>In epoch 7, the agent learnt the best actions on right side of the grid, which is either take <code>DOWN</code> or <code>RIGHT</code> action to reach the goal grid.</li>
<li>In eppch 14, the value function converges, which the optimal path now is to take <code>UP</code> from start then always take <code>RIGHT</code> until close to the goal.</li>
</ul>
<p><img alt="Value Iteration Visualization" loading="lazy" src="/images/CliffWalking-04-08-2025_01_17_PM.png"></p>
<h3 id="53-policy-iteration">5.3 Policy Iteration<a hidden class="anchor" aria-hidden="true" href="#53-policy-iteration">#</a></h3>
<p>In previous <strong>Value Iteration</strong> method, during iterations we only updated the value function until convergence, the policy is derived implicitely from the value function. So can we optimize the policy directly? This comes into another dynamic programming method in MDPs, <strong>Policy Iteration</strong>.</p>
<p>A policy iteration consists of two parts:</p>
<ol>
<li>
<p><strong>Policy Evaluation</strong>, given a policy $\pi$, compute its state-value function $V^{\pi}(s)$, which is the expected return of following the policy $\pi$.
$$
V(s_t) = \sum P(s_{t+1} | s_t, \pi) * [r(s_t, \pi, s_{t+1}) + \gamma * V(s_{t+1})]
$$</p>
</li>
<li>
<p><strong>Policy Improvment</strong>, update the agent&rsquo;s policy respect to the current value function.
$$
\pi_{new}(s) = \underset{a}{argmax}\ \sum P(s_{t+1} | s_t, \pi) * [r(s_t, \pi, s_{t+1}) + \gamma * V(s_{t+1})]
$$</p>
</li>
</ol>
<h4 id="531-policy-iteration-for-cliff-walking">5.3.1 Policy Iteration for Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#531-policy-iteration-for-cliff-walking">#</a></h4>
<ul>
<li>Initialization:
<ul>
<li>Intialize each state&rsquo;s value function to 0: $V(s) = 0$</li>
<li>Initialize policy to take all 4 actions in all states.</li>
</ul>
</li>
<li>Step 1, <strong>policy evaluation</strong>, for each state $s_t$
<ul>
<li>Search over its policy&rsquo;s actions $a_t$  each $a_t$ leads to a new state $s_{t+1}$</li>
<li>Update current state&rsquo;s value function with the mean reward of policy actions. $V(s_t) = \underset{a}{mean}(r_a + \gamma * V(s_{t+1}))$</li>
<li>Repeat until the value function convergent.</li>
</ul>
</li>
<li>Step 2, <strong>policy improvement</strong>, for each state $s_t$
<ul>
<li>Seach over current policy&rsquo;s actions $a_t$ at each $s_t$, compute its value function.</li>
<li>Update the policy $\pi(s_t)$ by only keeping actions with the largest value function.</li>
</ul>
</li>
<li>Repeat step 1 and 2 until the policy doens&rsquo;t change.</li>
</ul>
<h4 id="532-python-implementation-of-policy-iteration-for-cliff-walking">5.3.2 Python Implementation of Policy Iteration for Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#532-python-implementation-of-policy-iteration-for-cliff-walking">#</a></h4>
<p>Let&rsquo;s first implement the <strong>policy evaluation</strong> function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">policy_eval</span>(env, V, policy, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, tolerance<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-4</span>):
</span></span><span style="display:flex;"><span>    V_new <span style="color:#f92672">=</span> V<span style="color:#f92672">.</span>copy()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        delta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(env<span style="color:#f92672">.</span>height):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(env<span style="color:#f92672">.</span>width):
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">if</span> (i, j) <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> env<span style="color:#f92672">.</span>cliff <span style="color:#f92672">and</span> (i, j) <span style="color:#f92672">!=</span> env<span style="color:#f92672">.</span>end:
</span></span><span style="display:flex;"><span>                    values <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>                    <span style="color:#66d9ef">for</span> a <span style="color:#f92672">in</span> policy[(i, j)]:
</span></span><span style="display:flex;"><span>                        (i_new, j_new), reward <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(i, j, a)
</span></span><span style="display:flex;"><span>                        values<span style="color:#f92672">.</span>append(V[(i_new, j_new)] <span style="color:#f92672">*</span> gamma <span style="color:#f92672">+</span> reward)
</span></span><span style="display:flex;"><span>                    
</span></span><span style="display:flex;"><span>                    V_new[(i, j)] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(values)
</span></span><span style="display:flex;"><span>                    delta <span style="color:#f92672">=</span> max(delta, abs(V_new[(i, j)] <span style="color:#f92672">-</span> V[(i, j)]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> delta <span style="color:#f92672">&lt;</span> tolerance:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>        V <span style="color:#f92672">=</span> V_new<span style="color:#f92672">.</span>copy()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> V_new
</span></span></code></pre></div><ul>
<li>This function is very similar to the <code>value_iterate</code> function in value interation, except one major difference:
<ul>
<li>In <code>value_iterate</code>, we compute value functions among all actions and used <code>np.max(values)</code> to pick the best action, which means we are implicitely changing the policy using <code>argmax</code>.</li>
<li>In <code>policy_eval</code>, we only iterate actions in existing policy <code>policy[(i, j)]</code>, and used <code>np.mean</code> to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here.</li>
</ul>
</li>
<li>The function returned a new value function <code>V_new</code> after convergence.</li>
</ul>
<p>Then let&rsquo;s implement the <strong>policy improvement</strong> step:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">policy_improve</span>(env, V, policy, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>):
</span></span><span style="display:flex;"><span>    policy_new <span style="color:#f92672">=</span> defaultdict(list)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    policy_stable <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(env<span style="color:#f92672">.</span>height):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(env<span style="color:#f92672">.</span>width):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> (i, j) <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> env<span style="color:#f92672">.</span>cliff <span style="color:#f92672">and</span> (i, j) <span style="color:#f92672">!=</span> env<span style="color:#f92672">.</span>end:
</span></span><span style="display:flex;"><span>                values <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">for</span> a <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">4</span>):
</span></span><span style="display:flex;"><span>                    (i_new, j_new), reward <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(i, j, a)
</span></span><span style="display:flex;"><span>                    values<span style="color:#f92672">.</span>append(V[(i_new, j_new)] <span style="color:#f92672">*</span> gamma <span style="color:#f92672">+</span> reward)
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>                max_val <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(values)
</span></span><span style="display:flex;"><span>                best_actions <span style="color:#f92672">=</span> [a <span style="color:#66d9ef">for</span> a, v <span style="color:#f92672">in</span> enumerate(values) <span style="color:#66d9ef">if</span> v <span style="color:#f92672">==</span> max_val]
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">if</span> set(best_actions) <span style="color:#f92672">!=</span> set(policy[(i, j)]):
</span></span><span style="display:flex;"><span>                    policy_stable <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>                policy_new[(i, j)] <span style="color:#f92672">=</span> best_actions
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> policy_new, policy_stable
</span></span></code></pre></div><ul>
<li>The <code>policy_improve</code> is a one step optimization, it takes in the current value function <code>V</code>, picked the <code>argmax</code> action to update the policy, it also takes in current policy <code>policy</code> to compare whether there is any changes between the two policy. It returns both the updated policy <code>policy_new</code> and a boolean indicated whether the policy changed during optimization.</li>
</ul>
<p>Combining these two sub-steps, we can train using policy iteration:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">policy_iteration_train</span>(env, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, tolerance<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>):
</span></span><span style="display:flex;"><span>    V <span style="color:#f92672">=</span> defaultdict(float)
</span></span><span style="display:flex;"><span>    policy <span style="color:#f92672">=</span> defaultdict(<span style="color:#66d9ef">lambda</span> : range(<span style="color:#ae81ff">4</span>))    
</span></span><span style="display:flex;"><span>    progress_data <span style="color:#f92672">=</span> [{<span style="color:#e6db74">&#34;V&#34;</span>: V<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;policy&#34;</span>: policy<span style="color:#f92672">.</span>copy()}]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Value evaluation</span>
</span></span><span style="display:flex;"><span>        V <span style="color:#f92672">=</span> policy_eval(env, V, policy, gamma)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Policy improvement</span>
</span></span><span style="display:flex;"><span>        policy, policy_stable <span style="color:#f92672">=</span> policy_improve(env, V, policy, gamma)
</span></span><span style="display:flex;"><span>        progress_data<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#34;V&#34;</span>: V<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;policy&#34;</span>: policy<span style="color:#f92672">.</span>copy()})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> policy_stable:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>        idx <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> progress_data
</span></span></code></pre></div><ul>
<li>For all states, value function <code>V</code> is default to 0, policy is default to all 4 actions.</li>
<li>The training iteration stoped until the the policy no longer changed.</li>
</ul>
<h4 id="533-results-and-evaluation">5.3.3 Results and Evaluation<a hidden class="anchor" aria-hidden="true" href="#533-results-and-evaluation">#</a></h4>
<p>We run training using gamma=0.9, it converges in 6 epoches</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dp_progress <span style="color:#f92672">=</span> policy_iteration_train(env, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, tolerance<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Trained </span><span style="color:#e6db74">{</span>len(dp_progress)<span style="color:#e6db74">}</span><span style="color:#e6db74"> epoches&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Trained 6 epoches
</span></span></code></pre></div><p>We also visualize the value function and policy in epoch <code>1, 3, 5</code>:</p>
<ul>
<li>In epoch 1, because the initialized policy includes all actions, this leads to grid in the <code>i=2</code> row has a low value function as it has 25% of falling into the cliff and incur <code>-100</code> reward, so the learnt policy for most grids is to move upward and avoid the cliff.</li>
<li>In later epoches, since the policy no longer includes actions that leads to fall off the cliff, the value function improved for all grids, also it learnt the optimal path towards the goal grid.</li>
</ul>
<p><img alt="Policy Iteration Visualization" loading="lazy" src="/images/CliffWalking-04-08-2025_02_49_PM.png"></p>
<h2 id="6-monte-carlo-methods">6. Monte-Carlo Methods<a hidden class="anchor" aria-hidden="true" href="#6-monte-carlo-methods">#</a></h2>
<p>It&rsquo;s nice that we solved the cliff walking problem with DP methods, and what&rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:</p>
<ul>
<li>The transition probability: $P(s_{t+1} | s_t, a_t)$</li>
<li>The reward function : $r(s_{t+1}, a_t, s_t)$</li>
</ul>
<p>What if the agent is in another environment that itself doesn&rsquo;t know any of such information ahead? Assume the agent was placed in the <code>start</code> location, with no knowledge about:</p>
<ul>
<li>where is the <code>goal</code> grid, and how to reach it.</li>
<li>Which grid it will go to if taking an action and what reward it will get.</li>
</ul>
<p>Then the agent need to interact with the environment to generate <strong>episodes</strong> (sequence of states, actions, rewards) until it reached the <code>goal</code> grid, and learn these information and otpimize the policy during the interaction.</p>
<p>Compare the two methods, DP is like a <strong>planner</strong> who knows the full map and compute the best path. Monte-Carlo is like an <strong>explorer</strong> that tries different routes and keep optimizing the policy.</p>
<h3 id="61-interaction-environment">6.1 Interaction Environment<a hidden class="anchor" aria-hidden="true" href="#61-interaction-environment">#</a></h3>
<p>We first need to chang the <code>CliffWalk</code> environment to mimic an interaction environment.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CliffWalk</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, height, width, start, end, cliff):
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">...</span> <span style="color:#75715e"># Ignore previous codes</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actions <span style="color:#f92672">=</span> [(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>), (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)]  <span style="color:#75715e"># up, down, left, right</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>agent_pos <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>start
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">reset</span>(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>agent_pos <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>start
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>agent_pos
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">step</span>(self, a: int) <span style="color:#f92672">-&gt;</span> tuple[tuple[int, int], int, bool]:
</span></span><span style="display:flex;"><span>        i, j <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>agent_pos
</span></span><span style="display:flex;"><span>        ni, nj <span style="color:#f92672">=</span> i <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>actions[a][<span style="color:#ae81ff">0</span>], j <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>actions[a][<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Fell into Cliff, get -100 reward, back to start point</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (ni, nj) <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>cliff:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>start, <span style="color:#f92672">-</span><span style="color:#ae81ff">100</span>, <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Move, get -1 reward</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">&lt;=</span> ni <span style="color:#f92672">&lt;</span> self<span style="color:#f92672">.</span>height <span style="color:#f92672">and</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">&lt;=</span> nj <span style="color:#f92672">&lt;</span> self<span style="color:#f92672">.</span>width:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>agent_pos <span style="color:#f92672">=</span> ni, nj
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Move out of grid, get -1 reward</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>agent_pos <span style="color:#f92672">=</span> i, j
</span></span><span style="display:flex;"><span>        done, reward <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>agent_pos <span style="color:#f92672">==</span> self<span style="color:#f92672">.</span>end:
</span></span><span style="display:flex;"><span>            done, reward <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>, <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>agent_pos, reward, done
</span></span></code></pre></div><p>Compare the new implementation of <code>step</code> function with previous one:</p>
<ul>
<li>The new implementation only takes an <code>action</code> index, it tracks the agent&rsquo;s state using <code>self.agent_pos</code></li>
<li>We are forbidden to compute the state and reward for any $&lt;state, action&gt;$ now. The agent has to reach to a specific $s_t$ and take an $a_t$, call <code>step</code> to finally get the $s_{t+1}, r_t$ from interaction.</li>
<li>The <code>step</code> function returns a boolean varaible <code>done</code> indicating whether the agent reached the <code>goal</code> grid</li>
</ul>
<h3 id="62-monte-carlo-simulation">6.2 Monte-Carlo Simulation<a hidden class="anchor" aria-hidden="true" href="#62-monte-carlo-simulation">#</a></h3>
<p>Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $&lt;state, action&gt;$ pair by averaging the total return oberseved after visiting a state across multiple episodes.</p>
<h3 id="621-epsilon-search-algorithm">6.2.1 $\epsilon$-search algorithm<a hidden class="anchor" aria-hidden="true" href="#621-epsilon-search-algorithm">#</a></h3>
<p>In RL system, it&rsquo;s very common to face the exploration vs exploitation dillema:</p>
<ul>
<li><strong>Exploitation</strong>: Pick the best known action so far (greedy)</li>
<li><strong>Exploration</strong>: Try other actions to discover potentially better ones</li>
</ul>
<p>If the agent always acts greedily, it may get stuck in suboptimal path, without getting oppourtunity to discover potential better paths. The $\epsilon$-search try to balance this by introducing a random $\epsilon$, in each step:</p>
<ul>
<li><strong>Exploration</strong>: With pobability $\epsilon$, choose a random action.</li>
<li><strong>Exploitation</strong>: With probability $1 - \epsilon$, choose action with highest value: $a = argmax\ Q(s, a)$</li>
</ul>
<h3 id="622-monte-carlo-method-in-cliff-walking">6.2.2 Monte-Carlo method in Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#622-monte-carlo-method-in-cliff-walking">#</a></h3>
<ul>
<li>
<p>Step 0, Initilization:</p>
<ul>
<li>Initialize a random value function: $Q(s, a)$</li>
<li>Initialize an $\epsilon$-greedy policy</li>
</ul>
</li>
<li>
<p>Step 1, Generate episodes:</p>
<ul>
<li>From the <code>start</code> state, follow current policy to generate full episode until the agent reached <code>goal</code> grid, we will get a sequence of $&lt;s_t, a_t, r_t&gt;$</li>
</ul>
</li>
<li>
<p>Step 2, Update value function: $Q(s, a)$</p>
<ul>
<li>For each $&lt;s_t, a_t&gt;$ pair in episode trace, compute its return by $G_t = r\ + \gamma*G_{t+1}$</li>
<li>Update $Q(s, a)$ by averaging returns across multiple episodes.</li>
</ul>
</li>
<li>
<p>Step 3, Improve policy:</p>
<ul>
<li>The new policy is the $\epsilon$-greedy policy with updated value function $Q(s, a)$.</li>
</ul>
</li>
<li>
<p>Repeat step 1-3 until the policy converges.</p>
</li>
</ul>
<h3 id="63-python-implementation-of-mc-in-cliff-walking">6.3 Python Implementation of MC in Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#63-python-implementation-of-mc-in-cliff-walking">#</a></h3>
<h4 id="631-epsilon-greedy-search">6.3.1 $\epsilon$-greedy search<a hidden class="anchor" aria-hidden="true" href="#631-epsilon-greedy-search">#</a></h4>
<p>This function implements the $\epsilon$-search to pick the action,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">epsilon_greedy</span>(action_values, epsilon):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand() <span style="color:#f92672">&lt;</span> epsilon:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Random action</span>
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Optimzed action</span>
</span></span><span style="display:flex;"><span>        max_val <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(action_values)
</span></span><span style="display:flex;"><span>        best_actions <span style="color:#f92672">=</span> [i <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">4</span>) <span style="color:#66d9ef">if</span> action_values[i] <span style="color:#f92672">==</span> max_val]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Random pick among best actions</span>
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(best_actions)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> action
</span></span></code></pre></div><h4 id="632-mc-simulation">6.3.2 MC-Simulation<a hidden class="anchor" aria-hidden="true" href="#632-mc-simulation">#</a></h4>
<p>This function simulates 1 episode of MC simulation.</p>
<ul>
<li>At the beginning, <code>env.reset()</code> set the agent to <code>start</code> state.</li>
<li><code>Q</code> is the Value table, with key is the current location <code>(i, j)</code>, value is a list of size 4, the value at index <code>k</code> represents value for action <code>k</code>.</li>
<li>The simulation stop after it reached the <code>goal</code> state.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mc_simulation</span>(env, Q, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    curr_eps <span style="color:#f92672">=</span> epsilon
</span></span><span style="display:flex;"><span>    episode_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> epsilon_greedy(Q[state], epsilon)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        next_state, reward, done <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step_interactive(action)
</span></span><span style="display:flex;"><span>        episode_data<span style="color:#f92672">.</span>append((state, action, reward))
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> next_state
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> episode_data, done
</span></span></code></pre></div><h4 id="633-value-function-update">6.3.3 Value Function Update<a hidden class="anchor" aria-hidden="true" href="#633-value-function-update">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">improve_policy</span>(Q, returns, episode_data, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Compute reward</span>
</span></span><span style="display:flex;"><span>    visited <span style="color:#f92672">=</span> set()
</span></span><span style="display:flex;"><span>    G <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> reversed(range(len(episode_data))):
</span></span><span style="display:flex;"><span>        state_t, action_t, reward_t <span style="color:#f92672">=</span> episode_data[t]
</span></span><span style="display:flex;"><span>        G <span style="color:#f92672">=</span> gamma<span style="color:#f92672">*</span>G <span style="color:#f92672">+</span> reward_t
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># First-time update</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (state_t, action_t) <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> visited:
</span></span><span style="display:flex;"><span>            visited<span style="color:#f92672">.</span>add((state_t, action_t))
</span></span><span style="display:flex;"><span>            returns[(state_t, action_t)]<span style="color:#f92672">.</span>append(G)
</span></span><span style="display:flex;"><span>            Q[state_t][action_t] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(returns[(state_t, action_t)])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    policy <span style="color:#f92672">=</span> defaultdict(int)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> Q<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        policy[k] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(v)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> Q, policy
</span></span></code></pre></div><ul>
<li><code>returns</code> is a dictionary, with key being a <code>&lt;state, action&gt;</code> combination, value being a list that stored its expected reward in each episode.</li>
<li>For each episode, we traversed backwards, iteratively computing each state&rsquo;s value using function: $G_t = r_t + \gamma * G_{t+1}$.</li>
</ul>
<h4 id="634-monte-carlo-training">6.3.4 Monte-Carlo training<a hidden class="anchor" aria-hidden="true" href="#634-monte-carlo-training">#</a></h4>
<p>Combining the previous steps, we can train the agent:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">monte_carlo_training</span>(env, num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>    Q <span style="color:#f92672">=</span> defaultdict(<span style="color:#66d9ef">lambda</span>: [<span style="color:#ae81ff">0.1</span>] <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>    returns <span style="color:#f92672">=</span> defaultdict(list)
</span></span><span style="display:flex;"><span>    progress_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> tqdm<span style="color:#f92672">.</span>tqdm(range(num_episodes)):
</span></span><span style="display:flex;"><span>        epsilon <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">0.01</span>, epsilon<span style="color:#f92672">*</span><span style="color:#ae81ff">0.99</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Run MC simulation</span>
</span></span><span style="display:flex;"><span>        episode_data, finished <span style="color:#f92672">=</span> mc_simulation(env, Q, epsilon)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> finished:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Policy improvement</span>
</span></span><span style="display:flex;"><span>        Q, policy <span style="color:#f92672">=</span> improve_policy(Q, returns, episode_data, gamma)
</span></span><span style="display:flex;"><span>        progress_data<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#34;Q&#34;</span>: copy<span style="color:#f92672">.</span>deepcopy(Q), <span style="color:#e6db74">&#34;policy&#34;</span>: copy<span style="color:#f92672">.</span>deepcopy(policy), <span style="color:#e6db74">&#34;episode&#34;</span>: episode})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> progress_data
</span></span></code></pre></div><ul>
<li><code>Q</code> is initilized by giving equal weights to each action.</li>
<li>We set $\epsilon$ to decay over episodes, <code>epsilon = max(0.01, epsilon*0.99)</code>. In earlier epoches, the agent has no prior knowledge, so we enough more exploration, then in later epoches focus more on exploitation.</li>
</ul>
<h4 id="635-results-and-visualizations">6.3.5 Results and Visualizations<a hidden class="anchor" aria-hidden="true" href="#635-results-and-visualizations">#</a></h4>
<p>We run MC sampling for 5000 episodes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>progress_data <span style="color:#f92672">=</span> monte_carlo_training(env, num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">5000</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>)
</span></span></code></pre></div><p><img alt="Monte-Carlo Visualization" loading="lazy" src="/images/CliffWalking-04-09-2025_04_41_PM.png"></p>
<p>We can see the learnt policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is a explainable:</p>
<ul>
<li>The $Q$ value fuction is averaged over episodes, an early cliff fall trace will drag the <strong>average return</strong> for those cliff-adjacent grids.</li>
<li>Also we used $\epsilon$-greedy policy, so even in later episodes when the agent learnt a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids.</li>
</ul>
<h2 id="7-temporal-difference-methods">7. Temporal-Difference Methods<a hidden class="anchor" aria-hidden="true" href="#7-temporal-difference-methods">#</a></h2>
<p>In previous illustration of Monte Carlo methods, it estimate the value function using complete episodes. While this is intuitively simple and unbiased, it&rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.</p>
<p>Temporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.</p>
<p>I found an intuitive way to understand the difference between TD and MC methods are compare this to <strong>Gradient Descent</strong> and <strong>SGD</strong> in neural netwrok optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.</p>
<ul>
<li>Gradient descent computes the gradient using the full dataset, while <em>SGD</em> compute using only data points in current batch, update the parameters, then move to the next batch.</li>
<li>Monte-Carlo methods generates a full episode, backpropogated along the episode to update value function. While TD methods run one step, used its TD difference to update value function, then move to the next step.</li>
</ul>
<p>Then how is TD-difference computed, remember we want to estimate value function using:
$$
V(s_t)\ = r_{t+1} + \gamma\ V(s_{t+1})
$$</p>
<p>So we can bootstrap at $s_t$, execute one more step and compute the value estimates and used it to update the value function:
$$
G(s_t) = r_{t+1} + \gamma\ V(s_{t+1})
\newline
\text{TD Error} = G(s_t) - V(s_t)
\newline
V(s_t) \leftarrow V(s_t) + \alpha \cdot (G(s_t) - V(s_t))
$$</p>
<ul>
<li>$\gamma$ is the discount factor</li>
<li>$\alpha$ is the single step learning rate</li>
</ul>
<h3 id="71-sarsa">7.1. SARSA<a hidden class="anchor" aria-hidden="true" href="#71-sarsa">#</a></h3>
<p>SARSA is one of the most straightforward awy in TD-methods. The idea is intuitive, using next step&rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step&rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function.
$$
G(s_t) = r_{t+1} + \gamma\ Q(s_{t+1}, a_{t+1}) \newline
Q(s_{t}, a_{t}) \leftarrow Q(s_{t}, a_{t}) + \alpha \cdot (G(s_t) - Q(s_t, a_t))
$$
In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $&lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}&gt;$, and this is why this method called <em>SARSA</em>.</p>
<h3 id="711-sarsa-method-in-cliff-walking">7.1.1 SARSA method in Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#711-sarsa-method-in-cliff-walking">#</a></h3>
<ul>
<li>
<p>Step 0, Initilization:</p>
<ul>
<li>Initialize a random value function: $Q(s, a)$</li>
<li>Initialize an $\epsilon$-greedy policy</li>
</ul>
</li>
<li>
<p>Step 1, Bootstrap a step:</p>
<ul>
<li>Agent in state $s_t$ and action $a_t$</li>
<li>Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$</li>
<li>Use the same policy to get the new action $a_{t+1}$</li>
</ul>
</li>
<li>
<p>Step 2, Update value function for the step: $Q(s_t, a_t)$</p>
<ul>
<li>$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \alpha \cdot (r_{t+1} + \gamma\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$</li>
</ul>
</li>
<li>
<p>Finish 1 episode by repeated running step 1-2 until the agent reached <code>goal</code> state.</p>
</li>
<li>
<p>Run above algorithm multiple times until the policy converge.</p>
</li>
</ul>
<h3 id="712-python-implementation-of-sarsa">7.1.2 Python Implementation of SARSA<a hidden class="anchor" aria-hidden="true" href="#712-python-implementation-of-sarsa">#</a></h3>
<h4 id="7121-sarsa">7.1.2.1 SARSA<a hidden class="anchor" aria-hidden="true" href="#7121-sarsa">#</a></h4>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sarsa_one_epoch</span>(env, Q, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Get init action</span>
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>    action <span style="color:#f92672">=</span> epsilon_greedy(Q[state], epsilon)
</span></span><span style="display:flex;"><span>    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
</span></span><span style="display:flex;"><span>        next_state, reward, done <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step_interactive(action)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Sample next action</span>
</span></span><span style="display:flex;"><span>        next_action <span style="color:#f92672">=</span> epsilon_greedy(Q[next_state], epsilon)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># TD-Update current function</span>
</span></span><span style="display:flex;"><span>        Q[state][action] <span style="color:#f92672">+=</span> alpha<span style="color:#f92672">*</span>(reward <span style="color:#f92672">+</span> gamma<span style="color:#f92672">*</span>Q[next_state][next_action] <span style="color:#f92672">-</span> Q[state][action])
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        state, action <span style="color:#f92672">=</span> next_state, next_action
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    policy <span style="color:#f92672">=</span> defaultdict(int)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> Q<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        best_actions <span style="color:#f92672">=</span> [a <span style="color:#66d9ef">for</span> a, i <span style="color:#f92672">in</span> enumerate(v) <span style="color:#66d9ef">if</span> i <span style="color:#f92672">==</span> np<span style="color:#f92672">.</span>max(v)]
</span></span><span style="display:flex;"><span>        policy[k] <span style="color:#f92672">=</span> best_actions
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> Q, policy
</span></span></code></pre></div><ul>
<li>We use the same $\epsilon$-greedy search to get the action</li>
<li>The $Q$ value function is updated within each step of the epoch, this is called <strong>1-step SARSA</strong>, alternatively, we can also update $Q$ value function every fixed number of steps, which is called <strong>$n$-step SARSA</strong></li>
</ul>
<p>To train multiple episodes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">td_sarsa_training</span>(env, num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Key: position, Value: value for of each action</span>
</span></span><span style="display:flex;"><span>    Q <span style="color:#f92672">=</span> defaultdict(<span style="color:#66d9ef">lambda</span>: np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">4</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.01</span>)
</span></span><span style="display:flex;"><span>    progress_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> tqdm<span style="color:#f92672">.</span>tqdm(range(num_episodes)):
</span></span><span style="display:flex;"><span>        epsilon <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">0.01</span>, epsilon<span style="color:#f92672">*</span><span style="color:#ae81ff">0.95</span>)
</span></span><span style="display:flex;"><span>        Q, policy <span style="color:#f92672">=</span> sarsa_one_epoch(env, Q, gamma, epsilon, alpha)
</span></span><span style="display:flex;"><span>        progress_data<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#34;Q&#34;</span>: Q<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;policy&#34;</span>: policy<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;episode&#34;</span>: episode})
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> progress_data
</span></span></code></pre></div><ul>
<li>Similar to that of Monte-Carlo methods, we used a decaying $\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes.</li>
</ul>
<h4 id="7122-visualization-and-result">7.1.2.2 Visualization and Result<a hidden class="anchor" aria-hidden="true" href="#7122-visualization-and-result">#</a></h4>
<p>We train SARSA for 10000 episodes, and visualize the result</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sarsa_progress <span style="color:#f92672">=</span> td_sarsa_training(env, num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">10000</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>)
</span></span></code></pre></div><p><img alt="SARSA Training Visualization" loading="lazy" src="/images/CliffWalking-04-09-2025_09_47_PM.png"></p>
<p>The learnt policy in <code>epoch=9999</code> is similar to that learnt from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:</p>
<ul>
<li>The agent used $\epsilon$-greedy search, so even the agent learnt a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learnt to walk far away from the cliff, taking the constant cost of extra <code>-1</code> reward, to avoid a potential <code>-100</code> reward.</li>
</ul>
<h3 id="72-q-learning">7.2 Q-Learning<a hidden class="anchor" aria-hidden="true" href="#72-q-learning">#</a></h3>
<p>Let&rsquo;s recap the SARSA algorithm again, it used $\epsilon$-greedy search on $Q$ value functions for two purposes:</p>
<ol>
<li><strong>Planning</strong>: Decide the action $a_{t+1}$ of next step</li>
<li><strong>Policy Update</strong>: use the actual action $a_{t+1}$ to update value function.</li>
</ol>
<p>So this policy have to encorporate a trade-off between exploration and exploitation. What if we have two policies:</p>
<ol>
<li>One <strong>Behavior Policy</strong> that focused on exploration, it decides the interaction with the environment.</li>
<li>One <strong>Target Policy</strong> that focused on exploitation, it doesn&rsquo;t do interaction, but focused on learning from previous interactions.</li>
</ol>
<p>Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focused on greedily learning the optimal policy, without being penalized by random exploratary behaviors.</p>
<p>This new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as <strong>On-Policy</strong> vs <strong>Off-Policy</strong>:</p>
<ul>
<li><strong>On-Policy</strong> learns the value of the policy it is actually using to make decisions.</li>
<li><strong>Off-Policy</strong> Learns the value of a different policy than the one it is currently using to make decisions.</li>
</ul>
<p>In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:</p>
<p>$$
G(s_t) = r_{t+1} + \gamma\ Q(s_{t+1}, a_{t+1}) \newline
Q(s_{t}, a_{t}) \leftarrow Q(s_{t}, a_{t}) + \alpha \cdot (G(s_t) - Q(s_t, a_t))
$$</p>
<p>In Q-Learning, we used the theoretical optimal next action instead of actual next action for updates:
$$
G(s_t) = r_{t+1} + \gamma\ \underset{a}{max}\ Q(s_{t+1}) \newline
Q(s_{t}, a_{t}) \leftarrow Q(s_{t}, a_{t}) + \alpha \cdot (G(s_t) - Q(s_t, a_t))
$$</p>
<h3 id="721-q-learning-in-cliff-walking">7.2.1 Q-Learning in Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#721-q-learning-in-cliff-walking">#</a></h3>
<ul>
<li>
<p>Step 0, Initilization:</p>
<ul>
<li>Initialize a random value function: $Q(s, a)$</li>
<li>Initialize an $\epsilon$-greedy policy</li>
</ul>
</li>
<li>
<p>Step 1, Bootstrap a step:</p>
<ul>
<li>Agent in state $s_t$ and action $a_t$</li>
<li>Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$</li>
</ul>
</li>
<li>
<p>Step 2, Update value function for the step: $Q(s_t, a_t)$</p>
<ul>
<li>$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \alpha \cdot (r_{t+1} + \gamma\ max\ Q(s_{t+1}, a) - Q(s_t, a_t))$</li>
</ul>
</li>
<li>
<p>Finish 1 episode by repeated running step 1-2 until the agent reached <code>goal</code> state.</p>
</li>
<li>
<p>Run above algorithm multiple times until the policy converge.</p>
</li>
</ul>
<p>This looks very similar to SARSA, the only difference is:</p>
<ul>
<li>We no longer need to sample $s_{t+1}$ in each step.</li>
<li>We used $max\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update.</li>
</ul>
<h3 id="722-python-implementation-of-q-learning">7.2.2 Python Implementation of Q-Learning<a hidden class="anchor" aria-hidden="true" href="#722-python-implementation-of-q-learning">#</a></h3>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">q_learning_one_epoch</span>(env, Q, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Get init action</span>
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> theta_greedy_action(Q, state, epsilon)
</span></span><span style="display:flex;"><span>        next_state, reward, done <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step_interactive(action)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># TD-Update current function</span>
</span></span><span style="display:flex;"><span>        Q[state][action] <span style="color:#f92672">+=</span> alpha<span style="color:#f92672">*</span>(reward <span style="color:#f92672">+</span> gamma<span style="color:#f92672">*</span>max(Q[next_state]) <span style="color:#f92672">-</span> Q[state][action])
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> next_state
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    policy <span style="color:#f92672">=</span> defaultdict(int)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> Q<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        best_actions <span style="color:#f92672">=</span> [a <span style="color:#66d9ef">for</span> a, i <span style="color:#f92672">in</span> enumerate(v) <span style="color:#66d9ef">if</span> i <span style="color:#f92672">==</span> np<span style="color:#f92672">.</span>max(v)]
</span></span><span style="display:flex;"><span>        policy[k] <span style="color:#f92672">=</span> best_actions
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> Q, policy
</span></span></code></pre></div><ul>
<li>We no longer computed <code>next_action</code> in each step.</li>
<li><code>Q</code> is updated using <code>max(Q[next_state])</code>.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">q_learning_training</span>(env, num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Key: position, Value: value for of each action</span>
</span></span><span style="display:flex;"><span>    Q <span style="color:#f92672">=</span> defaultdict(<span style="color:#66d9ef">lambda</span>: np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">4</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.01</span>)
</span></span><span style="display:flex;"><span>    progress_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> tqdm<span style="color:#f92672">.</span>tqdm(range(num_episodes)):
</span></span><span style="display:flex;"><span>        epsilon <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">0.01</span>, epsilon<span style="color:#f92672">*</span><span style="color:#ae81ff">0.95</span>)
</span></span><span style="display:flex;"><span>        Q, policy <span style="color:#f92672">=</span> q_learning_one_epoch(env, Q, gamma, epsilon, alpha)
</span></span><span style="display:flex;"><span>        progress_data<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#34;Q&#34;</span>: Q<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;policy&#34;</span>: policy<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;episode&#34;</span>: episode})
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> progress_data
</span></span></code></pre></div><h4 id="723-visualization-and-result">7.2.3 Visualization and Result<a hidden class="anchor" aria-hidden="true" href="#723-visualization-and-result">#</a></h4>
<p>Q-Learning converges faster than SARSA, we only trained 200 episodes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>q_learning_progress <span style="color:#f92672">=</span> q_learning_training(env, num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>)
</span></span></code></pre></div><p><img alt="Q-Learning Training Visualization" loading="lazy" src="/images/CliffWalking-04-09-2025_11_18_PM.png"></p>
<p>While SARSA found a <strong>safe path</strong> under randomness of $\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!</p>
<h2 id="8-summary">8. Summary<a hidden class="anchor" aria-hidden="true" href="#8-summary">#</a></h2>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/reinforcement-learning/">Reinforcement Learning</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="http://localhost:1313/posts/kv-cache/">
    <span class="title">Next »</span>
    <br>
    <span>KV Cache Explained</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Reinforcement Learning on x"
            href="https://x.com/intent/tweet/?text=Introduction%20to%20Reinforcement%20Learning&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f&amp;hashtags=ReinforcementLearning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Reinforcement Learning on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f&amp;title=Introduction%20to%20Reinforcement%20Learning&amp;summary=Introduction%20to%20Reinforcement%20Learning&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Reinforcement Learning on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f&title=Introduction%20to%20Reinforcement%20Learning">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Reinforcement Learning on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Reinforcement Learning on whatsapp"
            href="https://api.whatsapp.com/send?text=Introduction%20to%20Reinforcement%20Learning%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Reinforcement Learning on telegram"
            href="https://telegram.me/share/url?text=Introduction%20to%20Reinforcement%20Learning&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share Introduction to Reinforcement Learning on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=Introduction%20to%20Reinforcement%20Learning&u=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Hxx&#39;Log</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
