<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>An Introduction to Reinforcement Learning using Cliff Walking Problem | Hxx&#39;Log</title>
<meta name="keywords" content="Reinforcement Learning, Monte Carlo, Dynamic Programming, Temporal Difference">
<meta name="description" content="1. Getting Started with Reinforcement Learning
I&rsquo;ve been fascinated by Reinforcement Learning (RL) for some time, particularly after seeing its recent successes in refining Large Language Models (LLMs) through post-training. However, diving into RL can feel like entering a different world compared to supervised learning. The core concepts, mathematical notation, and terminology—terms like on-policy, off-policy, reward, value function, model-free, and agent—often seem unfamiliar and initially confusing for newcomers like me, who are more accustomed to the standard machine learning vocabulary of model, data, and loss function.">
<meta name="author" content="hxx">
<link rel="canonical" href="http://localhost:1313/posts/rl-intro/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.45e028aa8ce0961349adf411b013ee39406be2c0bc80d4ea3fc04555f7f4611a.css" integrity="sha256-ReAoqozglhNJrfQRsBPuOUBr4sC8gNTqP8BFVff0YRo=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/rl-intro/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js" onload="
  renderMathInElement(document.body, {
    delimiters: [
      {left: '$$', right: '$$', display: true},
      {left: '$', right: '$', display: false},
      {left: '\\(', right: '\\)', display: false},
      {left: '\\[', right: '\\]', display: true}
    ],
    throwOnError: false
  });
"></script>


</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Hxx&#39;Log (Alt + H)">Hxx&#39;Log</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/posts/" title="Posts">
                    <span>Posts</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      An Introduction to Reinforcement Learning using Cliff Walking Problem
      <span class="entry-hint" title="Draft">
        <svg xmlns="http://www.w3.org/2000/svg" height="35" viewBox="0 -960 960 960" fill="currentColor">
          <path
            d="M160-410v-60h300v60H160Zm0-165v-60h470v60H160Zm0-165v-60h470v60H160Zm360 580v-123l221-220q9-9 20-13t22-4q12 0 23 4.5t20 13.5l37 37q9 9 13 20t4 22q0 11-4.5 22.5T862.09-380L643-160H520Zm300-263-37-37 37 37ZM580-220h38l121-122-18-19-19-18-122 121v38Zm141-141-19-18 37 37-18-19Z" />
        </svg>
      </span>
    </h1>
    <div class="post-meta"><span title='2025-04-10 00:00:00 +0000 UTC'>April 10, 2025</span>&nbsp;·&nbsp;31 min&nbsp;·&nbsp;hxx

</div>
  </header> <div class="toc">
    <details  open>
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-getting-started-with-reinforcement-learning" aria-label="1. Getting Started with Reinforcement Learning">1. Getting Started with Reinforcement Learning</a></li>
                <li>
                    <a href="#2-what-is-reinforcement-learning" aria-label="2. What is Reinforcement Learning?">2. What is Reinforcement Learning?</a><ul>
                        
                <li>
                    <a href="#21-core-components-of-rl" aria-label="2.1 Core Components of RL">2.1 Core Components of RL</a><ul>
                        
                <li>
                    <a href="#211-policy-pi" aria-label="2.1.1 Policy ($\pi$)">2.1.1 Policy ($\pi$)</a></li>
                <li>
                    <a href="#212-value-function" aria-label="2.1.2 Value Function">2.1.2 Value Function</a></li>
                <li>
                    <a href="#213-model" aria-label="2.1.3 Model">2.1.3 Model</a></li></ul>
                </li>
                <li>
                    <a href="#22-optimization-of-rl" aria-label="2.2 Optimization of RL">2.2 Optimization of RL</a><ul>
                        
                <li>
                    <a href="#221-value-based-agent" aria-label="2.2.1 Value-based Agent">2.2.1 Value-based Agent</a></li>
                <li>
                    <a href="#222-policy-based-agent" aria-label="2.2.2 Policy-based Agent">2.2.2 Policy-based Agent</a></li>
                <li>
                    <a href="#223-actor-critic" aria-label="2.2.3 Actor-Critic">2.2.3 Actor-Critic</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#3-example-problem-cliff-walking-problem" aria-label="3. Example Problem, Cliff Walking Problem">3. Example Problem, Cliff Walking Problem</a><ul>
                        
                <li>
                    <a href="#31-problem-statement" aria-label="3.1 Problem Statement:">3.1 Problem Statement:</a></li>
                <li>
                    <a href="#32-python-implementation" aria-label="3.2 Python Implementation">3.2 Python Implementation</a><ul>
                        
                <li>
                    <a href="#321-cliff-walking-environment" aria-label="3.2.1 Cliff Walking Environment">3.2.1 Cliff Walking Environment</a></li>
                <li>
                    <a href="#322-define-the-step-function" aria-label="3.2.2 Define the Step Function">3.2.2 Define the Step Function</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#4-dynamic-programming-methods" aria-label="4. Dynamic Programming Methods">4. Dynamic Programming Methods</a><ul>
                        
                <li>
                    <a href="#41-introduction" aria-label="4.1 Introduction">4.1 Introduction</a></li>
                <li>
                    <a href="#411-markov-property" aria-label="4.1.1 Markov Property">4.1.1 Markov Property</a></li>
                <li>
                    <a href="#412-bellman-optimal-function" aria-label="4.1.2 Bellman Optimal Function">4.1.2 Bellman Optimal Function</a></li>
                <li>
                    <a href="#42-value-iteration" aria-label="4.2 Value Iteration">4.2 Value Iteration</a><ul>
                        
                <li>
                    <a href="#421-value-iteration-for-cliff-walking" aria-label="4.2.1 Value Iteration for Cliff Walking">4.2.1 Value Iteration for Cliff Walking</a></li>
                <li>
                    <a href="#422-python-implementation-of-value-iteration-for-cliff-walking" aria-label="4.2.2 Python Implementation of Value Iteration for Cliff Walking">4.2.2 Python Implementation of Value Iteration for Cliff Walking</a><ul>
                        
                <li>
                    <a href="#4221-one-epoch-update" aria-label="4.2.2.1 One Epoch Update">4.2.2.1 One Epoch Update</a></li>
                <li>
                    <a href="#4222-training" aria-label="4.2.2.2 Training">4.2.2.2 Training</a></li>
                <li>
                    <a href="#4223-result-and-visualization" aria-label="4.2.2.3 Result and Visualization">4.2.2.3 Result and Visualization</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#43-policy-iteration" aria-label="4.3 Policy Iteration">4.3 Policy Iteration</a><ul>
                        
                <li>
                    <a href="#431-policy-iteration-for-cliff-walking" aria-label="4.3.1 Policy Iteration for Cliff Walking">4.3.1 Policy Iteration for Cliff Walking</a></li>
                <li>
                    <a href="#432-python-implementation-of-policy-iteration-for-cliff-walking" aria-label="4.3.2 Python Implementation of Policy Iteration for Cliff Walking">4.3.2 Python Implementation of Policy Iteration for Cliff Walking</a><ul>
                        
                <li>
                    <a href="#4321-policy-evaluation" aria-label="4.3.2.1 Policy Evaluation">4.3.2.1 Policy Evaluation</a></li>
                <li>
                    <a href="#4322-policy-improvement" aria-label="4.3.2.2 Policy Improvement">4.3.2.2 Policy Improvement</a></li>
                <li>
                    <a href="#4323-training" aria-label="4.3.2.3 Training">4.3.2.3 Training</a></li>
                <li>
                    <a href="#4324-results-and-evaluation" aria-label="4.3.2.4 Results and Evaluation">4.3.2.4 Results and Evaluation</a></li></ul>
                </li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#5-monte-carlo-methods" aria-label="5. Monte-Carlo Methods">5. Monte-Carlo Methods</a><ul>
                        
                <li>
                    <a href="#51-introduction" aria-label="5.1 Introduction">5.1 Introduction</a></li>
                <li>
                    <a href="#52-interaction-environment" aria-label="5.2 Interaction Environment">5.2 Interaction Environment</a></li>
                <li>
                    <a href="#53-monte-carlo-simulation" aria-label="5.3 Monte-Carlo Simulation">5.3 Monte-Carlo Simulation</a><ul>
                        
                <li>
                    <a href="#531-epsilon-search-algorithm" aria-label="5.3.1 $\epsilon$-search algorithm">5.3.1 $\epsilon$-search algorithm</a></li>
                <li>
                    <a href="#532-monte-carlo-method-in-cliff-walking" aria-label="5.3.2 Monte-Carlo method in Cliff Walking">5.3.2 Monte-Carlo method in Cliff Walking</a></li>
                <li>
                    <a href="#533-python-implementation-of-mc-in-cliff-walking" aria-label="5.3.3 Python Implementation of MC in Cliff Walking">5.3.3 Python Implementation of MC in Cliff Walking</a><ul>
                        
                <li>
                    <a href="#5331-epsilon-greedy-search" aria-label="5.3.3.1 $\epsilon$-greedy search">5.3.3.1 $\epsilon$-greedy search</a></li>
                <li>
                    <a href="#5332-mc-simulation" aria-label="5.3.3.2 MC-Simulation">5.3.3.2 MC-Simulation</a></li>
                <li>
                    <a href="#5333-value-function-update" aria-label="5.3.3.3 Value Function Update">5.3.3.3 Value Function Update</a></li>
                <li>
                    <a href="#5334-monte-carlo-training" aria-label="5.3.3.4 Monte-Carlo training">5.3.3.4 Monte-Carlo training</a></li>
                <li>
                    <a href="#5335-results-and-visualizations" aria-label="5.3.3.5 Results and Visualizations">5.3.3.5 Results and Visualizations</a></li></ul>
                </li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#6-temporal-difference-methods" aria-label="6. Temporal-Difference Methods">6. Temporal-Difference Methods</a><ul>
                        
                <li>
                    <a href="#61-introduction" aria-label="6.1 Introduction">6.1 Introduction</a></li>
                <li>
                    <a href="#62-sarsa" aria-label="6.2 SARSA">6.2 SARSA</a><ul>
                        
                <li>
                    <a href="#621-sarsa-method-in-cliff-walking" aria-label="6.2.1 SARSA method in Cliff Walking">6.2.1 SARSA method in Cliff Walking</a></li>
                <li>
                    <a href="#622-python-implementation-of-sarsa" aria-label="6.2.2 Python Implementation of SARSA">6.2.2 Python Implementation of SARSA</a><ul>
                        
                <li>
                    <a href="#6121-sarsa" aria-label="6.1.2.1 SARSA">6.1.2.1 SARSA</a></li>
                <li>
                    <a href="#6122-visualization-and-result" aria-label="6.1.2.2 Visualization and Result">6.1.2.2 Visualization and Result</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#63-q-learning" aria-label="6.3 Q-Learning">6.3 Q-Learning</a><ul>
                        
                <li>
                    <a href="#631-q-learning-in-cliff-walking" aria-label="6.3.1 Q-Learning in Cliff Walking">6.3.1 Q-Learning in Cliff Walking</a></li>
                <li>
                    <a href="#632-python-implementation-of-q-learning" aria-label="6.3.2 Python Implementation of Q-Learning">6.3.2 Python Implementation of Q-Learning</a><ul>
                        
                <li>
                    <a href="#6321-q-learning" aria-label="6.3.2.1 Q-Learning">6.3.2.1 Q-Learning</a></li>
                <li>
                    <a href="#6322-visualization-and-result" aria-label="6.3.2.2 Visualization and Result">6.3.2.2 Visualization and Result</a></li></ul>
                </li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#7-summary" aria-label="7. Summary">7. Summary</a><ul>
                        
                <li>
                    <a href="#71-whats-more" aria-label="7.1. What&rsquo;s More">7.1. What&rsquo;s More</a></li></ul>
                </li>
                <li>
                    <a href="#8-references" aria-label="8. References">8. References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="1-getting-started-with-reinforcement-learning">1. Getting Started with Reinforcement Learning<a hidden class="anchor" aria-hidden="true" href="#1-getting-started-with-reinforcement-learning">#</a></h2>
<p>I&rsquo;ve been fascinated by Reinforcement Learning (RL) for some time, particularly after seeing its recent successes in refining Large Language Models (LLMs) through post-training. However, diving into RL can feel like entering a different world compared to supervised learning. The core concepts, mathematical notation, and terminology—terms like <code>on-policy</code>, <code>off-policy</code>, <code>reward</code>, <code>value function</code>, <code>model-free</code>, and <code>agent</code>—often seem unfamiliar and initially confusing for newcomers like me, who are more accustomed to the standard machine learning vocabulary of <code>model</code>, <code>data</code>, and <code>loss function</code>.</p>
<p>Recently, I dedicated time to exploring RL more deeply. I was pleasantly surprised to find Richard Sutton and Andrew Barto&rsquo;s book, <em>Reinforcement Learning: An Introduction</em>, remarkably accessible, even for beginners. This post aims to summarize and share some fundamental concepts I&rsquo;ve learned.</p>
<p>In this article, we&rsquo;ll cover the basic elements of RL and explore the <code>Cliff Walking</code> problem—a classic RL task—using three foundational approaches:</p>
<ol>
<li><strong>Dynamic Programming (DP)</strong></li>
<li><strong>Monte Carlo (MC) Methods</strong></li>
<li><strong>Temporal-Difference (TD) Learning</strong></li>
</ol>
<p>This discussion is primarily based on Chapters 1 through 6 of Sutton &amp; Barto&rsquo;s book.</p>
<h2 id="2-what-is-reinforcement-learning">2. What is Reinforcement Learning?<a hidden class="anchor" aria-hidden="true" href="#2-what-is-reinforcement-learning">#</a></h2>
<p>With established paradigms like supervised and unsupervised learning, why do we need RL? How does it stand apart?</p>
<p><strong>Machine Learning</strong>, as defined by <a href="https://en.wikipedia.org/wiki/Machine_learning">Wikipedia</a>, centers on algorithms that learn patterns from data ($X$) and generalize to new, unseen data. The goal is often to understand the underlying distribution of $X$ or predict outcomes based on it.</p>
<ul>
<li>
<p><strong>Supervised Learning</strong>: Learns from labeled data, typically pairs of input features and corresponding outputs $&lt;x_i, y_i&gt;$. The objective is to learn a mapping function or the conditional probability distribution $p(y | x)$. Examples include image classification (predicting a label $y$ for an image $x$) and sequence prediction tasks like those performed by GPT models (predicting the next token based on previous tokens).</p>
</li>
<li>
<p><strong>Unsupervised Learning</strong>: Works with unlabeled data, consisting only of input features $x_i$. The goal is to uncover hidden structures, patterns, or distributions within the data itself, without predefined categories or outcomes. Clustering and dimensionality reduction are common examples.</p>
</li>
<li>
<p><strong>Reinforcement Learning</strong>: Differs significantly because it deals with learning optimal behaviors through interaction within a dynamic environment over time. Unlike supervised learning with static $&lt;x_i, y_i&gt;$ pairs or unsupervised learning focusing solely on $x_i$, RL involves an <strong>agent</strong> learning a <strong>policy</strong> $\pi(a | s)$. This policy maps situations (<strong>states</strong>, $s$) to <strong>actions</strong> ($a$). The agent learns by receiving feedback (<strong>rewards</strong>) from the <strong>environment</strong> based on its actions, influencing future states and subsequent rewards. The temporal dimension and the agent&rsquo;s active role in shaping its experience are key distinctions.</p>
</li>
</ul>
<p>Consider a self-driving car navigating traffic. The car (agent) must constantly make decisions (accelerate, brake, turn - actions) based on its current situation (traffic, road conditions - state). Supervised learning struggles here because creating labeled data for every conceivable driving scenario is impractical. RL provides a framework for the car to learn effective driving strategies by directly interacting with its environment (simulated or real) and learning from the consequences (rewards or penalties) of its actions.</p>
<h3 id="21-core-components-of-rl">2.1 Core Components of RL<a hidden class="anchor" aria-hidden="true" href="#21-core-components-of-rl">#</a></h3>
<p>RL problems model the interaction between two main entities:</p>
<p><img alt="Agent Environment Interaction" loading="lazy" src="/images/ae1.png"></p>
<ul>
<li><strong>Agent</strong>: The learner and decision-maker. It perceives the environment&rsquo;s state and chooses actions.</li>
<li><strong>Environment</strong>: Everything external to the agent with which it interacts. It responds to the agent&rsquo;s actions by changing its state and providing rewards.</li>
</ul>
<p>The interaction unfolds through a sequence of discrete time steps:</p>
<ol>
<li>At time $t$, the agent observes the environment&rsquo;s state $s_t$.</li>
<li>Based on $s_t$, the agent selects an action $a_t$ according to its policy.</li>
<li>The environment transitions to a new state $s_{t+1}$ as a result of action $a_t$.</li>
<li>The environment provides a reward $r_{t+1}$ to the agent, indicating the immediate consequence of the transition.</li>
</ol>
<p>This cycle repeats. The <strong>goal</strong> of the RL agent is typically to learn a policy that maximizes the <strong>cumulative reward</strong> over the long run.</p>
<p>An RL agent&rsquo;s behavior and learning process are often characterized by three key components:</p>
<ul>
<li><strong>Policy ($\pi$)</strong>: The agent&rsquo;s strategy or behavior function. It dictates how the agent chooses actions based on the observed state. $a_t \sim \pi(\cdot|s_t)$.</li>
<li><strong>Value Function ($V$ or $Q$)</strong>: A prediction of future rewards. It estimates how good it is for the agent to be in a particular state ($V(s)$) or to take a specific action in a state ($Q(s, a)$), assuming the agent follows a certain policy thereafter. Value functions are crucial for evaluating policies and guiding action selection towards maximizing cumulative reward.</li>
<li><strong>Model (Optional)</strong>: The agent&rsquo;s internal representation of how the environment works. A model predicts what the next state and reward will be given a current state and action. $P(s_{t+1}, r_{t+1} | s_t, a_t)$. Agents that use a model are called <strong>model-based</strong>, while those that learn directly from experience without building an explicit model are <strong>model-free</strong>.</li>
</ul>
<h4 id="211-policy-pi">2.1.1 Policy ($\pi$)<a hidden class="anchor" aria-hidden="true" href="#211-policy-pi">#</a></h4>
<p>The policy defines the agent&rsquo;s way of behaving at a given time. Mathematically, it&rsquo;s a mapping from perceived states to actions to be taken when in those states.</p>
<ul>
<li>
<p><strong>Stochastic Policy</strong>: Outputs a probability distribution over actions. The agent samples an action from this distribution.
$\pi(a | s) = P(A_t = a | S_t = s)$
This is common during learning to encourage exploration.</p>
</li>
<li>
<p><strong>Deterministic Policy</strong>: Directly outputs a specific action for each state.
$a_t = \pi(s_t)$
Often, the goal is to learn an optimal deterministic policy.</p>
</li>
</ul>
<h4 id="212-value-function">2.1.2 Value Function<a hidden class="anchor" aria-hidden="true" href="#212-value-function">#</a></h4>
<p>In an RL system, the end goal is to find a strategy to win games like Chess or Go. At any time $t$, we want a metric to evaluate the effectiveness of the current policy.</p>
<p>The reward $r_t$ only refers to the immediate reward at time $t$, not the overall winning chance. Thus, we define $V_t$ as the overall reward at time $t$, including the current immediate reward $R_t$ and its expected future reward $V_{t+1}$.</p>
<p>$V_t = r_t + \gamma V_{t+1}$</p>
<p>The gamma ($\gamma$) discount factor is a crucial component in reinforcement learning. It determines the importance of future rewards compared to immediate rewards. A value of $\gamma$ close to 0 makes the agent short-sighted by prioritizing immediate rewards, while a value close to 1 encourages the agent to consider long-term rewards.</p>
<p>The reward at time $t$ is determined by its current state $s_t$, the action taken $a_t$, and its next state $s_{t+1}$.</p>
<p>$r_t = R(s_t, a_t, s_{t+1})$</p>
<p>Assuming we take $T$ steps from time $0$ to $T-1$, these steps form a trajectory $\tau$, and the sum reward is represented as the reward over the trajectory.</p>
<p>$$
\tau = (s_0, a_0, r_0, s_1, a_1, r_1, &hellip;, s_{T-1}, a_{T-1}, r_{T-1}) \newline</p>
<p>R(\tau) = \sum_{t=0}^{T-1}r_t <br>
$$</p>
<h4 id="213-model">2.1.3 Model<a hidden class="anchor" aria-hidden="true" href="#213-model">#</a></h4>
<p>A model defines how the environment interacts with the agent, comprising $&lt;S, A, P, R&gt;$:</p>
<ul>
<li>$S$: The space of all possible states.</li>
<li>$A$: The space of all possible actions.</li>
<li>$P$: The transformation function of how states change, $P(s_{t+1}|s_t, a_t)$.</li>
<li>$R$: How rewards are calculated for each state, $R(s_t, a_t)$.</li>
</ul>
<p>If these four elements are known, we can model the interaction without actual interaction, known as <strong>model-based learning</strong>.</p>
<p>In reality, while $S$ and $A$ are often known, the transformation and reward parts are either fully unknown or hard to estimate, so agents need to interact with the real environment to observe states and rewards, known as <strong>model-free learning</strong>.</p>
<p>Comparing the two, <strong>model-free learning</strong> relies on real interaction to get the next state and reward, while <strong>model-based learning</strong> models these without specific interaction.</p>
<h3 id="22-optimization-of-rl">2.2 Optimization of RL<a hidden class="anchor" aria-hidden="true" href="#22-optimization-of-rl">#</a></h3>
<p>The optimization of an RL task can be divided into two parts:</p>
<ul>
<li><strong>Value Estimation</strong>: Given a strategy $\pi$, evaluate its effectiveness, computing $V_\pi$.</li>
<li><strong>Policy Optimization</strong>: Given the value function $V_\pi$, optimize to get a better policy $\pi$.</li>
</ul>
<p>This is similar to k-Means clustering, where we have two optimization targets and optimize them iteratively to get the optimal answer. In k-means:</p>
<ul>
<li>Given the current cluster assignment of each point, compute the optimal centroid. Similar to <em>value estimation</em>.</li>
<li>Given the current optimal centroid, find a better cluster assignment for each point. Similar to <em>policy optimization</em>.</li>
</ul>
<p>Are both steps necessary in RL optimization? The answer is no.</p>
<h4 id="221-value-based-agent">2.2.1 Value-based Agent<a hidden class="anchor" aria-hidden="true" href="#221-value-based-agent">#</a></h4>
<p>An agent can learn only the value function $V_\pi$, maintaining a table mapping $&lt;S, A&gt;$ to $V$. In each step, it picks the action that will maximize the ultimate value. In this type of work, the agent doesn&rsquo;t explicitly have a strategy or policy.</p>
<p>$a_t = \text{argmax}\ V(a | s_t)$</p>
<h4 id="222-policy-based-agent">2.2.2 Policy-based Agent<a hidden class="anchor" aria-hidden="true" href="#222-policy-based-agent">#</a></h4>
<p>A policy agent directly learns the policy, and each step it outputs the distribution of the next action without knowing the value function.</p>
<p>$a_t \sim P(a|s_t)$</p>
<h4 id="223-actor-critic">2.2.3 Actor-Critic<a hidden class="anchor" aria-hidden="true" href="#223-actor-critic">#</a></h4>
<p>An agent can learn both $\pi$ and $V_{\pi}$ as described above.</p>
<ul>
<li><strong>Actor</strong>: Learning of policy $\pi$.</li>
<li><strong>Critic</strong>: Learning of value function $V_\pi$.</li>
</ul>
<h2 id="3-example-problem-cliff-walking-problem">3. Example Problem, Cliff Walking Problem<a hidden class="anchor" aria-hidden="true" href="#3-example-problem-cliff-walking-problem">#</a></h2>
<p>Let&rsquo;s explore a problem to illustrate the various components and optimization methods in RL. The Cliff Walking problem is a classic reinforcement learning environment introduced in Sutton &amp; Barto’s book, <em>Reinforcement Learning: An Introduction</em>.</p>
<h3 id="31-problem-statement">3.1 Problem Statement:<a hidden class="anchor" aria-hidden="true" href="#31-problem-statement">#</a></h3>
<p><img alt="Cliff Walking Figure" loading="lazy" src="/images/cliff-walking.png"></p>
<ul>
<li>The world is represented as a 4×12 grid.</li>
<li>The start state is at the bottom-left corner $(3, 0)$, and the goal state is at the bottom-right corner $(3, 11)$.</li>
<li>The bottom row between the start and goal is referred to as the cliff $(3, 1-10)$. If the agent steps into any of these cliff cells, it falls off, receives a large negative reward (e.g., -100), and is reset to the start.</li>
<li>Each non-terminal move incurs a reward of -1.</li>
</ul>
<p>Mapping this problem to RL components:</p>
<ul>
<li><strong>Agent</strong>: The robot navigating the grid, aiming to find a path from the start to the goal while maximizing rewards.</li>
<li><strong>Environment</strong>: The 4x12 grid world, including transition dynamics and rewards for each move. This environment is fully observable and deterministic.</li>
<li><strong>State</strong>: The state space comprises all possible locations of the agent on the grid. There are 48 grid cells, excluding the 10 cliff cells, resulting in 38 possible states.</li>
<li><strong>Action</strong>: The action space consists of four possible moves: <code>UP</code>, <code>DOWN</code>, <code>LEFT</code>, <code>RIGHT</code>.</li>
<li><strong>Reward</strong>: A scalar signal from the environment for each move:
<ul>
<li>-1 for each normal move.</li>
<li>-100 if the agent falls into the cliff.</li>
<li>0 upon reaching the goal.</li>
</ul>
</li>
<li><strong>Policy</strong>: The strategy the agent should adopt to reach the goal state. It maps states to actions, indicating the direction the agent should take in each grid cell, e.g., <code>(3, 0) -&gt; MOVE UP</code>. The objective is to discover such a policy.</li>
</ul>
<h3 id="32-python-implementation">3.2 Python Implementation<a hidden class="anchor" aria-hidden="true" href="#32-python-implementation">#</a></h3>
<h4 id="321-cliff-walking-environment">3.2.1 Cliff Walking Environment<a hidden class="anchor" aria-hidden="true" href="#321-cliff-walking-environment">#</a></h4>
<p>The environment can be defined as follows:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CliffWalk</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, height, width, start, end, cliff):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>height <span style="color:#f92672">=</span> height
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>width <span style="color:#f92672">=</span> width
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>start <span style="color:#f92672">=</span> start
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>end <span style="color:#f92672">=</span> end
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>cliff <span style="color:#f92672">=</span> set(cliff)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actions <span style="color:#f92672">=</span> [(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>), (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)]  <span style="color:#75715e"># up, down, left, right</span>
</span></span></code></pre></div><p>Initialize the environment with specified start, end, and cliff cells:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>start <span style="color:#f92672">=</span> (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">0</span>)
</span></span><span style="display:flex;"><span>end <span style="color:#f92672">=</span> (<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">11</span>)
</span></span><span style="display:flex;"><span>height <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span>
</span></span><span style="display:flex;"><span>width <span style="color:#f92672">=</span> <span style="color:#ae81ff">12</span>
</span></span><span style="display:flex;"><span>cliff_cells <span style="color:#f92672">=</span> [(<span style="color:#ae81ff">3</span>, i) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">11</span>)]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>env <span style="color:#f92672">=</span> CliffWalk(height, width, start, end, cliff_cells)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Check GitHub for visualization code.</span>
</span></span><span style="display:flex;"><span>env<span style="color:#f92672">.</span>render_plot()
</span></span></code></pre></div><p><img alt="Cliff Walking Visualization" loading="lazy" src="/images/CliffWalking-04-08-2025_10_12_AM.png"></p>
<h4 id="322-define-the-step-function">3.2.2 Define the Step Function<a hidden class="anchor" aria-hidden="true" href="#322-define-the-step-function">#</a></h4>
<p>Next, define the rules and rewards for the agent&rsquo;s actions:</p>
<ul>
<li><code>self.actions</code> defines four types of actions, each representing a step in a direction.</li>
<li>The <code>step</code> function takes the current location <code>(i, j)</code> and action index, executes it, and returns a tuple representing:
<ul>
<li>The agent&rsquo;s new position after the action.</li>
<li>The reward for the current action.</li>
</ul>
</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">step</span>(self, i: int, j: int, a: int) <span style="color:#f92672">-&gt;</span> tuple[tuple[int, int], int]:
</span></span><span style="display:flex;"><span>        ni, nj <span style="color:#f92672">=</span> i <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>actions[a][<span style="color:#ae81ff">0</span>], j <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>actions[a][<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Fell into Cliff, get -100 reward, back to start point</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (ni, nj) <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>cliff:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>start, <span style="color:#f92672">-</span><span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (ni, nj) <span style="color:#f92672">==</span> self<span style="color:#f92672">.</span>end:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> (ni, nj), <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Move, get -1 reward</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">&lt;=</span> ni <span style="color:#f92672">&lt;</span> self<span style="color:#f92672">.</span>height <span style="color:#f92672">and</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">&lt;=</span> nj <span style="color:#f92672">&lt;</span> self<span style="color:#f92672">.</span>width:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> (ni, nj), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Move out of grid, get -1 reward</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> (i, j), <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
</span></span></code></pre></div><p>The movement rules and rewards are:</p>
<ul>
<li>The agent receives -1 for each normal move.</li>
<li>The agent receives -100 if it falls off the cliff.</li>
<li>The agent receives 0 upon reaching the goal.</li>
<li>If the agent moves out of the grid, it remains in the same cell and still receives -1.</li>
</ul>
<p>With the environment set up, we can now solve the problem using three RL methods: dynamic programming, Monte Carlo, and temporal-difference methods. A comparison of these methods will be provided at the end of the article.</p>
<h2 id="4-dynamic-programming-methods">4. Dynamic Programming Methods<a hidden class="anchor" aria-hidden="true" href="#4-dynamic-programming-methods">#</a></h2>
<h3 id="41-introduction">4.1 Introduction<a hidden class="anchor" aria-hidden="true" href="#41-introduction">#</a></h3>
<p>Dynamic programming (DP) is a method used in reinforcement learning to solve Markov decision processes (MDPs) by breaking them down into simpler subproblems. It works by iteratively improving the value function, which estimates the expected return of states, and deriving an optimal policy from these values. DP requires a complete model of the environment, including the transition probabilities and reward functions.</p>
<h3 id="411-markov-property">4.1.1 Markov Property<a hidden class="anchor" aria-hidden="true" href="#411-markov-property">#</a></h3>
<p>A Markov decision process (MDP) is defined by five elements:
$$
MDP = &lt;S, A, P, R, \gamma&gt;
$$</p>
<ul>
<li>$S$: state space</li>
<li>$A$: action space</li>
<li>$P$: <strong>transition probability</strong> from a state and action to its next state, $p(s_{t+1}|s_t, a_t)$</li>
<li>$R$: <strong>reward function</strong> immediate reward after a transition, $r(s_{t+1}, s_t, a_t)$</li>
<li>$\gamma$: <strong>discount factor</strong> that weights the importance of future rewards</li>
</ul>
<p>A decision process has the <strong>Markov Property</strong> if its next state and reward depend only on its current state and action, not the full history. The cliff walking problem satisfies the <strong>Markovian property</strong>.</p>
<h3 id="412-bellman-optimal-function">4.1.2 Bellman Optimal Function<a hidden class="anchor" aria-hidden="true" href="#412-bellman-optimal-function">#</a></h3>
<p>The Bellman optimality equation defines the optimal value function as:
$$
V^*(s) = \max V_\pi(s)
$$</p>
<p>Here, we search for a policy $\pi$ that maximizes the value of state $V$. The resulting policy is our optimal policy.
$$
\pi^*(s) = argmax V_\pi(s)
$$</p>
<p>For each state $s$, we search over its possible actions to maximize the value:
$$
\pi^*(a | s) = 1, if\ a = argmax\ Q(s, a)
$$</p>
<p>Dynamic programming utilizes these equations to iteratively update the value function and improve the policy until convergence, ensuring that the policy becomes optimal.</p>
<h3 id="42-value-iteration">4.2 Value Iteration<a hidden class="anchor" aria-hidden="true" href="#42-value-iteration">#</a></h3>
<p>Value iteration is a dynamic programming method used to compute the optimal policy and value function for a Markov decision process (MDP). It iteratively updates the value function by considering the expected returns of all possible actions at each state and selecting the action that maximizes this return.</p>
<h4 id="421-value-iteration-for-cliff-walking">4.2.1 Value Iteration for Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#421-value-iteration-for-cliff-walking">#</a></h4>
<ul>
<li>Initialize each state&rsquo;s value function to 0: $V(s) = 0$</li>
<li>For each state $s_t$:
<ul>
<li>Search over its possible actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$</li>
<li>Update the current state&rsquo;s value function with the action that bears the largest reward: $V(s_t) = \underset{a}{\max}(r_a + V(s_{t+1}))$</li>
</ul>
</li>
<li>Repeat the previous steps until convergence</li>
</ul>
<h4 id="422-python-implementation-of-value-iteration-for-cliff-walking">4.2.2 Python Implementation of Value Iteration for Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#422-python-implementation-of-value-iteration-for-cliff-walking">#</a></h4>
<h5 id="4221-one-epoch-update">4.2.2.1 One Epoch Update<a hidden class="anchor" aria-hidden="true" href="#4221-one-epoch-update">#</a></h5>
<p>Below is the implementation for one iteration of value update:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">value_iterate</span>(env, V, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Run one epoch of value iteration&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    V_new <span style="color:#f92672">=</span> V<span style="color:#f92672">.</span>copy()
</span></span><span style="display:flex;"><span>    policy <span style="color:#f92672">=</span> defaultdict(list)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    delta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(env<span style="color:#f92672">.</span>height):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(env<span style="color:#f92672">.</span>width):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> (i, j) <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> cliff_cells <span style="color:#f92672">and</span> (i, j) <span style="color:#f92672">!=</span> (end):
</span></span><span style="display:flex;"><span>                values <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">for</span> a <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">4</span>):
</span></span><span style="display:flex;"><span>                    (i_new, j_new), reward<span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(i, j, a)
</span></span><span style="display:flex;"><span>                    values<span style="color:#f92672">.</span>append(V[(i_new, j_new)] <span style="color:#f92672">*</span> gamma <span style="color:#f92672">+</span> reward)
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>                max_value <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(values)
</span></span><span style="display:flex;"><span>                best_actions <span style="color:#f92672">=</span> [a <span style="color:#66d9ef">for</span> a, v <span style="color:#f92672">in</span> enumerate(values) <span style="color:#66d9ef">if</span> v <span style="color:#f92672">==</span> max_value]
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span>                V_new[(i, j)] <span style="color:#f92672">=</span> max_value
</span></span><span style="display:flex;"><span>                policy[(i, j)] <span style="color:#f92672">=</span> best_actions
</span></span><span style="display:flex;"><span>                delta <span style="color:#f92672">=</span> max(delta, abs(V_new[(i, j)] <span style="color:#f92672">-</span> V[(i, j)]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> V_new, policy, delta
</span></span></code></pre></div><ul>
<li><code>V</code> is the value function, its key is a grid location of <code>(i, j)</code>, and its value is initialized to 0.</li>
<li>We iterate over all grid locations that are not cliffs or goal locations. For each grid, we iterate over its 4 actions and pick the action with the largest value to update the current value.</li>
<li>We use <code>max(abs(V_new[(i, j)] - V[i, j]))</code> as the difference between value iterations.</li>
</ul>
<p>Value iteration does not explicitly optimize the policy; instead, it is learned implicitly by selecting the action that maximizes its next value state.</p>
<h5 id="4222-training">4.2.2.2 Training<a hidden class="anchor" aria-hidden="true" href="#4222-training">#</a></h5>
<p>To train the value iteration until convergence:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">value_iteration_train</span>(env, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, tolerance<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>):
</span></span><span style="display:flex;"><span>    progress_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    V <span style="color:#f92672">=</span> defaultdict(float)
</span></span><span style="display:flex;"><span>    policy <span style="color:#f92672">=</span> defaultdict(list)
</span></span><span style="display:flex;"><span>    progress_data<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#34;V&#34;</span>: V<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;policy&#34;</span>: policy<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;delta&#34;</span>: float(<span style="color:#e6db74">&#34;inf&#34;</span>)})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        V, policy, delta <span style="color:#f92672">=</span> value_iterate(env, V, gamma)
</span></span><span style="display:flex;"><span>        progress_data<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#34;V&#34;</span>: V<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;policy&#34;</span>: policy<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;delta&#34;</span>: delta})
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> delta <span style="color:#f92672">&lt;</span> tolerance:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> progress_data
</span></span></code></pre></div><ul>
<li><code>gamma</code> is a discount factor that defines the current value of future rewards.</li>
<li>The training iteration stops when the difference between two value functions is <code>&lt;tolerance</code>.</li>
<li>We return the <code>V</code> and <code>policy</code> data during training for evaluation purposes.</li>
</ul>
<h5 id="4223-result-and-visualization">4.2.2.3 Result and Visualization<a hidden class="anchor" aria-hidden="true" href="#4223-result-and-visualization">#</a></h5>
<p>We run training using <code>gamma=0.9</code>, and it converges in 15 epochs.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dp_progress <span style="color:#f92672">=</span> value_iteration_train(env, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, tolerance<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Trained </span><span style="color:#e6db74">{</span>len(dp_progress)<span style="color:#e6db74">}</span><span style="color:#e6db74"> epochs&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Trained 15 epochs
</span></span></code></pre></div><p>We visualize both the value function and policy in epochs <code>1, 7, 14</code>:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>epoches <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">14</span>]
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, ax <span style="color:#f92672">in</span> enumerate(axs):
</span></span><span style="display:flex;"><span>    iter <span style="color:#f92672">=</span> epoches[i <span style="color:#f92672">//</span> <span style="color:#ae81ff">2</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> i <span style="color:#f92672">%</span> <span style="color:#ae81ff">2</span>:
</span></span><span style="display:flex;"><span>        env<span style="color:#f92672">.</span>render_plot(policy<span style="color:#f92672">=</span>dp_progress[iter][<span style="color:#e6db74">&#39;policy&#39;</span>], title <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Cliff Walking Policy in Epoch </span><span style="color:#e6db74">{</span>iter<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>, ax<span style="color:#f92672">=</span>ax)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        env<span style="color:#f92672">.</span>render_plot(value<span style="color:#f92672">=</span>dp_progress[iter][<span style="color:#e6db74">&#39;V&#39;</span>], title <span style="color:#f92672">=</span> <span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;Cliff Walking Value in Epoch </span><span style="color:#e6db74">{</span>iter<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>, ax<span style="color:#f92672">=</span>ax)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>tight_layout()
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()
</span></span></code></pre></div><p>From the visualization:</p>
<ul>
<li>In epoch 1, the agent learns to avoid the cliff.</li>
<li>In epoch 7, the agent learns the best actions on the right side of the grid, which are either to take <code>DOWN</code> or <code>RIGHT</code> actions to reach the goal grid.</li>
<li>In epoch 14, the value function converges, and the optimal path is to take <code>UP</code> from the start and then always take <code>RIGHT</code> until close to the goal.</li>
</ul>
<p><img alt="Value Iteration Visualization" loading="lazy" src="/images/CliffWalking-04-08-2025_01_17_PM.png"></p>
<h3 id="43-policy-iteration">4.3 Policy Iteration<a hidden class="anchor" aria-hidden="true" href="#43-policy-iteration">#</a></h3>
<p>In the previous <strong>Value Iteration</strong> method, during iterations, we only updated the value function until convergence. The policy is derived implicitly from the value function. So, can we optimize the policy directly? This leads us to another dynamic programming method in MDPs, <strong>Policy Iteration</strong>.</p>
<p>A policy iteration consists of two parts:</p>
<ol>
<li>
<p><strong>Policy Evaluation</strong>: Given a policy $\pi$, compute its state-value function $V^{\pi}(s)$, which is the expected return of following the policy $\pi$.
$$
V(s_t) = \sum P(s_{t+1} | s_t, \pi) * [r(s_t, \pi, s_{t+1}) + \gamma * V(s_{t+1})]
$$</p>
</li>
<li>
<p><strong>Policy Improvement</strong>: Update the agent&rsquo;s policy with respect to the current value function.
$$
\pi_{new}(s) = \underset{a}{argmax}\ \sum P(s_{t+1} | s_t, \pi) * [r(s_t, \pi, s_{t+1}) + \gamma * V(s_{t+1})]
$$</p>
</li>
</ol>
<h4 id="431-policy-iteration-for-cliff-walking">4.3.1 Policy Iteration for Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#431-policy-iteration-for-cliff-walking">#</a></h4>
<ul>
<li>Step 0, Initialization:
<ul>
<li>Initialize each state&rsquo;s value function to 0: $V(s) = 0$</li>
<li>Initialize the policy to take all 4 actions in all states.</li>
</ul>
</li>
<li>Step 1, <strong>policy evaluation</strong>, for each state $s_t$:
<ul>
<li>Search over its policy&rsquo;s actions $a_t$, each $a_t$ leads to a new state $s_{t+1}$.</li>
<li>Update the current state&rsquo;s value function with the mean reward of policy actions. $V(s_t) = \underset{a}{mean}(r_a + \gamma * V(s_{t+1}))$</li>
<li>Repeat until the value function converges.</li>
</ul>
</li>
<li>Step 2, <strong>policy improvement</strong>, for each state $s_t$:
<ul>
<li>Search over the current policy&rsquo;s actions $a_t$ at each $s_t$, compute its value function.</li>
<li>Update the policy $\pi(s_t)$ by only keeping actions with the largest value function.</li>
</ul>
</li>
<li>Repeat steps 1 and 2 until the policy doesn&rsquo;t change.</li>
</ul>
<h4 id="432-python-implementation-of-policy-iteration-for-cliff-walking">4.3.2 Python Implementation of Policy Iteration for Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#432-python-implementation-of-policy-iteration-for-cliff-walking">#</a></h4>
<h5 id="4321-policy-evaluation">4.3.2.1 Policy Evaluation<a hidden class="anchor" aria-hidden="true" href="#4321-policy-evaluation">#</a></h5>
<p>Let&rsquo;s first implement the <strong>policy evaluation</strong> function:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">policy_eval</span>(env, V, policy, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, tolerance<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-4</span>):
</span></span><span style="display:flex;"><span>    V_new <span style="color:#f92672">=</span> V<span style="color:#f92672">.</span>copy()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        delta <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(env<span style="color:#f92672">.</span>height):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(env<span style="color:#f92672">.</span>width):
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">if</span> (i, j) <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> env<span style="color:#f92672">.</span>cliff <span style="color:#f92672">and</span> (i, j) <span style="color:#f92672">!=</span> env<span style="color:#f92672">.</span>end:
</span></span><span style="display:flex;"><span>                    values <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>                    <span style="color:#66d9ef">for</span> a <span style="color:#f92672">in</span> policy[(i, j)]:
</span></span><span style="display:flex;"><span>                        (i_new, j_new), reward <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(i, j, a)
</span></span><span style="display:flex;"><span>                        values<span style="color:#f92672">.</span>append(V[(i_new, j_new)] <span style="color:#f92672">*</span> gamma <span style="color:#f92672">+</span> reward)
</span></span><span style="display:flex;"><span>                    
</span></span><span style="display:flex;"><span>                    V_new[(i, j)] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(values)
</span></span><span style="display:flex;"><span>                    delta <span style="color:#f92672">=</span> max(delta, abs(V_new[(i, j)] <span style="color:#f92672">-</span> V[(i, j)]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> delta <span style="color:#f92672">&lt;</span> tolerance:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>        V <span style="color:#f92672">=</span> V_new<span style="color:#f92672">.</span>copy()
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> V_new
</span></span></code></pre></div><ul>
<li>This function is very similar to the <code>value_iterate</code> function in value iteration, except for one major difference:
<ul>
<li>In <code>value_iterate</code>, we compute value functions among all actions and use <code>np.max(values)</code> to pick the best action, which means we are implicitly changing the policy using <code>argmax</code>.</li>
<li>In <code>policy_eval</code>, we only iterate actions in the existing policy <code>policy[(i, j)]</code>, and use <code>np.mean</code> to calculate the expected value function, which means we are only doing evaluation instead of policy optimization here.</li>
</ul>
</li>
<li>The function returns a new value function <code>V_new</code> after convergence.</li>
</ul>
<h5 id="4322-policy-improvement">4.3.2.2 Policy Improvement<a hidden class="anchor" aria-hidden="true" href="#4322-policy-improvement">#</a></h5>
<p>Then let&rsquo;s implement the <strong>policy improvement</strong> step:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">policy_improve</span>(env, V, policy, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>):
</span></span><span style="display:flex;"><span>    policy_new <span style="color:#f92672">=</span> defaultdict(list)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    policy_stable <span style="color:#f92672">=</span> <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(env<span style="color:#f92672">.</span>height):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(env<span style="color:#f92672">.</span>width):
</span></span><span style="display:flex;"><span>            current_state <span style="color:#f92672">=</span> (i, j)
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Skip cliff cells and the terminal goal state</span>
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> current_state <span style="color:#f92672">in</span> env<span style="color:#f92672">.</span>cliff <span style="color:#f92672">or</span> current_state <span style="color:#f92672">==</span> env<span style="color:#f92672">.</span>end:
</span></span><span style="display:flex;"><span>                <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            action_values <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> a <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">4</span>):
</span></span><span style="display:flex;"><span>                (i_new, j_new), reward <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step(i, j, a)
</span></span><span style="display:flex;"><span>                action_values<span style="color:#f92672">.</span>append(V[(i_new, j_new)] <span style="color:#f92672">*</span> gamma <span style="color:#f92672">+</span> reward)
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            max_val <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(action_values)
</span></span><span style="display:flex;"><span>            best_actions <span style="color:#f92672">=</span> [a <span style="color:#66d9ef">for</span> a, v <span style="color:#f92672">in</span> enumerate(action_values) <span style="color:#66d9ef">if</span> v <span style="color:#f92672">==</span> max_val]
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> set(best_actions) <span style="color:#f92672">!=</span> set(policy[(i, j)]):
</span></span><span style="display:flex;"><span>                policy_stable <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>            policy_new[(i, j)] <span style="color:#f92672">=</span> best_actions
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> policy_new, policy_stable
</span></span></code></pre></div><ul>
<li>The <code>policy_improve</code> is a one-step optimization, it takes in the current value function <code>V</code>, picked the <code>argmax</code> action to update the policy, it also takes in the current policy <code>policy</code> to compare whether there is any change between the two policies. It returns both the updated policy <code>policy_new</code> and a boolean indicating whether the policy changed during optimization.</li>
</ul>
<h5 id="4323-training">4.3.2.3 Training<a hidden class="anchor" aria-hidden="true" href="#4323-training">#</a></h5>
<p>Combining these two sub-steps, we can train using policy iteration:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">policy_iteration_train</span>(env, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, tolerance<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>):
</span></span><span style="display:flex;"><span>    V <span style="color:#f92672">=</span> defaultdict(float)
</span></span><span style="display:flex;"><span>    policy <span style="color:#f92672">=</span> defaultdict(<span style="color:#66d9ef">lambda</span> : range(<span style="color:#ae81ff">4</span>))    
</span></span><span style="display:flex;"><span>    progress_data <span style="color:#f92672">=</span> [{<span style="color:#e6db74">&#34;V&#34;</span>: V<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;policy&#34;</span>: policy<span style="color:#f92672">.</span>copy()}]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#66d9ef">True</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Value evaluation</span>
</span></span><span style="display:flex;"><span>        V <span style="color:#f92672">=</span> policy_eval(env, V, policy, gamma)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Policy improvement</span>
</span></span><span style="display:flex;"><span>        policy, policy_stable <span style="color:#f92672">=</span> policy_improve(env, V, policy, gamma)
</span></span><span style="display:flex;"><span>        progress_data<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#34;V&#34;</span>: V<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;policy&#34;</span>: policy<span style="color:#f92672">.</span>copy()})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> policy_stable:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>
</span></span><span style="display:flex;"><span>        idx <span style="color:#f92672">+=</span> <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> progress_data
</span></span></code></pre></div><ul>
<li>For all states, the value function <code>V</code> is default to 0, and the policy is default to all 4 actions.</li>
<li>The training iteration stops until the policy no longer changes.</li>
</ul>
<h5 id="4324-results-and-evaluation">4.3.2.4 Results and Evaluation<a hidden class="anchor" aria-hidden="true" href="#4324-results-and-evaluation">#</a></h5>
<p>We run training using <code>gamma=0.9</code>, and it converges in 6 epochs.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dp_progress <span style="color:#f92672">=</span> policy_iteration_train(env, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, tolerance<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Trained </span><span style="color:#e6db74">{</span>len(dp_progress)<span style="color:#e6db74">}</span><span style="color:#e6db74"> epochs&#34;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-text" data-lang="text"><span style="display:flex;"><span>Trained 6 epochs
</span></span></code></pre></div><p>We also visualize the value function and policy in epochs <code>1, 3, 5</code>:</p>
<ul>
<li>In epoch 1, because the initialized policy includes all actions, this leads to grid in the <code>i=2</code> row having a low value function as it has 25% of falling into the cliff and incurring <code>-100</code> reward, so the learned policy for most grids is to move upward and avoid the cliff.</li>
<li>In later epochs, since the policy no longer includes actions that lead to falling off the cliff, the value function improves for all grids, and it learns the optimal path towards the goal grid.</li>
</ul>
<p><img alt="Policy Iteration Visualization" loading="lazy" src="/images/CliffWalking-04-08-2025_02_49_PM.png"></p>
<h2 id="5-monte-carlo-methods">5. Monte-Carlo Methods<a hidden class="anchor" aria-hidden="true" href="#5-monte-carlo-methods">#</a></h2>
<h3 id="51-introduction">5.1 Introduction<a hidden class="anchor" aria-hidden="true" href="#51-introduction">#</a></h3>
<p>It&rsquo;s nice that we solved the cliff walking problem with DP methods, and what&rsquo;s more? Remember in DP we assumed full knowledge of the environment - specifically:</p>
<ul>
<li>The transition probability: $P(s_{t+1} | s_t, a_t)$</li>
<li>The reward function : $r(s_{t+1}, a_t, s_t)$</li>
</ul>
<p>What if the agent is in another environment that itself doesn&rsquo;t know any of such information ahead? Assume the agent was placed in the <code>start</code> location, with no knowledge about:</p>
<ul>
<li>Where is the <code>goal</code> grid, and how to reach it.</li>
<li>Which grid it will go to if taking an action and what reward it will get.</li>
</ul>
<p>Then the agent needs to interact with the environment to generate <strong>episodes</strong> (sequence of states, actions, rewards) until it reached the <code>goal</code> grid, and learn these information and optimize the policy during the interaction.</p>
<p>Compare the two methods, DP is like a <strong>planner</strong> who knows the full map and computes the best path. Monte-Carlo is like an <strong>explorer</strong> that tries different routes and keeps optimizing the policy.</p>
<h3 id="52-interaction-environment">5.2 Interaction Environment<a hidden class="anchor" aria-hidden="true" href="#52-interaction-environment">#</a></h3>
<p>We first need to change the <code>CliffWalk</code> environment to mimic an interaction environment.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CliffWalk</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, height, width, start, end, cliff):
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">...</span> <span style="color:#75715e"># Ignore previous codes</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>actions <span style="color:#f92672">=</span> [(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>), (<span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>), (<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)]  <span style="color:#75715e"># up, down, left, right</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>agent_pos <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>start
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">reset</span>(self):
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>agent_pos <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>start
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>agent_pos
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">step</span>(self, a: int) <span style="color:#f92672">-&gt;</span> tuple[tuple[int, int], int, bool]:
</span></span><span style="display:flex;"><span>        i, j <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>agent_pos
</span></span><span style="display:flex;"><span>        ni, nj <span style="color:#f92672">=</span> i <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>actions[a][<span style="color:#ae81ff">0</span>], j <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>actions[a][<span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Fell into Cliff, get -100 reward, back to start point</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (ni, nj) <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>cliff:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>start, <span style="color:#f92672">-</span><span style="color:#ae81ff">100</span>, <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (ni, nj) <span style="color:#f92672">==</span> self<span style="color:#f92672">.</span>end:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> (ni, nj), <span style="color:#ae81ff">0</span>, <span style="color:#66d9ef">True</span>
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Move, get -1 reward</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">&lt;=</span> ni <span style="color:#f92672">&lt;</span> self<span style="color:#f92672">.</span>height <span style="color:#f92672">and</span> <span style="color:#ae81ff">0</span> <span style="color:#f92672">&lt;=</span> nj <span style="color:#f92672">&lt;</span> self<span style="color:#f92672">.</span>width:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>agent_pos <span style="color:#f92672">=</span> ni, nj
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#75715e"># Move out of grid, get -1 reward</span>
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>agent_pos <span style="color:#f92672">=</span> i, j
</span></span><span style="display:flex;"><span>        done, reward <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> self<span style="color:#f92672">.</span>agent_pos, reward, done
</span></span></code></pre></div><p>Compare the new implementation of <code>step</code> function with the previous one:</p>
<ul>
<li>The new implementation only takes an <code>action</code> index, it tracks the agent&rsquo;s state using <code>self.agent_pos</code></li>
<li>We are forbidden to compute the state and reward for any $&lt;state, action&gt;$ now. The agent has to reach a specific $s_t$ and take an $a_t$, call <code>step</code> to finally get the $s_{t+1}, r_t$ from interaction.</li>
<li>The <code>step</code> function returns a boolean variable <code>done</code> indicating whether the agent reached the <code>goal</code> grid</li>
</ul>
<h3 id="53-monte-carlo-simulation">5.3 Monte-Carlo Simulation<a hidden class="anchor" aria-hidden="true" href="#53-monte-carlo-simulation">#</a></h3>
<p>Monte Carlo (MC) methods learn from complete episodes of interaction with the environment. The core idea is to estimate the value of a $&lt;state, action&gt;$ pair by averaging the total return observed after visiting a state across multiple episodes.</p>
<h4 id="531-epsilon-search-algorithm">5.3.1 $\epsilon$-search algorithm<a hidden class="anchor" aria-hidden="true" href="#531-epsilon-search-algorithm">#</a></h4>
<p>In RL systems, it&rsquo;s very common to face the exploration vs exploitation dilemma:</p>
<ul>
<li><strong>Exploitation</strong>: Pick the best known action so far (greedy)</li>
<li><strong>Exploration</strong>: Try other actions to discover potentially better ones</li>
</ul>
<p>If the agent always acts greedily, it may get stuck in suboptimal paths, without getting opportunities to discover potentially better paths. The $\epsilon$-search tries to balance this by introducing a random $\epsilon$, in each step:</p>
<ul>
<li><strong>Exploration</strong>: With probability $\epsilon$, choose a random action.</li>
<li><strong>Exploitation</strong>: With probability $1 - \epsilon$, choose the action with the highest value: $a = argmax\ Q(s, a)$</li>
</ul>
<h4 id="532-monte-carlo-method-in-cliff-walking">5.3.2 Monte-Carlo method in Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#532-monte-carlo-method-in-cliff-walking">#</a></h4>
<ul>
<li>
<p>Step 0, Initialization:</p>
<ul>
<li>Initialize a random value function: $Q(s, a)$</li>
<li>Initialize an $\epsilon$-greedy policy</li>
</ul>
</li>
<li>
<p>Step 1, Generate episodes:</p>
<ul>
<li>From the <code>start</code> state, follow the current policy to generate a full episode until the agent reaches the <code>goal</code> grid, we will get a sequence of $&lt;s_t, a_t, r_t&gt;$</li>
</ul>
</li>
<li>
<p>Step 2, Update value function: $Q(s, a)$</p>
<ul>
<li>For each $&lt;s_t, a_t&gt;$ pair in the episode trace, compute its return by $G_t = r\ + \gamma*G_{t+1}$</li>
<li>Update $Q(s, a)$ by averaging returns across multiple episodes.</li>
</ul>
</li>
<li>
<p>Step 3, Improve policy:</p>
<ul>
<li>The new policy is the $\epsilon$-greedy policy with the updated value function $Q(s, a)$.</li>
</ul>
</li>
<li>
<p>Repeat step 1-3 until the policy converges.</p>
</li>
</ul>
<h4 id="533-python-implementation-of-mc-in-cliff-walking">5.3.3 Python Implementation of MC in Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#533-python-implementation-of-mc-in-cliff-walking">#</a></h4>
<h5 id="5331-epsilon-greedy-search">5.3.3.1 $\epsilon$-greedy search<a hidden class="anchor" aria-hidden="true" href="#5331-epsilon-greedy-search">#</a></h5>
<p>This function implements the $\epsilon$-search to pick the action,</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">epsilon_greedy</span>(action_values, epsilon):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand() <span style="color:#f92672">&lt;</span> epsilon:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Random action</span>
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Optimized action</span>
</span></span><span style="display:flex;"><span>        max_val <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(action_values)
</span></span><span style="display:flex;"><span>        best_actions <span style="color:#f92672">=</span> [i <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">4</span>) <span style="color:#66d9ef">if</span> action_values[i] <span style="color:#f92672">==</span> max_val]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Random pick among best actions</span>
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(best_actions)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> action
</span></span></code></pre></div><h5 id="5332-mc-simulation">5.3.3.2 MC-Simulation<a hidden class="anchor" aria-hidden="true" href="#5332-mc-simulation">#</a></h5>
<p>This function simulates 1 episode of MC simulation.</p>
<ul>
<li>At the beginning, <code>env.reset()</code> sets the agent to the <code>start</code> state.</li>
<li><code>Q</code> is the Value table, with the key being the current location <code>(i, j)</code>, and the value being a list of size 4, the value at index <code>k</code> represents the value for action <code>k</code>.</li>
<li>The simulation stops after it reaches the <code>goal</code> state.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">mc_simulation</span>(env, Q, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>    curr_eps <span style="color:#f92672">=</span> epsilon
</span></span><span style="display:flex;"><span>    episode_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> epsilon_greedy(Q[state], epsilon)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        next_state, reward, done <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step_interactive(action)
</span></span><span style="display:flex;"><span>        episode_data<span style="color:#f92672">.</span>append((state, action, reward))
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> next_state
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> episode_data, done
</span></span></code></pre></div><h5 id="5333-value-function-update">5.3.3.3 Value Function Update<a hidden class="anchor" aria-hidden="true" href="#5333-value-function-update">#</a></h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">improve_policy</span>(Q, returns, episode_data, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Compute reward</span>
</span></span><span style="display:flex;"><span>    visited <span style="color:#f92672">=</span> set()
</span></span><span style="display:flex;"><span>    G <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> t <span style="color:#f92672">in</span> reversed(range(len(episode_data))):
</span></span><span style="display:flex;"><span>        state_t, action_t, reward_t <span style="color:#f92672">=</span> episode_data[t]
</span></span><span style="display:flex;"><span>        G <span style="color:#f92672">=</span> gamma<span style="color:#f92672">*</span>G <span style="color:#f92672">+</span> reward_t
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># First-time update</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (state_t, action_t) <span style="color:#f92672">not</span> <span style="color:#f92672">in</span> visited:
</span></span><span style="display:flex;"><span>            visited<span style="color:#f92672">.</span>add((state_t, action_t))
</span></span><span style="display:flex;"><span>            returns[(state_t, action_t)]<span style="color:#f92672">.</span>append(G)
</span></span><span style="display:flex;"><span>            Q[state_t][action_t] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>mean(returns[(state_t, action_t)])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    policy <span style="color:#f92672">=</span> defaultdict(int)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> Q<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        policy[k] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>argmax(v)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> Q, policy
</span></span></code></pre></div><ul>
<li><code>returns</code> is a dictionary, with the key being a <code>&lt;state, action&gt;</code> combination, and the value being a list that stores its expected reward in each episode.</li>
<li>For each episode, we traversed backwards, iteratively computing each state&rsquo;s value using the function: $G_t = r_t + \gamma * G_{t+1}$.</li>
</ul>
<h5 id="5334-monte-carlo-training">5.3.3.4 Monte-Carlo training<a hidden class="anchor" aria-hidden="true" href="#5334-monte-carlo-training">#</a></h5>
<p>Combining the previous steps, we can train the agent:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">monte_carlo_training</span>(env, num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>    Q <span style="color:#f92672">=</span> defaultdict(<span style="color:#66d9ef">lambda</span>: [<span style="color:#ae81ff">0.1</span>] <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>)
</span></span><span style="display:flex;"><span>    returns <span style="color:#f92672">=</span> defaultdict(list)
</span></span><span style="display:flex;"><span>    progress_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> tqdm<span style="color:#f92672">.</span>tqdm(range(num_episodes)):
</span></span><span style="display:flex;"><span>        epsilon <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">0.01</span>, epsilon<span style="color:#f92672">*</span><span style="color:#ae81ff">0.99</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Run MC simulation</span>
</span></span><span style="display:flex;"><span>        episode_data, finished <span style="color:#f92672">=</span> mc_simulation(env, Q, epsilon)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> <span style="color:#f92672">not</span> finished:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">continue</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Policy improvement</span>
</span></span><span style="display:flex;"><span>        Q, policy <span style="color:#f92672">=</span> improve_policy(Q, returns, episode_data, gamma)
</span></span><span style="display:flex;"><span>        progress_data<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#34;Q&#34;</span>: copy<span style="color:#f92672">.</span>deepcopy(Q), <span style="color:#e6db74">&#34;policy&#34;</span>: copy<span style="color:#f92672">.</span>deepcopy(policy), <span style="color:#e6db74">&#34;episode&#34;</span>: episode})
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> progress_data
</span></span></code></pre></div><ul>
<li><code>Q</code> is initialized by giving equal weights to each action.</li>
<li>We set $\epsilon$ to decay over episodes, <code>epsilon = max(0.01, epsilon*0.99)</code>. In earlier episodes, the agent has no prior knowledge, so we encourage more exploration, then in later episodes focus more on exploitation.</li>
</ul>
<h5 id="5335-results-and-visualizations">5.3.3.5 Results and Visualizations<a hidden class="anchor" aria-hidden="true" href="#5335-results-and-visualizations">#</a></h5>
<p>We run MC sampling for 5000 episodes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>progress_data <span style="color:#f92672">=</span> monte_carlo_training(env, num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">5000</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.3</span>)
</span></span></code></pre></div><p><img alt="Monte-Carlo Visualization" loading="lazy" src="/images/CliffWalking-04-09-2025_04_41_PM.png"></p>
<p>We can see the learned policy is not ideally the optimal shortest path, and the agent is trying to avoid the grid next to the cliff in its first several steps, why? This is explainable:</p>
<ul>
<li>The $Q$ value function is averaged over episodes, an early cliff fall trace will drag the <strong>average return</strong> for those cliff-adjacent grids.</li>
<li>Also, we used $\epsilon$-greedy policy, so even in later episodes when the agent learned a good policy, they will still randomly explore and occasionally fall off the cliff in cliff-adjacent grids.</li>
</ul>
<h2 id="6-temporal-difference-methods">6. Temporal-Difference Methods<a hidden class="anchor" aria-hidden="true" href="#6-temporal-difference-methods">#</a></h2>
<h3 id="61-introduction">6.1 Introduction<a hidden class="anchor" aria-hidden="true" href="#61-introduction">#</a></h3>
<p>In previous illustrations of Monte Carlo methods, it estimates the value function using complete episodes. While this is intuitively simple and unbiased, it&rsquo;s very sample-inefficient. The value function updates only happen at the end of episodes, learning can be slow—especially in environments with long or variable episode lengths.</p>
<p>Temporal-Difference (TD) methods address these limitations by updating value estimates after each time step using bootstrapped predictions, leading to faster and more stable learning.</p>
<p>I found an intuitive way to understand the difference between TD and MC methods are compare this to <strong>Gradient Descent</strong> and <strong>SGD</strong> in neural network optimization, but in the temporal axis, view one step in RL as one batch in supervised model training.</p>
<ul>
<li>Gradient descent computes the gradient using the full dataset, while <em>SGD</em> compute using only data points in the current batch, update the parameters, then move to the next batch.</li>
<li>Monte-Carlo methods generate a full episode, backpropagate along the episode to update the value function. While TD methods run one step, use its TD difference to update the value function, then move to the next step.</li>
</ul>
<p>Then how is TD-difference computed, remember we want to estimate value function using:
$$
V(s_t)\ = r_{t+1} + \gamma\ V(s_{t+1})
$$</p>
<p>So we can bootstrap at $s_t$, execute one more step and compute the value estimates and use it to update the value function:
$$
G(s_t) = r_{t+1} + \gamma\ V(s_{t+1})
\newline
\text{TD Error} = G(s_t) - V(s_t)
\newline
V(s_t) \leftarrow V(s_t) + \alpha \cdot (G(s_t) - V(s_t))
$$</p>
<ul>
<li>$\gamma$ is the discount factor</li>
<li>$\alpha$ is the single step learning rate</li>
</ul>
<h3 id="62-sarsa">6.2 SARSA<a hidden class="anchor" aria-hidden="true" href="#62-sarsa">#</a></h3>
<p>SARSA is one of the most straightforward ways in TD-methods. The idea is intuitive, using next step&rsquo;s $Q(s_{t+1}, a_{t+1})$ to subtract current step&rsquo;s $Q(s_{t}, a_{t})$ as the TD error, and update value function.
$$
G(s_t) = r_{t+1} + \gamma\ Q(s_{t+1}, a_{t+1}) \newline
Q(s_{t}, a_{t}) \leftarrow Q(s_{t}, a_{t}) + \alpha \cdot (G(s_t) - Q(s_t, a_t))
$$
In every step, we need to get its current state $s_t$, action $a_t$, bootstrap one step forward, get the reward $r_{t+1}$, the new state $s_{t+1}$ and action $a_{t+1}$. In each step, we need the sequence of $&lt;s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}&gt;$, and this is why this method is called <em>SARSA</em>.</p>
<h4 id="621-sarsa-method-in-cliff-walking">6.2.1 SARSA method in Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#621-sarsa-method-in-cliff-walking">#</a></h4>
<ul>
<li>
<p>Step 0, Initialization:</p>
<ul>
<li>Initialize a random value function: $Q(s, a)$</li>
<li>Initialize an $\epsilon$-greedy policy</li>
</ul>
</li>
<li>
<p>Step 1, Bootstrap a step:</p>
<ul>
<li>Agent in state $s_t$ and action $a_t$</li>
<li>Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$</li>
<li>Use the same policy to get the new action $a_{t+1}$</li>
</ul>
</li>
<li>
<p>Step 2, Update value function for the step: $Q(s_t, a_t)$</p>
<ul>
<li>$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \alpha \cdot (r_{t+1} + \gamma\ Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t))$</li>
</ul>
</li>
<li>
<p>Finish 1 episode by repeated running step 1-2 until the agent reached <code>goal</code> state.</p>
</li>
<li>
<p>Run above algorithm multiple times until the policy converges.</p>
</li>
</ul>
<p>This looks very similar to SARSA, the only difference is:</p>
<ul>
<li>We no longer need to sample $s_{t+1}$ in each step.</li>
<li>We used $max\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update.</li>
</ul>
<h4 id="622-python-implementation-of-sarsa">6.2.2 Python Implementation of SARSA<a hidden class="anchor" aria-hidden="true" href="#622-python-implementation-of-sarsa">#</a></h4>
<h5 id="6121-sarsa">6.1.2.1 SARSA<a hidden class="anchor" aria-hidden="true" href="#6121-sarsa">#</a></h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sarsa_one_epoch</span>(env, Q, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Get init action</span>
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>    action <span style="color:#f92672">=</span> epsilon_greedy(Q[state], epsilon)
</span></span><span style="display:flex;"><span>    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
</span></span><span style="display:flex;"><span>        next_state, reward, done <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step_interactive(action)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Sample next action</span>
</span></span><span style="display:flex;"><span>        next_action <span style="color:#f92672">=</span> epsilon_greedy(Q[next_state], epsilon)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># TD-Update current function</span>
</span></span><span style="display:flex;"><span>        Q[state][action] <span style="color:#f92672">+=</span> alpha<span style="color:#f92672">*</span>(reward <span style="color:#f92672">+</span> gamma<span style="color:#f92672">*</span>Q[next_state][next_action] <span style="color:#f92672">-</span> Q[state][action])
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        state, action <span style="color:#f92672">=</span> next_state, next_action
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    policy <span style="color:#f92672">=</span> defaultdict(int)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> Q<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        best_actions <span style="color:#f92672">=</span> [a <span style="color:#66d9ef">for</span> a, i <span style="color:#f92672">in</span> enumerate(v) <span style="color:#66d9ef">if</span> i <span style="color:#f92672">==</span> np<span style="color:#f92672">.</span>max(v)]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Random pick among best actions</span>
</span></span><span style="display:flex;"><span>        policy[k] <span style="color:#f92672">=</span> best_actions
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> Q, policy
</span></span></code></pre></div><ul>
<li>We use the same $\epsilon$-greedy search to get the action</li>
<li>The $Q$ value function is updated within each step of the epoch, this is called <strong>1-step SARSA</strong>, alternatively, we can also update $Q$ value function every fixed number of steps, which is called <strong>$n$-step SARSA</strong></li>
</ul>
<p>To train multiple episodes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">td_sarsa_training</span>(env, num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Key: position, Value: value for of each action</span>
</span></span><span style="display:flex;"><span>    Q <span style="color:#f92672">=</span> defaultdict(<span style="color:#66d9ef">lambda</span>: np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">4</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.01</span>)
</span></span><span style="display:flex;"><span>    progress_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> tqdm<span style="color:#f92672">.</span>tqdm(range(num_episodes)):
</span></span><span style="display:flex;"><span>        epsilon <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">0.01</span>, epsilon<span style="color:#f92672">*</span><span style="color:#ae81ff">0.95</span>)
</span></span><span style="display:flex;"><span>        Q, policy <span style="color:#f92672">=</span> sarsa_one_epoch(env, Q, gamma, epsilon, alpha)
</span></span><span style="display:flex;"><span>        progress_data<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#34;Q&#34;</span>: Q<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;policy&#34;</span>: policy<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;episode&#34;</span>: episode})
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> progress_data
</span></span></code></pre></div><ul>
<li>Similar to that of Monte-Carlo methods, we used a decaying $\epsilon$ for action search, to encourage more exploration in early episodes and more exploitation in later episodes.</li>
</ul>
<h5 id="6122-visualization-and-result">6.1.2.2 Visualization and Result<a hidden class="anchor" aria-hidden="true" href="#6122-visualization-and-result">#</a></h5>
<p>We train SARSA for 10000 episodes, and visualize the result</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sarsa_progress <span style="color:#f92672">=</span> td_sarsa_training(env, num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">10000</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>)
</span></span></code></pre></div><p><img alt="SARSA Training Visualization" loading="lazy" src="/images/CliffWalking-04-09-2025_09_47_PM.png"></p>
<p>The learned policy in <code>epoch=9999</code> is similar to that learned from MC methods, that it tries to avoid the cliff-adjacent grids, the reasoning is also similar:</p>
<ul>
<li>The agent used $\epsilon$-greedy search, so even the agent learned a good policy, its exploration nature may still lead to fall off in cliff-adjacent grids. So the agent learned to walk far away from the cliff, taking the constant cost of extra <code>-1</code> reward, to avoid a potential <code>-100</code> reward.</li>
</ul>
<h3 id="63-q-learning">6.3 Q-Learning<a hidden class="anchor" aria-hidden="true" href="#63-q-learning">#</a></h3>
<p>Let&rsquo;s recap the SARSA algorithm again, it used $\epsilon$-greedy search on $Q$ value functions for two purposes:</p>
<ol>
<li><strong>Planning</strong>: Decide the action $a_{t+1}$ of the next step</li>
<li><strong>Policy Update</strong>: use the actual action $a_{t+1}$ to update the value function.</li>
</ol>
<p>So this policy has to incorporate a trade-off between exploration and exploitation. What if we have two policies:</p>
<ol>
<li>One <strong>Behavior Policy</strong> that focuses on exploration, it decides the interaction with the environment.</li>
<li>One <strong>Target Policy</strong> that focuses on exploitation, it doesn&rsquo;t do interaction, but focuses on learning from previous interactions.</li>
</ol>
<p>Then the behavior policy can be more aggressive to keep exploring risky areas, without fearing these risky behaviors affect its value function. On the other hand, its target policy focuses on greedily learning the optimal policy, without being penalized by random exploratory behaviors.</p>
<p>This new method is called Q-Learning, the difference between SARSA and Q-Learning can also formalize as <strong>On-Policy</strong> vs <strong>Off-Policy</strong>:</p>
<ul>
<li><strong>On-Policy</strong> learns the value of the policy it is actually using to make decisions.</li>
<li><strong>Off-Policy</strong> Learns the value of a different policy than the one it is currently using to make decisions.</li>
</ul>
<p>In SARSA, we used the actual value $Q(s_{t+1}, a_{t+1})$ to update the value function:</p>
<p>$$
G(s_t) = r_{t+1} + \gamma\ Q(s_{t+1}, a_{t+1}) \newline
Q(s_{t}, a_{t}) \leftarrow Q(s_{t}, a_{t}) + \alpha \cdot (G(s_t) - Q(s_t, a_t))
$$</p>
<p>In Q-Learning, we used the theoretical optimal next action instead of the actual next action for updates:
$$
G(s_t) = r_{t+1} + \gamma\ \underset{a}{max}\ Q(s_{t+1}) \newline
Q(s_{t}, a_{t}) \leftarrow Q(s_{t}, a_{t}) + \alpha \cdot (G(s_t) - Q(s_t, a_t))
$$</p>
<h4 id="631-q-learning-in-cliff-walking">6.3.1 Q-Learning in Cliff Walking<a hidden class="anchor" aria-hidden="true" href="#631-q-learning-in-cliff-walking">#</a></h4>
<ul>
<li>
<p>Step 0, Initialization:</p>
<ul>
<li>Initialize a random value function: $Q(s, a)$</li>
<li>Initialize an $\epsilon$-greedy policy</li>
</ul>
</li>
<li>
<p>Step 1, Bootstrap a step:</p>
<ul>
<li>Agent in state $s_t$ and action $a_t$</li>
<li>Bootstrap $a_t$, get the reward $r_{t+1}$ and new state $s_{t+t}$</li>
</ul>
</li>
<li>
<p>Step 2, Update value function for the step: $Q(s_t, a_t)$</p>
<ul>
<li>$Q(s_{t}, a_{t}) = Q(s_{t}, a_{t}) + \alpha \cdot (r_{t+1} + \gamma\ max\ Q(s_{t+1}, a) - Q(s_t, a_t))$</li>
</ul>
</li>
<li>
<p>Finish 1 episode by repeated running step 1-2 until the agent reached <code>goal</code> state.</p>
</li>
<li>
<p>Run above algorithm multiple times until the policy converges.</p>
</li>
</ul>
<p>This looks very similar to SARSA, the only difference is:</p>
<ul>
<li>We no longer need to sample $s_{t+1}$ in each step.</li>
<li>We used $max\ Q(s_{t+1})$ instead of $Q(s_{t+1}, a_{t+1})$ for value update.</li>
</ul>
<h4 id="632-python-implementation-of-q-learning">6.3.2 Python Implementation of Q-Learning<a hidden class="anchor" aria-hidden="true" href="#632-python-implementation-of-q-learning">#</a></h4>
<h5 id="6321-q-learning">6.3.2.1 Q-Learning<a hidden class="anchor" aria-hidden="true" href="#6321-q-learning">#</a></h5>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">q_learning_one_epoch</span>(env, Q, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Get init action</span>
</span></span><span style="display:flex;"><span>    state <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>reset()
</span></span><span style="display:flex;"><span>    done <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">while</span> <span style="color:#f92672">not</span> done:
</span></span><span style="display:flex;"><span>        action <span style="color:#f92672">=</span> theta_greedy_action(Q, state, epsilon)
</span></span><span style="display:flex;"><span>        next_state, reward, done <span style="color:#f92672">=</span> env<span style="color:#f92672">.</span>step_interactive(action)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># TD-Update current function</span>
</span></span><span style="display:flex;"><span>        Q[state][action] <span style="color:#f92672">+=</span> alpha<span style="color:#f92672">*</span>(reward <span style="color:#f92672">+</span> gamma<span style="color:#f92672">*</span>max(Q[next_state]) <span style="color:#f92672">-</span> Q[state][action])
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        state <span style="color:#f92672">=</span> next_state
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    policy <span style="color:#f92672">=</span> defaultdict(int)
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> k, v <span style="color:#f92672">in</span> Q<span style="color:#f92672">.</span>items():
</span></span><span style="display:flex;"><span>        best_actions <span style="color:#f92672">=</span> [a <span style="color:#66d9ef">for</span> a, i <span style="color:#f92672">in</span> enumerate(v) <span style="color:#66d9ef">if</span> i <span style="color:#f92672">==</span> np<span style="color:#f92672">.</span>max(v)]
</span></span><span style="display:flex;"><span>        policy[k] <span style="color:#f92672">=</span> best_actions
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> Q, policy
</span></span></code></pre></div><ul>
<li>We no longer computed <code>next_action</code> in each step.</li>
<li><code>Q</code> is updated using <code>max(Q[next_state])</code>.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">q_learning_training</span>(env, num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">0.9</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Key: position, Value: value for of each action</span>
</span></span><span style="display:flex;"><span>    Q <span style="color:#f92672">=</span> defaultdict(<span style="color:#66d9ef">lambda</span>: np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>rand(<span style="color:#ae81ff">4</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.01</span>)
</span></span><span style="display:flex;"><span>    progress_data <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> episode <span style="color:#f92672">in</span> tqdm<span style="color:#f92672">.</span>tqdm(range(num_episodes)):
</span></span><span style="display:flex;"><span>        epsilon <span style="color:#f92672">=</span> max(<span style="color:#ae81ff">0.01</span>, epsilon<span style="color:#f92672">*</span><span style="color:#ae81ff">0.95</span>)
</span></span><span style="display:flex;"><span>        Q, policy <span style="color:#f92672">=</span> q_learning_one_epoch(env, Q, gamma, epsilon, alpha)
</span></span><span style="display:flex;"><span>        progress_data<span style="color:#f92672">.</span>append({<span style="color:#e6db74">&#34;Q&#34;</span>: Q<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;policy&#34;</span>: policy<span style="color:#f92672">.</span>copy(), <span style="color:#e6db74">&#34;episode&#34;</span>: episode})
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> progress_data
</span></span></code></pre></div><h5 id="6322-visualization-and-result">6.3.2.2 Visualization and Result<a hidden class="anchor" aria-hidden="true" href="#6322-visualization-and-result">#</a></h5>
<p>Q-Learning converges faster than SARSA, we only trained 200 episodes.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>q_learning_progress <span style="color:#f92672">=</span> q_learning_training(env, num_episodes<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">1.0</span>, epsilon<span style="color:#f92672">=</span><span style="color:#ae81ff">0.1</span>, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.2</span>)
</span></span></code></pre></div><p><img alt="Q-Learning Training Visualization" loading="lazy" src="/images/CliffWalking-04-09-2025_11_18_PM.png"></p>
<p>While SARSA found a <strong>safe path</strong> under randomness of $\epsilon$-greedy, Q-Learning found the shortest optimal path- It learns to hug off the cliff!</p>
<h2 id="7-summary">7. Summary<a hidden class="anchor" aria-hidden="true" href="#7-summary">#</a></h2>
<ul>
<li>
<p><strong>Model-Based vs. Model-Free</strong>: Dynamic Programming (DP) is a model-based approach requiring full knowledge of the environment&rsquo;s dynamics. In contrast, Monte Carlo (MC) and Temporal-Difference (TD) methods are model-free, learning directly from interactions.</p>
</li>
<li>
<p><strong>On-Policy vs. Off-Policy</strong>: SARSA is an on-policy method, meaning it evaluates and improves the policy that is used to make decisions. Q-Learning is off-policy, as it evaluates the optimal policy independently of the agent&rsquo;s actions.</p>
</li>
<li>
<p><strong>Exploration vs. Exploitation</strong>: Reinforcement Learning (RL) faces the dilemma of choosing between exploring new actions to discover their effects and exploiting known actions to maximize reward. Techniques like $\epsilon$-greedy balance this trade-off.</p>
</li>
<li>
<p><strong>Bellman Equations</strong>: Central to DP and TD methods, these equations provide the foundation for iteratively improving value functions and policies.</p>
</li>
<li>
<p><strong>Comparison of DP, MC, and TD</strong>:</p>
<ul>
<li><strong>DP</strong>: Requires a complete model of the environment and is computationally intensive but provides exact solutions.</li>
<li><strong>MC</strong>: Sample-based and learns from complete episodes, making it intuitive but potentially inefficient for long episodes.</li>
<li><strong>TD</strong>: Bootstrap-based, updating estimates at each step, offering a balance between DP&rsquo;s precision and MC&rsquo;s sampling.</li>
<li><strong>Intuitive Comparison</strong>: If Temporal-Difference methods require broader updates, they resemble Dynamic Programming, as DP considers all states for updates. Conversely, if TD methods require deeper updates, they resemble Monte Carlo methods, which rely on complete episodes for learning.</li>
</ul>
</li>
</ul>
<h3 id="71-whats-more">7.1. What&rsquo;s More<a hidden class="anchor" aria-hidden="true" href="#71-whats-more">#</a></h3>
<p>In the cliff walking problem, the state space is relatively small, consisting of only about 20 grid cells. This allows us to use tabular methods, where we store the value function, either $V(s)$ or $Q(s, a)$, in a table. However, in more complex problems like self-driving cars or Atari games, the state space becomes significantly larger. This complexity necessitates the use of parametrized methods, which employ function approximations, such as neural networks, to estimate value functions or policies.</p>
<p>These advanced methods enable RL algorithms to handle complex, high-dimensional environments, paving the way for techniques like Deep Q-Learning and Policy Gradient methods. Such approaches are often employed in the post-training of large language models (LLMs).</p>
<h2 id="8-references">8. References<a hidden class="anchor" aria-hidden="true" href="#8-references">#</a></h2>
<ul>
<li>Sutton, R. S., &amp; Barto, A. G. (2018). <a href="http://www.incompleteideas.net/book/the-book-2nd.html"><em>Reinforcement Learning: An Introduction (2nd ed.)</em></a>. MIT Press.</li>
<li>Wang, Q., Yang, Y., &amp; Jiang, J. (2022). <a href="https://github.com/datawhalechina/easy-rl"><em>Easy RL: Reinforcement Learning Tutorial</em></a>. Posts &amp; Telecom Press.</li>
</ul>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/reinforcement-learning/">Reinforcement Learning</a></li>
      <li><a href="http://localhost:1313/tags/monte-carlo/">Monte Carlo</a></li>
      <li><a href="http://localhost:1313/tags/dynamic-programming/">Dynamic Programming</a></li>
      <li><a href="http://localhost:1313/tags/temporal-difference/">Temporal Difference</a></li>
    </ul>
<nav class="paginav">
  <a class="next" href="http://localhost:1313/posts/kv-cache/">
    <span class="title">Next »</span>
    <br>
    <span>KV Cache Explained</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share An Introduction to Reinforcement Learning using Cliff Walking Problem on x"
            href="https://x.com/intent/tweet/?text=An%20Introduction%20to%20Reinforcement%20Learning%20using%20Cliff%20Walking%20Problem&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f&amp;hashtags=ReinforcementLearning%2cMonteCarlo%2cDynamicProgramming%2cTemporalDifference">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share An Introduction to Reinforcement Learning using Cliff Walking Problem on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f&amp;title=An%20Introduction%20to%20Reinforcement%20Learning%20using%20Cliff%20Walking%20Problem&amp;summary=An%20Introduction%20to%20Reinforcement%20Learning%20using%20Cliff%20Walking%20Problem&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share An Introduction to Reinforcement Learning using Cliff Walking Problem on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f&title=An%20Introduction%20to%20Reinforcement%20Learning%20using%20Cliff%20Walking%20Problem">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share An Introduction to Reinforcement Learning using Cliff Walking Problem on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share An Introduction to Reinforcement Learning using Cliff Walking Problem on whatsapp"
            href="https://api.whatsapp.com/send?text=An%20Introduction%20to%20Reinforcement%20Learning%20using%20Cliff%20Walking%20Problem%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share An Introduction to Reinforcement Learning using Cliff Walking Problem on telegram"
            href="https://telegram.me/share/url?text=An%20Introduction%20to%20Reinforcement%20Learning%20using%20Cliff%20Walking%20Problem&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share An Introduction to Reinforcement Learning using Cliff Walking Problem on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=An%20Introduction%20to%20Reinforcement%20Learning%20using%20Cliff%20Walking%20Problem&u=http%3a%2f%2flocalhost%3a1313%2fposts%2frl-intro%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/">Hxx&#39;Log</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
