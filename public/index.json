[{"content":"","permalink":"http://localhost:1313/about/","summary":"","title":"About"},{"content":"What is KV Cache? I am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post Transformers KV Caching Explained very intuitive, so I\u0026rsquo;ll just steal the gif here.\nTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is: $$ q_{i} = embed_i * W_q \\quad(1, d_{model}) $$ $$ k_{i} = embed_i * W_k \\quad(1, d_{model}) $$ $$ v_{i} = embed_i * W_v \\quad(1, d_{model}) $$ $$ K = concat(k_{0}, k_{1}, \u0026hellip;, k_{i}) \\quad(i+1, d_{model}) $$ $$ Attn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1) $$ $$ Output = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}] \\quad(1, d_{model}) $$\nAs we can see, at step $i$, its output is computed using that step\u0026rsquo;s query $q_{i}$, as well as keys and values of all tokens up to $i$. So the intuition of KV Cache pretty straightforward: to store keys and values of all tokens up to $i$, so to avoid execssive computation during matrix multiplications.\nHow many FLOPs are saved by KV Cache? Let\u0026rsquo;s run an analysis on the FLOPs of attention layer.\nFLOPs for matrix multiplication:\nIf we are doing matrix multiplication between matrices of respective size of $(m, n)$ and $(n, p)$:\nA signle multiplication is 1 operation. A single addition is 1 operation. Computing element at $(i, j)$ would take n multiplcaitions and (n-1) addtions, in total $2n - 1$ operations. The output is a matrix of size $(m,p)$, and total operations is $(2n-1) * m * p$, we ignore the $-1$ notion for simplicity, so in total $2mnp$ operations.\nAssuming we have GPT model with $n$ layers, each transformer block has $k$ heads. The model dimension is $d_{model}$, and each head has $d_{model} / k$ dimension. Assuming we are doing batch inference on $b$ samples with sequence length $s$.\nTotal flops without KV Cache: 1. Embedding Lookup This part does not has arithmetic operations, only table lookups, ignore it.\n2. Self-Attention For a self attention layer, at step $i$,\nCompute $Q$: compute $q_i$ only, $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ Compute $K$: compute $k_{0-\u0026gt;i}$, $2b * i * d_{model}^2$ FLOPs. $$ (b, i, d_{model}) . (d_{model}, d_{model}) = (b, i, d_{model}) $$ Compute $V$: similar to step 2, $2b * i * d_{model}^2$ FLOPs $QK^T$, $2b * i * d_{model}$ FLOPs. $$ (b, 1, d_{model}) . (b, i, d_{model}) = (b, 1, i) $$ Weighted Value $attn*V$: $2b * i * d_{model}$ FLOPs. $$ (b, 1, i) . (b, i, d_{model}) = (b, 1, d_model) $$ Linear projection: $2b * d_{model}^2$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, d_{model}) = (b, 1, d_{model}) $$ 3. MLP\nThere are two matrix multiplications in MLP, each with $8b*d_{model}^2$ FLOPs.\n$$ (b, 1, d_{model}) . (d_{model}, 4d_{model}) = (b, 1, 4d_{model}) $$ $$ (b, 1, 4d_{model}) . (4d_{model}, d_{model}) = (b, 1, d_{model}) $$\n4. Final projection layer The final layer is to project the output to vocab size $V$, which is $2b * d_{model}* V$ FLOPs. $$ (b, 1, d_{model}) . (d_{model}, V) = (b, 1, V) $$\nTo sum these numbers up, as well as integral $i$ over $[1, s]$, in a GPT with $L$ layers, we have total flops:\n$$ FLOPs = (2b * d_{model}^2 * s^2 + 20b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nFlops with addtional KV Cache: When KV Cache is used, the main optimization happened when computing $K$ and $V$ in self attention layer. Instead of doing matrix multiplication to compute $K_{j \\in [0, i]}$ and $V_{j \\in [0, i]}$, we cached and fetched $K_{j \\in [0, i-1]}$ and $V_{j \\in [0, i-1]}$, and only compute $K_j$ and $V_j$. The FLOPs at step $i$ is reduced from $2b \\times d_{model}^2 \\times i$ to $2b \\times d_{model}^2 $. Integral over $i$, th quaratic part of $s$ decreasefrom $2bd_{model}^2s^2$ to $4bd_{model}^2s$.\nThe total FLOPs becomes:\n$$ FLOPs_{sum_{i=1}^s} = (24b * d_{model}^2 * s) * L + 2b * d_{model} * V * s $$\nWithout KV Cache, the operations scaled quadratically with the sequence length $s$. With KV Cache, the operations scale linearly with $s$, which makes it more efficient for longer sequences.\nFLOPs calculation with an example Let\u0026rsquo;s look at the FLOPs calculation using GPT3-medium as an example. Say we have: $$ d_{model} = 1024, L = 24, V = 50257 $$\nSequence Length (s) Without KV Cache With KV Cache Reduction Percentage 10 $1.11 \\times 10^{10}$ $7.07 \\times 10^9$ 36.29% 100 $5.64 \\times 10^{11}$ $7.07 \\times 10^{10}$ 87.46% 500 $1.29 \\times 10^{13}$ $3.53 \\times 10^{11}$ 97.26% 1000 $5.09 \\times 10^{13}$ $7.07 \\times 10^{11}$ 98.61% 2000 $2.03 \\times 10^{14}$ $1.41 \\times 10^{12}$ 99.30% 4000 $8.08 \\times 10^{14}$ $2.83 \\times 10^{12}$ 99.65% 8000 $3.23 \\times 10^{15}$ $5.66 \\times 10^{12}$ 99.82% Example run using KV Cache Last not least, we can test the effectiveness of KV Cache using huggingface\u0026rsquo;s transformer library.\ndef test_transformer_kv_cache(model_name=\u0026#34;gpt2\u0026#34;, prompt=\u0026#34;Hello, I\u0026#39;m a language model\u0026#34;, num_new_tokens=50, num_runs=5, use_gpu=False): import time import torch from transformers import AutoModelForCausalLM, AutoTokenizer device = \u0026#34;cuda\u0026#34; if torch.cuda.is_available() and use_gpu else \u0026#34;cpu\u0026#34; print(f\u0026#34;Using device: {device}\u0026#34;) # Load model and tokenizer model = AutoModelForCausalLM.from_pretrained(model_name).to(device) tokenizer = AutoTokenizer.from_pretrained(model_name) # Tokenize input input_ids = tokenizer(prompt, return_tensors=\u0026#34;pt\u0026#34;).input_ids.to(device) input_length = input_ids.shape[1] results = { \u0026#34;with_kv_cache\u0026#34;: [], \u0026#34;without_kv_cache\u0026#34;: [] } print(f\u0026#34;Running inference with model: {model_name}\u0026#34;) print(f\u0026#34;Input prompt: \u0026#39;{prompt}\u0026#39; (Length: {input_length} tokens)\u0026#34;) print(f\u0026#34;Generating {num_new_tokens} new tokens, averaging over {num_runs} runs\\n\u0026#34;) for use_kv_cache in [False, True]: cache_status = \u0026#34;with\u0026#34; if use_kv_cache else \u0026#34;without\u0026#34; print(f\u0026#34;Testing {cache_status} KV cache...\u0026#34;) for run in range(num_runs): start_time = time.time() # Generate using model.generate with appropriate use_cache setting with torch.no_grad(): output = model.generate( input_ids, max_new_tokens=num_new_tokens, use_cache=use_kv_cache, do_sample=False, # Deterministic generation (greedy) pad_token_id=tokenizer.eos_token_id ) elapsed = time.time() - start_time results[f\u0026#34;{cache_status}_kv_cache\u0026#34;].append(elapsed) print(f\u0026#34; Run {run+1}/{num_runs}: {elapsed:.4f} seconds\u0026#34;) avg_time = sum(results[f\u0026#34;{cache_status}_kv_cache\u0026#34;]) / num_runs print(f\u0026#34;Average time {cache_status} KV cache: {avg_time:.4f} seconds\\n\u0026#34;) # Calculate speedup avg_time_without_kv = sum(results[\u0026#34;without_kv_cache\u0026#34;]) / num_runs avg_time_with_kv = sum(results[\u0026#34;with_kv_cache\u0026#34;]) / num_runs speedup = avg_time_without_kv / avg_time_with_kv reduction_percentage = (1 - avg_time_with_kv / avg_time_without_kv) * 100 print(\u0026#34;Results summary:\u0026#34;) print(f\u0026#34;- Without KV cache: {avg_time_without_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- With KV cache: {avg_time_with_kv:.4f} seconds\u0026#34;) print(f\u0026#34;- Speedup factor: {speedup:.2f}x\u0026#34;) print(f\u0026#34;- Time reduction: {reduction_percentage:.2f}%\u0026#34;) return We run GPT2 on Google Colab with a T4 GPU. The results are as follows:\nUsing device: cuda Running inference with model: gpt2 Input prompt: \u0026#39;Hello, I\u0026#39;m a language model\u0026#39; (Length: 7 tokens) Generating 1000 new tokens, averaging over 5 runs Results summary: - Without KV cache: 43.3307 seconds - With KV cache: 8.3611 seconds - Speedup factor: 5.18x - Time reduction: 80.70% ","permalink":"http://localhost:1313/posts/kv-cache/","summary":"\u003ch3 id=\"what-is-kv-cache\"\u003eWhat is KV Cache?\u003c/h3\u003e\n\u003cp\u003eI am not intended to spend too much time on details of KV cache. But as a reference, I found the interpretation in this this post \u003ca href=\"https://medium.com/@joaolages/kv-caching-explained-276520203249\"\u003eTransformers KV Caching Explained\u003c/a\u003e very intuitive, so I\u0026rsquo;ll just steal the gif here.\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"Comparison of self-attention with and without KV attention\" loading=\"lazy\" src=\"/images/kv-cache.gif\"\u003e\u003c/p\u003e\n\u003cp\u003eTo summarize, in auto-regressive language model,when generating a new token, all its previous tokens are fed into the attention layer for computation. In an attention layer, denote the text input/generation sequence as $X$, where as $i$ th token is $x_i$. When in step $i$, we are predicting $X_i$, the formula is:\n$$\nq_{i} = embed_i * W_q \\quad(1, d_{model})\n$$\n$$\nk_{i} = embed_i * W_k \\quad(1, d_{model})\n$$\n$$\nv_{i} = embed_i * W_v \\quad(1, d_{model})\n$$\n$$\nK = concat(k_{0}, k_{1}, \u0026hellip;, k_{i})  \\quad(i+1, d_{model})\n$$\n$$\nAttn = softmax(q_{i} * K^T / \\sqrt{d_{model}}) \\quad(1, i+1)\n$$\n$$\nOutput = Attn * [v_{0}, v_{1}, \u0026hellip;, v_{i}]  \\quad(1, d_{model})\n$$\u003c/p\u003e","title":"KV Cache Explained"}]